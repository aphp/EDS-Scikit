{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Getting started eds-scikit is a tool to assist data scientists working on the AP-HP\u2019s Clinical Data Warehouse. It is specifically targeted for OMOP-standardized data to: Ease access and analysis of data Allow a better transfer of knowledge between projects Improve research reproduciblity As an example, the following figure was obtained using various functionalities from eds-scikit. How was it done ? Click on the figure above to jump to the tutorial using various functionalities from eds-scikit, or continue reading the introduction! Using eds-scikit with I2B2 Although designed for OMOP databases, eds-scikit provides a connector for I2B2 databases is available. We don't guarantee its exhaustivity, but it should allow you to use functionnalities of the library seamlessly. Quick start Installation Requirements eds-scikit stands on the shoulders of Spark 2.4 which runs on Java 8 and Python ~3.7.1. If you work on AP-HP's CDW, those requirements are already fulfilled, so please disregard the following steps. Else, it is essential to: Install a version of Python \u2265 3.7.1 and < 3.8. Install OpenJDK 8 , an open-source reference implementation of Java 8 wit the following command lines: Linux (Debian, Ubunutu, etc.) Mac Windows $ sudo apt-get update $ sudo apt-get install openjdk-8-jdk ---> 100% For more details, check this installation guide $ brew tap AdoptOpenJDK/openjdk $ brew install --cask adoptopenjdk8 ---> 100% For more details, check this installation guide Follow this installation guide You can install eds-scikit via pip: $ pip install eds-scikit ---> 100% color:green Successfully installed eds_scikit ! Possible issue with pip If you get an an error during installation, please try downgrading pip via pip install -U \"pip<23\" before install eds-scikit Improving performances on distributed data It is highly recommanded (but not mandatory) to use the helper function eds_scikit.improve_performances to optimaly configure PySpark and Koalas. You can simply call import eds_scikit spark , sc , sql = eds_scikit . improve_performances () The function will return A SparkSession A SparkContext An sql function to execute SQL queries A first example: Merging visits together Let's tackle a common problem when dealing with clinical data: Merging close/consecutive visits into stays . As detailled in the dedicated section , eds-scikit is expecting to work with Pandas or Koalas DataFrames. We provide various connectors to facilitate data fetching, namely a Hive connector and a Postgres connector Using a Hive DataBase Using a Postgres DataBase Else from eds_scikit.io import HiveData data = HiveData ( DB_NAME ) visit_occurrence = data . visit_occurrence # (1) With this connector, visit_occurrence will be a Pandas DataFrame I2B2 If DB_NAME points to an I2B2 database, use data = HiveData(DB_NAME, database_type=\"I2B2\") from eds_scikit.io import PostgresData DB_NAME = \"my_db\" SCHEMA = \"my_schema\" USER = \"my_username\" data = PostgresData ( DB_NAME , schema = SCHEMA , user = USER ) # (1) visit_occurrence = data . visit_occurrence # (2) This connector expects a .pgpass file storing the connection parameters With this connector, visit_occurrence will be a Pandas DataFrame You can use eds-scikit with data from any source, as long as: - It follows the OMOP format - It is a Pandas or Koalas DataFrame import pandas as pd visit_occurrence = pd . read_csv ( \"./data/visit_occurrence.csv\" ) visit_occurrence For the sake of the example, only columns of interest are shown here. visit_occurrence_id person_id visit_start_datetime visit_end_datetime visit_source_value row_status_source_value care_site_id 0 A 999 2021-01-01 00:00:00 2021-01-05 00:00:00 hospitalis\u00e9s courant 1 1 B 999 2021-01-04 00:00:00 2021-01-08 00:00:00 hospitalis\u00e9s courant 1 2 C 999 2021-01-12 00:00:00 2021-01-18 00:00:00 hospitalis\u00e9s courant 1 3 D 999 2021-01-13 00:00:00 2021-01-14 00:00:00 urgence courant 1 4 E 999 2021-01-19 00:00:00 2021-01-21 00:00:00 hospitalis\u00e9s courant 2 5 F 999 2021-01-25 00:00:00 2021-01-27 00:00:00 hospitalis\u00e9s supprim\u00e9 1 6 G 999 2017-01-01 00:00:00 NaT hospitalis\u00e9s courant 1 # Importing the desired functions: from eds_scikit.period.stays import merge_visits , get_stays_duration # Calling the first function: computing stays visit_occurrence = merge_visits ( visit_occurrence ) As you can see, the function added a STAY_ID concept, grouping visits together visit_occurrence[[\"visit_occurrence_id\",\"STAY_ID\"]] visit_occurrence_id STAY_ID 0 A A 1 B A 2 C C 3 D C 4 E E 5 F F 6 G G # Calling the second function: computing stays duration stays = get_stays_duration ( visit_occurrence , missing_end_date_handling = \"coerce\" ) Here, each stay duration was calculated, dealing with potential overlaps and inclusions.: stays STAY_ID t_start t_end STAY_DURATION A 2021-01-01 00:00:00 2021-01-08 00:00:00 168 C 2021-01-12 00:00:00 2021-01-18 00:00:00 144 E 2021-01-19 00:00:00 2021-01-21 00:00:00 48 F 2021-01-25 00:00:00 2021-01-27 00:00:00 48 G 2017-01-01 00:00:00 NaT NaN About the code above As you noticed, the pipeline above is fairly straightforward, needing only the visit_occurrence DataFrame as input. However, it is also highly customizable, and you should always look into all the various availables options for the functions you're using. For instance, the following parameters could have been used: visit_occurrence = merge_visits ( visit_occurrence , remove_deleted_visits = True , long_stay_threshold = timedelta ( days = 365 ), long_stay_filtering = \"all\" , max_timedelta = timedelta ( hours = 24 ), merge_different_hospitals = False , merge_different_source_values = [ \"hospitalis\u00e9s\" , \"urgence\" ], ) stays = get_stays_duration ( visit_occurrence , algo = \"sum_of_visits_duration\" , missing_end_date_handling = \"coerce\" ) A word about AP-HP Specifics of AP-HP CDW eds-scikit was developped by AP-HP's Data Science team with the help of Inria's Soda team. As such, it is especially well fitted for AP-HP's Data Warehouse. In this doc, we use the following card to mention information that might be useful when using eds-scikit with AP-HP's data : Some information Here, we might for instance suggest some parameters for a function that should be used given AP-HP's data. EDS-NLP Also, a rule-based NLP library ( EDS-NLP ) designed to work on clinical texts was developped in parallel with eds-scikit. We decided not to include EDS-NLP as a dependency. Still, some functions might require an input \u00e0 la note_nlp : For instance, the current function designed to extract consultation dates from a visit_occurrence car work either on structured data only or with dates extracted in text and compiled in a DataFrame. You are free to use the method of your choice to get this DataFrame , as long as it contains the necessary columns as mentionned in the documentation. Note that we mention with the following card the availability of an EDS-NLP dedicated pipeline : A dedicated pipe For the example above, a consultation date pipeline exists. Moreover, methods are available to run an EDS-NLP pipeline on a Pandas, Spark or even Koalas DataFrame ! Contributing to eds-scikit We welcome contributions! Fork the project and create a pull request. Take a look at the dedicated page for details. Citation If you use eds-scikit , please cite us as below. @misc { eds-scikit , author = {Petit-Jean, Thomas and Remaki, Adam and Maladi\u00e8re, Vincent and Varoquaux, Ga\u00ebl and Bey, Romain} , doi = {10.5281/zenodo.7401549} , title = {eds-scikit: data analysis on OMOP databases} , url = {https://github.com/aphp/eds-scikit} }","title":"Home"},{"location":"#getting-started","text":"eds-scikit is a tool to assist data scientists working on the AP-HP\u2019s Clinical Data Warehouse. It is specifically targeted for OMOP-standardized data to: Ease access and analysis of data Allow a better transfer of knowledge between projects Improve research reproduciblity As an example, the following figure was obtained using various functionalities from eds-scikit. How was it done ? Click on the figure above to jump to the tutorial using various functionalities from eds-scikit, or continue reading the introduction! Using eds-scikit with I2B2 Although designed for OMOP databases, eds-scikit provides a connector for I2B2 databases is available. We don't guarantee its exhaustivity, but it should allow you to use functionnalities of the library seamlessly.","title":"Getting started"},{"location":"#quick-start","text":"","title":"Quick start"},{"location":"#installation","text":"Requirements eds-scikit stands on the shoulders of Spark 2.4 which runs on Java 8 and Python ~3.7.1. If you work on AP-HP's CDW, those requirements are already fulfilled, so please disregard the following steps. Else, it is essential to: Install a version of Python \u2265 3.7.1 and < 3.8. Install OpenJDK 8 , an open-source reference implementation of Java 8 wit the following command lines: Linux (Debian, Ubunutu, etc.) Mac Windows $ sudo apt-get update $ sudo apt-get install openjdk-8-jdk ---> 100% For more details, check this installation guide $ brew tap AdoptOpenJDK/openjdk $ brew install --cask adoptopenjdk8 ---> 100% For more details, check this installation guide Follow this installation guide You can install eds-scikit via pip: $ pip install eds-scikit ---> 100% color:green Successfully installed eds_scikit ! Possible issue with pip If you get an an error during installation, please try downgrading pip via pip install -U \"pip<23\" before install eds-scikit Improving performances on distributed data It is highly recommanded (but not mandatory) to use the helper function eds_scikit.improve_performances to optimaly configure PySpark and Koalas. You can simply call import eds_scikit spark , sc , sql = eds_scikit . improve_performances () The function will return A SparkSession A SparkContext An sql function to execute SQL queries","title":"Installation"},{"location":"#a-first-example-merging-visits-together","text":"Let's tackle a common problem when dealing with clinical data: Merging close/consecutive visits into stays . As detailled in the dedicated section , eds-scikit is expecting to work with Pandas or Koalas DataFrames. We provide various connectors to facilitate data fetching, namely a Hive connector and a Postgres connector Using a Hive DataBase Using a Postgres DataBase Else from eds_scikit.io import HiveData data = HiveData ( DB_NAME ) visit_occurrence = data . visit_occurrence # (1) With this connector, visit_occurrence will be a Pandas DataFrame I2B2 If DB_NAME points to an I2B2 database, use data = HiveData(DB_NAME, database_type=\"I2B2\") from eds_scikit.io import PostgresData DB_NAME = \"my_db\" SCHEMA = \"my_schema\" USER = \"my_username\" data = PostgresData ( DB_NAME , schema = SCHEMA , user = USER ) # (1) visit_occurrence = data . visit_occurrence # (2) This connector expects a .pgpass file storing the connection parameters With this connector, visit_occurrence will be a Pandas DataFrame You can use eds-scikit with data from any source, as long as: - It follows the OMOP format - It is a Pandas or Koalas DataFrame import pandas as pd visit_occurrence = pd . read_csv ( \"./data/visit_occurrence.csv\" ) visit_occurrence For the sake of the example, only columns of interest are shown here. visit_occurrence_id person_id visit_start_datetime visit_end_datetime visit_source_value row_status_source_value care_site_id 0 A 999 2021-01-01 00:00:00 2021-01-05 00:00:00 hospitalis\u00e9s courant 1 1 B 999 2021-01-04 00:00:00 2021-01-08 00:00:00 hospitalis\u00e9s courant 1 2 C 999 2021-01-12 00:00:00 2021-01-18 00:00:00 hospitalis\u00e9s courant 1 3 D 999 2021-01-13 00:00:00 2021-01-14 00:00:00 urgence courant 1 4 E 999 2021-01-19 00:00:00 2021-01-21 00:00:00 hospitalis\u00e9s courant 2 5 F 999 2021-01-25 00:00:00 2021-01-27 00:00:00 hospitalis\u00e9s supprim\u00e9 1 6 G 999 2017-01-01 00:00:00 NaT hospitalis\u00e9s courant 1 # Importing the desired functions: from eds_scikit.period.stays import merge_visits , get_stays_duration # Calling the first function: computing stays visit_occurrence = merge_visits ( visit_occurrence ) As you can see, the function added a STAY_ID concept, grouping visits together visit_occurrence[[\"visit_occurrence_id\",\"STAY_ID\"]] visit_occurrence_id STAY_ID 0 A A 1 B A 2 C C 3 D C 4 E E 5 F F 6 G G # Calling the second function: computing stays duration stays = get_stays_duration ( visit_occurrence , missing_end_date_handling = \"coerce\" ) Here, each stay duration was calculated, dealing with potential overlaps and inclusions.: stays STAY_ID t_start t_end STAY_DURATION A 2021-01-01 00:00:00 2021-01-08 00:00:00 168 C 2021-01-12 00:00:00 2021-01-18 00:00:00 144 E 2021-01-19 00:00:00 2021-01-21 00:00:00 48 F 2021-01-25 00:00:00 2021-01-27 00:00:00 48 G 2017-01-01 00:00:00 NaT NaN About the code above As you noticed, the pipeline above is fairly straightforward, needing only the visit_occurrence DataFrame as input. However, it is also highly customizable, and you should always look into all the various availables options for the functions you're using. For instance, the following parameters could have been used: visit_occurrence = merge_visits ( visit_occurrence , remove_deleted_visits = True , long_stay_threshold = timedelta ( days = 365 ), long_stay_filtering = \"all\" , max_timedelta = timedelta ( hours = 24 ), merge_different_hospitals = False , merge_different_source_values = [ \"hospitalis\u00e9s\" , \"urgence\" ], ) stays = get_stays_duration ( visit_occurrence , algo = \"sum_of_visits_duration\" , missing_end_date_handling = \"coerce\" )","title":"A first example: Merging visits together"},{"location":"#a-word-about-ap-hp","text":"","title":"A word about AP-HP"},{"location":"#specifics-of-ap-hp-cdw","text":"eds-scikit was developped by AP-HP's Data Science team with the help of Inria's Soda team. As such, it is especially well fitted for AP-HP's Data Warehouse. In this doc, we use the following card to mention information that might be useful when using eds-scikit with AP-HP's data : Some information Here, we might for instance suggest some parameters for a function that should be used given AP-HP's data.","title":"Specifics of AP-HP CDW"},{"location":"#eds-nlp","text":"Also, a rule-based NLP library ( EDS-NLP ) designed to work on clinical texts was developped in parallel with eds-scikit. We decided not to include EDS-NLP as a dependency. Still, some functions might require an input \u00e0 la note_nlp : For instance, the current function designed to extract consultation dates from a visit_occurrence car work either on structured data only or with dates extracted in text and compiled in a DataFrame. You are free to use the method of your choice to get this DataFrame , as long as it contains the necessary columns as mentionned in the documentation. Note that we mention with the following card the availability of an EDS-NLP dedicated pipeline : A dedicated pipe For the example above, a consultation date pipeline exists. Moreover, methods are available to run an EDS-NLP pipeline on a Pandas, Spark or even Koalas DataFrame !","title":"EDS-NLP"},{"location":"#contributing-to-eds-scikit","text":"We welcome contributions! Fork the project and create a pull request. Take a look at the dedicated page for details.","title":"Contributing to eds-scikit"},{"location":"#citation","text":"If you use eds-scikit , please cite us as below. @misc { eds-scikit , author = {Petit-Jean, Thomas and Remaki, Adam and Maladi\u00e8re, Vincent and Varoquaux, Ga\u00ebl and Bey, Romain} , doi = {10.5281/zenodo.7401549} , title = {eds-scikit: data analysis on OMOP databases} , url = {https://github.com/aphp/eds-scikit} }","title":"Citation"},{"location":"changelog/","text":"Changelog Unreleased Changed Support for pyarrow > 0.17.0 Added biology module refacto load_koalas() not by default in init .py but called in the improve_performance function adding app_name in improve_performances to facilitate app monitoring Fixed Generation of an inclusion/exclusion flowchart in plotting improve_performance moved from init .py to io/improve_performance.py file Caching in spark instead of koalas to improve speed v0.1.6 (2023-09-27) Added Module event_sequences to visualize individual sequences of events. Module age_pyramid to quickly visualize the age and gender distributions in a cohort. Fixed Compatibility with EDS-TeVa and EDSNLP . v0.1.5 (2023-04-05) Added BaseData class as a parent class for HiveData, PandasData and PostgresData. Phentyping class with 4 implemented phenotyes. Custom logger to display useful information during computation. Fixed Add caching to speedup computations. Updated method to persist tables as parquet locally, with a support for ORC-stored I2B2 database. v0.1.4 (2023-02-09) Added Allow saving DB locally in client or cluster mode. Add data cleaning function to handle incorrect datetime in spark. Filter biology config on care site. Adding person-dependent datetime_ref to plot_age_pyramid . Fixed Consultations date for OMOP & I2B2 v0.1.3 (2023-02-02) Added New BackendDispatcher to handle framework-specific functions I2B2 to OMOP connector v0.1.2 (2022-12-05) Added Adding CITATION.cff Using mike as a documentation provider Fixed Correct build to PyPI Renaming from EDS-Scikit to eds-scikit v0.1.1 (2022-12-02) Added Various project metadata Full CI pipeline License checker in CI BackendDispatcher object to help with pandas / koalas manipulation Fixed Broken links in documentation and badges v0.1.0 (2022-12-01) Added Initial commit to GitHub","title":"Changelog"},{"location":"changelog/#changelog","text":"","title":"Changelog"},{"location":"changelog/#unreleased","text":"","title":"Unreleased"},{"location":"changelog/#changed","text":"Support for pyarrow > 0.17.0","title":"Changed"},{"location":"changelog/#added","text":"biology module refacto load_koalas() not by default in init .py but called in the improve_performance function adding app_name in improve_performances to facilitate app monitoring","title":"Added"},{"location":"changelog/#fixed","text":"Generation of an inclusion/exclusion flowchart in plotting improve_performance moved from init .py to io/improve_performance.py file Caching in spark instead of koalas to improve speed","title":"Fixed"},{"location":"changelog/#v016-2023-09-27","text":"","title":"v0.1.6 (2023-09-27)"},{"location":"changelog/#added_1","text":"Module event_sequences to visualize individual sequences of events. Module age_pyramid to quickly visualize the age and gender distributions in a cohort.","title":"Added"},{"location":"changelog/#fixed_1","text":"Compatibility with EDS-TeVa and EDSNLP .","title":"Fixed"},{"location":"changelog/#v015-2023-04-05","text":"","title":"v0.1.5 (2023-04-05)"},{"location":"changelog/#added_2","text":"BaseData class as a parent class for HiveData, PandasData and PostgresData. Phentyping class with 4 implemented phenotyes. Custom logger to display useful information during computation.","title":"Added"},{"location":"changelog/#fixed_2","text":"Add caching to speedup computations. Updated method to persist tables as parquet locally, with a support for ORC-stored I2B2 database.","title":"Fixed"},{"location":"changelog/#v014-2023-02-09","text":"","title":"v0.1.4 (2023-02-09)"},{"location":"changelog/#added_3","text":"Allow saving DB locally in client or cluster mode. Add data cleaning function to handle incorrect datetime in spark. Filter biology config on care site. Adding person-dependent datetime_ref to plot_age_pyramid .","title":"Added"},{"location":"changelog/#fixed_3","text":"Consultations date for OMOP & I2B2","title":"Fixed"},{"location":"changelog/#v013-2023-02-02","text":"","title":"v0.1.3 (2023-02-02)"},{"location":"changelog/#added_4","text":"New BackendDispatcher to handle framework-specific functions I2B2 to OMOP connector","title":"Added"},{"location":"changelog/#v012-2022-12-05","text":"","title":"v0.1.2 (2022-12-05)"},{"location":"changelog/#added_5","text":"Adding CITATION.cff Using mike as a documentation provider","title":"Added"},{"location":"changelog/#fixed_4","text":"Correct build to PyPI Renaming from EDS-Scikit to eds-scikit","title":"Fixed"},{"location":"changelog/#v011-2022-12-02","text":"","title":"v0.1.1 (2022-12-02)"},{"location":"changelog/#added_6","text":"Various project metadata Full CI pipeline License checker in CI BackendDispatcher object to help with pandas / koalas manipulation","title":"Added"},{"location":"changelog/#fixed_5","text":"Broken links in documentation and badges","title":"Fixed"},{"location":"changelog/#v010-2022-12-01","text":"","title":"v0.1.0 (2022-12-01)"},{"location":"changelog/#added_7","text":"Initial commit to GitHub","title":"Added"},{"location":"contributing/","text":"Contributing We welcome contributions! There are many ways to help. For example, you can: Help us track bugs by filing issues Suggest and help prioritise new functionalities Develop a new functionality! Help us make the library as straightforward as possible, by simply asking questions on whatever does not seem clear to you. Please do not hesitate to suggest functionalities you have developed and want to incorporate into eds-scikit. We will be glad to help! Also, any non-technical contribution (e.g. lists of ICD-10 codes curated for a research project) is also welcome. Development installation To be able to run the test suite, run the example notebooks and develop your own functionalities, you should clone the repo and install it locally. Spark and Java To run tests locally, you need to have Spark and Java. Whereas Spark will be installed as a dependency of PySpark, you may need to install Java yourself. Please check to installation procedure. # Clone the repository and change directory $ git clone https://github.com/aphp/eds-scikit.git ---> 100% $ cd eds-scikit # Create a virtual environment $ python -m venv venv $ source venv/bin/activate # Install dependencies and build resources $ pip install -e \".[dev, doc]\" # And switch to a new branch to begin developing $ git switch -c \"name_of_my_new_branch\" To make sure the pipeline will not fail because of formatting errors, we added pre-commit hooks using the pre-commit Python library. To use it, simply install it: $ pre-commit install The pre-commit hooks defined in the configuration will automatically run when you commit your changes, letting you know if something went wrong. The hooks only run on staged changes. To force-run it on all files, run: $ pre-commit run --all-files ---> 100% color:green All good ! Proposing a merge request At the very least, your changes should : Be well-documented ; Pass every tests, and preferably implement its own ; Follow the style guide. Testing your code We use the Pytest test suite. The following command will run the test suite. Writing your own tests is encouraged! python -m pytest ./tests Most tests are designed to run both with Pandas as Koalas DataFrames as input. However, to gain time, by default only Pandas testing is done. The above line of code is equivalent to python -m pytest ./tests -m \"not koalas\" However, you can also run tests using only Koalas input: python -m pytest ./tests -m \"koalas\" or using both inputs: python -m pytest ./tests -m \"\" Finally when developing, you might be interested to run tests for a single file, or even a single function. To do so: python -m pytest ./tests/my_file.py #(1) python -m pytest ./tests/my_file.py:my_test_function #(2) 1. Will run all tests found in this file 2. Will only run \"my_test_function\" Style Guide We use Black to reformat the code. While other formatter only enforce PEP8 compliance, Black also makes the code uniform. In short : Black reformats entire files in place. It is not configurable. Moreover, the CI/CD pipeline enforces a number of checks on the \"quality\" of the code. To wit, non black-formatted code will make the test pipeline fail. We use pre-commit to keep our codebase clean. Refer to the development install tutorial for tips on how to format your files automatically. Most modern editors propose extensions that will format files on save. On conventional commits We try to use conventional commits guidelines as much as possible. In short, prepend each commit message with one of the following prefix: fix: when patching a bug feat: when introducing a new feature If needed, you can also use one of the following: build:, chore:, ci:, docs:, style:, refactor:, perf:, test Documentation Make sure to document your improvements, both within the code with comprehensive docstrings, as well as in the documentation itself if need be. We use MkDocs for eds-scikit's documentation. You can checkout the changes you make with: # Install the requirements $ pip install \".[doc]\" ---> 100% color:green Installation successful # Run the documentation $ mkdocs serve Go to localhost:8000 to see your changes. MkDocs watches for changes in the documentation folder and automatically reloads the page. Warning MkDocs will automaticaly build code documentation by going through every .py file located in the eds_scikit directory (and sub-arborescence). It expects to find a __init__.py file in each directory, so make sure to create one if needed. Developing your own methods Even though the koalas project aim at covering most pandas functions for spark, there are some discrepancies. For instance, the pd.cut() method has no koalas alternative. To ease the development and switch gears efficiently between the two backends, we advice you to use the BackendDispatcher class and its collection of custom methods.","title":"Contributing"},{"location":"contributing/#contributing","text":"We welcome contributions! There are many ways to help. For example, you can: Help us track bugs by filing issues Suggest and help prioritise new functionalities Develop a new functionality! Help us make the library as straightforward as possible, by simply asking questions on whatever does not seem clear to you. Please do not hesitate to suggest functionalities you have developed and want to incorporate into eds-scikit. We will be glad to help! Also, any non-technical contribution (e.g. lists of ICD-10 codes curated for a research project) is also welcome.","title":"Contributing"},{"location":"contributing/#development-installation","text":"To be able to run the test suite, run the example notebooks and develop your own functionalities, you should clone the repo and install it locally. Spark and Java To run tests locally, you need to have Spark and Java. Whereas Spark will be installed as a dependency of PySpark, you may need to install Java yourself. Please check to installation procedure. # Clone the repository and change directory $ git clone https://github.com/aphp/eds-scikit.git ---> 100% $ cd eds-scikit # Create a virtual environment $ python -m venv venv $ source venv/bin/activate # Install dependencies and build resources $ pip install -e \".[dev, doc]\" # And switch to a new branch to begin developing $ git switch -c \"name_of_my_new_branch\" To make sure the pipeline will not fail because of formatting errors, we added pre-commit hooks using the pre-commit Python library. To use it, simply install it: $ pre-commit install The pre-commit hooks defined in the configuration will automatically run when you commit your changes, letting you know if something went wrong. The hooks only run on staged changes. To force-run it on all files, run: $ pre-commit run --all-files ---> 100% color:green All good !","title":"Development installation"},{"location":"contributing/#proposing-a-merge-request","text":"At the very least, your changes should : Be well-documented ; Pass every tests, and preferably implement its own ; Follow the style guide.","title":"Proposing a merge request"},{"location":"contributing/#testing-your-code","text":"We use the Pytest test suite. The following command will run the test suite. Writing your own tests is encouraged! python -m pytest ./tests Most tests are designed to run both with Pandas as Koalas DataFrames as input. However, to gain time, by default only Pandas testing is done. The above line of code is equivalent to python -m pytest ./tests -m \"not koalas\" However, you can also run tests using only Koalas input: python -m pytest ./tests -m \"koalas\" or using both inputs: python -m pytest ./tests -m \"\" Finally when developing, you might be interested to run tests for a single file, or even a single function. To do so: python -m pytest ./tests/my_file.py #(1) python -m pytest ./tests/my_file.py:my_test_function #(2) 1. Will run all tests found in this file 2. Will only run \"my_test_function\"","title":"Testing your code"},{"location":"contributing/#style-guide","text":"We use Black to reformat the code. While other formatter only enforce PEP8 compliance, Black also makes the code uniform. In short : Black reformats entire files in place. It is not configurable. Moreover, the CI/CD pipeline enforces a number of checks on the \"quality\" of the code. To wit, non black-formatted code will make the test pipeline fail. We use pre-commit to keep our codebase clean. Refer to the development install tutorial for tips on how to format your files automatically. Most modern editors propose extensions that will format files on save. On conventional commits We try to use conventional commits guidelines as much as possible. In short, prepend each commit message with one of the following prefix: fix: when patching a bug feat: when introducing a new feature If needed, you can also use one of the following: build:, chore:, ci:, docs:, style:, refactor:, perf:, test","title":"Style Guide"},{"location":"contributing/#documentation","text":"Make sure to document your improvements, both within the code with comprehensive docstrings, as well as in the documentation itself if need be. We use MkDocs for eds-scikit's documentation. You can checkout the changes you make with: # Install the requirements $ pip install \".[doc]\" ---> 100% color:green Installation successful # Run the documentation $ mkdocs serve Go to localhost:8000 to see your changes. MkDocs watches for changes in the documentation folder and automatically reloads the page. Warning MkDocs will automaticaly build code documentation by going through every .py file located in the eds_scikit directory (and sub-arborescence). It expects to find a __init__.py file in each directory, so make sure to create one if needed.","title":"Documentation"},{"location":"contributing/#developing-your-own-methods","text":"Even though the koalas project aim at covering most pandas functions for spark, there are some discrepancies. For instance, the pd.cut() method has no koalas alternative. To ease the development and switch gears efficiently between the two backends, we advice you to use the BackendDispatcher class and its collection of custom methods.","title":"Developing your own methods"},{"location":"project_description/","text":"Goal eds-scikit is a tool to assist datascientists working on the AP-HP's Clinical Data Warehouse. It is specifically targeted for OMOP-standardized data to: Ease access and analysis of data Allow a better transfer of knowledge between projects Improve research reproduciblity Main working principles Dealing with various data sizes Generally, data analysis can be done in two ways: Locally , by loading everything in RAM and working with e.g. Pandas In a distributed fashion, when dealing with a lot of data, by using e.g. Spark While working with Pandas is often more convenient, its use can be problematic once working with large cohorts. Thus, making eds-scikit a Pandas-only library wasn't conceivable. In order to allow analysis to be conducted at scale, eds-scikit integrates with Koalas . Koalas Koalas is a library implementing Pandas API on top of Spark . Basically, it allows for functions and methods developped for Pandas DataFrames to work on Spark DataFrames with close to no adjustments. Let us see a dummy example where one wants to count the number of visit occurrences per month . Using Spark (via PySpark) Using Pandas Suppose we have a Spark visit_occurrence DataFrame: type ( visit_occurrence_spark ) # Out: pyspark.sql.dataframe.DataFrame import pyspark.sql.functions as F def get_stats_spark ( visit_occurrence ): \"\"\" Computes the number of visits per month Parameters ---------- visit_occurrence : DataFrame Returns ------- stats : pd.DataFrame \"\"\" # Adding a month and year column visit_occurrence = visit_occurrence . withColumn ( \"year\" , F . year ( \"visit_start_datetime\" ) ) . withColumn ( \"month\" , F . month ( \"visit_start_datetime\" )) # Grouping and filtering stats = ( visit_occurrence . groupby ([ \"year\" , \"month\" ]) . count () . filter (( F . col ( \"year\" ) >= 2017 )) . toPandas () ) return stats stats_from_spark = get_stats_spark ( visit_occurrence_spark ) If the selected database contains few enough visits, we may have a visit_occurrence DataFrame small enough to fit in memory as a Pandas DataFrame. type ( visit_occurrence_pandas ) # Out: pandas.core.frame.DataFrame Then run the same analysis: def get_stats_pandas ( visit_occurrence ): \"\"\" Computes the number of visits per month Parameters ---------- visit_occurrence : DataFrame Returns ------- stats : pd.DataFrame \"\"\" # Adding a duration column visit_occurrence [ \"year\" ] = visit_occurrence [ \"visit_start_datetime\" ] . dt . year visit_occurrence [ \"month\" ] = visit_occurrence [ \"visit_start_datetime\" ] . dt . month # Grouping and filtering stats = ( visit_occurrence . groupby ([ \"year\" , \"month\" ]) . visit_occurrence_id . count () . reset_index () ) stats = stats [ stats [ \"year\" ] >= 2017 ] stats . columns = [ \"year\" , \"month\" , \"count\" ] return stats stats_from_pandas = get_stats_pandas ( visit_occurrence_pandas ) The two examples above clearly show the syntax differences between using Pandas and using Spark . In order for a library to work both with Pandas and Spark, one would need to developp each function twice to accomodate for those two frameworks. Another problem might occur if you are dealing with a huge cohort, forcing you to do your final analysis in a distributed manner via Spark. In that scenario, you coudn't test your code on a small Pandas DataFrame subset. The goal of Koalas is precisely to avoid this issue. It aims at allowing code to be written for Pandas DataFrames, and also run with (almost) no adjustements with Spark DataFrame: from databricks import koalas as ks # Converting the Spark DataFrame into a Koalas DataFrame visit_occurrence_koalas = visit_occurrence_spark . to_koalas () Info The code above allows the DataFrame to stay distributed \u2014as opposed to applying the .toPandas() method. We can now use the function we designed for Pandas with a Koalas DataFrame: stats_from_koalas = get_stats_pandas ( visit_occurrence_koalas ) Since we aggregated the data, its size is manageable so we can convert it back to Pandas for e.g. plotting stats_from_koalas = stats_from_koalas . to_pandas () Concept Most functions developped in the library implements a concept . For sake of clarity let us illustrate this notion with an example: The function tag_icu_care_site() can be used to tag a care site as being an ICU or not. We say that it implements the concept \"IS_ICU\" because it adds a column named \"IS_ICU\" to the input DataFrame , as it can be seen from the docstring: \"\"\" Returns ------- care_site: DataFrame Dataframe with 1 added column corresponding to the following concept: - 'IS_ICU' \"\"\" This follows a wide data format. However, when multiple concepts are added at once, it might be done in a long format, such as with the diabetes_from_icd10() function, which stores the diabetes type in a concept column, and the corresponding ICD-10 code in a value column: \"\"\" Returns ------- DataFrame Event DataFrame in **long** format (with a `concept` and a `value` column). The `concept` column contains one of the following: - DIABETES_TYPE_I - DIABETES_TYPE_II - DIABETES_MALNUTRITION - DIABETES_IN_PREGNANCY - OTHER_DIABETES_MELLITUS - DIABETES_INSIPIDUS The `value` column contains the corresponding ICD-10 code that was extracted \"\"\" Question Check this link for a (very) quick explanation if you aren't familiar with Long vs Wide data format. Algo Most functions also have an argument called algo , which allows you to choose how a specific concept will be implemented in a function. Let's check the docstring of the same function tag_icu_care_site() : \"\"\" Parameters ---------- care_site: DataFrame algo: str Possible values are: - `\"from_authorisation_type\"` - `\"from_regex_on_care_site_description\"` \"\"\" The function's signature shows that \"from_authorisation_type\" is the default algo , used if the algo argument isn't filled by the user. In the documentation, the different \"algo\" values will be displayed as tabs, along with a short description and optional algo-dependant parameters: Availables algorithms (values for \"algo\" ) Algo 1 (default) Algo 2 This \"algo\" is used by default. It does yadi yada. Specific parameters: This first parameter This second parameter And also this third one This second \"algo\" works differently. It has no additional parameters Please check the available algos when using a function from eds-scikit, to understand what each of them is doing and which one might fits you best.","title":"Project description"},{"location":"project_description/#goal","text":"eds-scikit is a tool to assist datascientists working on the AP-HP's Clinical Data Warehouse. It is specifically targeted for OMOP-standardized data to: Ease access and analysis of data Allow a better transfer of knowledge between projects Improve research reproduciblity","title":"Goal"},{"location":"project_description/#main-working-principles","text":"","title":"Main working principles"},{"location":"project_description/#dealing-with-various-data-sizes","text":"Generally, data analysis can be done in two ways: Locally , by loading everything in RAM and working with e.g. Pandas In a distributed fashion, when dealing with a lot of data, by using e.g. Spark While working with Pandas is often more convenient, its use can be problematic once working with large cohorts. Thus, making eds-scikit a Pandas-only library wasn't conceivable. In order to allow analysis to be conducted at scale, eds-scikit integrates with Koalas . Koalas Koalas is a library implementing Pandas API on top of Spark . Basically, it allows for functions and methods developped for Pandas DataFrames to work on Spark DataFrames with close to no adjustments. Let us see a dummy example where one wants to count the number of visit occurrences per month . Using Spark (via PySpark) Using Pandas Suppose we have a Spark visit_occurrence DataFrame: type ( visit_occurrence_spark ) # Out: pyspark.sql.dataframe.DataFrame import pyspark.sql.functions as F def get_stats_spark ( visit_occurrence ): \"\"\" Computes the number of visits per month Parameters ---------- visit_occurrence : DataFrame Returns ------- stats : pd.DataFrame \"\"\" # Adding a month and year column visit_occurrence = visit_occurrence . withColumn ( \"year\" , F . year ( \"visit_start_datetime\" ) ) . withColumn ( \"month\" , F . month ( \"visit_start_datetime\" )) # Grouping and filtering stats = ( visit_occurrence . groupby ([ \"year\" , \"month\" ]) . count () . filter (( F . col ( \"year\" ) >= 2017 )) . toPandas () ) return stats stats_from_spark = get_stats_spark ( visit_occurrence_spark ) If the selected database contains few enough visits, we may have a visit_occurrence DataFrame small enough to fit in memory as a Pandas DataFrame. type ( visit_occurrence_pandas ) # Out: pandas.core.frame.DataFrame Then run the same analysis: def get_stats_pandas ( visit_occurrence ): \"\"\" Computes the number of visits per month Parameters ---------- visit_occurrence : DataFrame Returns ------- stats : pd.DataFrame \"\"\" # Adding a duration column visit_occurrence [ \"year\" ] = visit_occurrence [ \"visit_start_datetime\" ] . dt . year visit_occurrence [ \"month\" ] = visit_occurrence [ \"visit_start_datetime\" ] . dt . month # Grouping and filtering stats = ( visit_occurrence . groupby ([ \"year\" , \"month\" ]) . visit_occurrence_id . count () . reset_index () ) stats = stats [ stats [ \"year\" ] >= 2017 ] stats . columns = [ \"year\" , \"month\" , \"count\" ] return stats stats_from_pandas = get_stats_pandas ( visit_occurrence_pandas ) The two examples above clearly show the syntax differences between using Pandas and using Spark . In order for a library to work both with Pandas and Spark, one would need to developp each function twice to accomodate for those two frameworks. Another problem might occur if you are dealing with a huge cohort, forcing you to do your final analysis in a distributed manner via Spark. In that scenario, you coudn't test your code on a small Pandas DataFrame subset. The goal of Koalas is precisely to avoid this issue. It aims at allowing code to be written for Pandas DataFrames, and also run with (almost) no adjustements with Spark DataFrame: from databricks import koalas as ks # Converting the Spark DataFrame into a Koalas DataFrame visit_occurrence_koalas = visit_occurrence_spark . to_koalas () Info The code above allows the DataFrame to stay distributed \u2014as opposed to applying the .toPandas() method. We can now use the function we designed for Pandas with a Koalas DataFrame: stats_from_koalas = get_stats_pandas ( visit_occurrence_koalas ) Since we aggregated the data, its size is manageable so we can convert it back to Pandas for e.g. plotting stats_from_koalas = stats_from_koalas . to_pandas ()","title":"Dealing with various data sizes"},{"location":"project_description/#concept","text":"Most functions developped in the library implements a concept . For sake of clarity let us illustrate this notion with an example: The function tag_icu_care_site() can be used to tag a care site as being an ICU or not. We say that it implements the concept \"IS_ICU\" because it adds a column named \"IS_ICU\" to the input DataFrame , as it can be seen from the docstring: \"\"\" Returns ------- care_site: DataFrame Dataframe with 1 added column corresponding to the following concept: - 'IS_ICU' \"\"\" This follows a wide data format. However, when multiple concepts are added at once, it might be done in a long format, such as with the diabetes_from_icd10() function, which stores the diabetes type in a concept column, and the corresponding ICD-10 code in a value column: \"\"\" Returns ------- DataFrame Event DataFrame in **long** format (with a `concept` and a `value` column). The `concept` column contains one of the following: - DIABETES_TYPE_I - DIABETES_TYPE_II - DIABETES_MALNUTRITION - DIABETES_IN_PREGNANCY - OTHER_DIABETES_MELLITUS - DIABETES_INSIPIDUS The `value` column contains the corresponding ICD-10 code that was extracted \"\"\" Question Check this link for a (very) quick explanation if you aren't familiar with Long vs Wide data format.","title":"Concept"},{"location":"project_description/#algo","text":"Most functions also have an argument called algo , which allows you to choose how a specific concept will be implemented in a function. Let's check the docstring of the same function tag_icu_care_site() : \"\"\" Parameters ---------- care_site: DataFrame algo: str Possible values are: - `\"from_authorisation_type\"` - `\"from_regex_on_care_site_description\"` \"\"\" The function's signature shows that \"from_authorisation_type\" is the default algo , used if the algo argument isn't filled by the user. In the documentation, the different \"algo\" values will be displayed as tabs, along with a short description and optional algo-dependant parameters: Availables algorithms (values for \"algo\" ) Algo 1 (default) Algo 2 This \"algo\" is used by default. It does yadi yada. Specific parameters: This first parameter This second parameter And also this third one This second \"algo\" works differently. It has no additional parameters Please check the available algos when using a function from eds-scikit, to understand what each of them is doing and which one might fits you best.","title":"Algo"},{"location":"datasets/care-site-emergency/","text":"Presentation Emergency This dataset is useful to extract emergency care sites from AP-HP's CDW This dataset contains care sites labelled as emergency. Those care sites were extracted and verified by Ariel COHEN, Judith LEBLANC, and an ER doctor validated them. Those emergency care sites are further divised into different categories, as defined in the concept \"EMERGENCY_TYPE\" . The different categories are: Urgences sp\u00e9cialis\u00e9es UHCD + Post-urgences Urgences p\u00e9diatriques Urgences g\u00e9n\u00e9rales adulte Consultation urgences SAMU / SMUR Warning This dataset was built in 2021. Structure and usage Internally, the dataset is returned by calling the function get_care_site_emergency_mapping() : from eds_scikit.resources import registry df = registry . get ( \"data\" , function_name = \"get_care_site_emergency_mapping\" )() It should return a Pandas Dataframe with 2 columns: care_site_source_value (OMOP column) EMERGENCY_TYPE (see above) Use your own data. It is as simple as registering a new loading function: custom_resources.py from eds_scikit.resources import registry @registry . data ( \"get_care_site_emergency_mapping\" ) def get_care_site_emergency_mapping (): \"\"\" Your code here \"\"\" return df Then simply import your custom_resources module before running eds-scikit's pipelines, and you're good to go.","title":"Emergency"},{"location":"datasets/care-site-emergency/#presentation","text":"Emergency This dataset is useful to extract emergency care sites from AP-HP's CDW This dataset contains care sites labelled as emergency. Those care sites were extracted and verified by Ariel COHEN, Judith LEBLANC, and an ER doctor validated them. Those emergency care sites are further divised into different categories, as defined in the concept \"EMERGENCY_TYPE\" . The different categories are: Urgences sp\u00e9cialis\u00e9es UHCD + Post-urgences Urgences p\u00e9diatriques Urgences g\u00e9n\u00e9rales adulte Consultation urgences SAMU / SMUR Warning This dataset was built in 2021.","title":"Presentation"},{"location":"datasets/care-site-emergency/#structure-and-usage","text":"Internally, the dataset is returned by calling the function get_care_site_emergency_mapping() : from eds_scikit.resources import registry df = registry . get ( \"data\" , function_name = \"get_care_site_emergency_mapping\" )() It should return a Pandas Dataframe with 2 columns: care_site_source_value (OMOP column) EMERGENCY_TYPE (see above)","title":"Structure and usage"},{"location":"datasets/care-site-emergency/#use-your-own-data","text":"It is as simple as registering a new loading function: custom_resources.py from eds_scikit.resources import registry @registry . data ( \"get_care_site_emergency_mapping\" ) def get_care_site_emergency_mapping (): \"\"\" Your code here \"\"\" return df Then simply import your custom_resources module before running eds-scikit's pipelines, and you're good to go.","title":"Use your own data."},{"location":"datasets/care-site-hierarchy/","text":"Presentation Care sites This dataset is useful to link AP-HP's care sites of various levels together To generate it, it uses the fact_relationship OMOP table, with the care_site domain and the A is part of B relation. Thus, it generates a wide-type table, effectively flattening out the hierarchical structure of each care site. This dataset is useful to find the parent of a care_site , e.g.: in which hospital is this UDS ( Unit\u00e9 De Soin ) ? in which UF ( Unit\u00e9 Fonctionnelle ) is this UMA ( Unit\u00e9 M\u00e9dico-Administrative ) ? Structure and usage In this dataset each row corresponds to a given care_site and the columns contain the ids of the parent care_site for several hierarchical level. Those columns are thus values contained in care_site_type_source_value . Internally, the dataset is returned by calling the function get_care_site_hierarchy() : from eds_scikit.resources import registry df = registry . get ( \"data\" , function_name = \"get_care_site_hierarchy\" )() Use your own data. It is as simple as registering a new loading function: custom_resources.py from eds_scikit.resources import registry # (1) @registry . data ( \"get_care_site_hierarchy\" ) # (2) def get_care_site_hierarchy (): \"\"\" Your code here \"\"\" return df The registry instance stores user-defined functions Using this decorator allows to register the function when importing the corresponding file Then simply import your custom_resources module before running eds-scikit's pipelines, and you're good to go. Structure and usage Internally, the dataset is returned by calling the function get_care_site_hierarchy() . It should return a Pandas Dataframe with the following columns: care_site_id (OMOP column): The identifier of the care site care_site_type_source_value (OMOP column): The type of care site Additionally, it can contains an arbitrary number of columns whose name are values from care_site_type_source_value , and whose values are care_site_id of the corresponding parent structure Generation function You can generate the dataset on your specific data using this function","title":"Hierarchy"},{"location":"datasets/care-site-hierarchy/#presentation","text":"Care sites This dataset is useful to link AP-HP's care sites of various levels together To generate it, it uses the fact_relationship OMOP table, with the care_site domain and the A is part of B relation. Thus, it generates a wide-type table, effectively flattening out the hierarchical structure of each care site. This dataset is useful to find the parent of a care_site , e.g.: in which hospital is this UDS ( Unit\u00e9 De Soin ) ? in which UF ( Unit\u00e9 Fonctionnelle ) is this UMA ( Unit\u00e9 M\u00e9dico-Administrative ) ?","title":"Presentation"},{"location":"datasets/care-site-hierarchy/#structure-and-usage","text":"In this dataset each row corresponds to a given care_site and the columns contain the ids of the parent care_site for several hierarchical level. Those columns are thus values contained in care_site_type_source_value . Internally, the dataset is returned by calling the function get_care_site_hierarchy() : from eds_scikit.resources import registry df = registry . get ( \"data\" , function_name = \"get_care_site_hierarchy\" )()","title":"Structure and usage"},{"location":"datasets/care-site-hierarchy/#use-your-own-data","text":"It is as simple as registering a new loading function: custom_resources.py from eds_scikit.resources import registry # (1) @registry . data ( \"get_care_site_hierarchy\" ) # (2) def get_care_site_hierarchy (): \"\"\" Your code here \"\"\" return df The registry instance stores user-defined functions Using this decorator allows to register the function when importing the corresponding file Then simply import your custom_resources module before running eds-scikit's pipelines, and you're good to go.","title":"Use your own data."},{"location":"datasets/care-site-hierarchy/#structure-and-usage_1","text":"Internally, the dataset is returned by calling the function get_care_site_hierarchy() . It should return a Pandas Dataframe with the following columns: care_site_id (OMOP column): The identifier of the care site care_site_type_source_value (OMOP column): The type of care site Additionally, it can contains an arbitrary number of columns whose name are values from care_site_type_source_value , and whose values are care_site_id of the corresponding parent structure","title":"Structure and usage"},{"location":"datasets/care-site-hierarchy/#generation-function","text":"You can generate the dataset on your specific data using this function","title":"Generation function"},{"location":"datasets/concepts-sets/","text":"Concepts-sets A concepts-set is a generic concept that has been deemed appropriate for most biological analyses. It is a group of several biological concepts representing the same biological entity. Concepts-sets This dataset is listing common biological entities in AP-HP's Data Warehouse. Below, one can see the list of default concepts-set provided by the library. Preview concepts_set_name GLIMS_ANABIO_concept_code concepts_set_category ALAT_Activity ['A0002', 'G1804', 'J7373', 'E2067', 'F2629'] hepatic_panel ASAT_Activity ['A0022', 'G1800', 'E2068', 'F2628'] hepatic_panel Activated_Partial_Thromboplastin_Time ['A1792', 'L7286', 'A7748'] coagulation Adenovirus ['I5915', 'I7952', 'J2229', 'J2988', 'J8817', 'K1527', 'J9968'] virology Albumine_Blood_Concentration ['D2358', 'C6841', 'C2102', 'G6616', 'L2260', 'A0006', 'E4799', 'I2013'] proteins B-HCG_Blood_Concentration ['A7426', 'F2353', 'A0164', 'L2277'] diabete B.pertussis ['I7748', 'I7968', 'K1531'] virology BNP_Concentration ['C8189', 'B5596', 'A2128'] cardiac_biomarkers BNP_and_NTProBNP_Concentration ['C8189', 'B5596', 'A2128', 'A7333', 'J7267', 'J7959'] cardiac_biomarkers Bicarbonate_Blood_Concentration ['A0422', 'H9622', 'C6408', 'F4161', 'A2136', 'J7371', 'G2031'] ionogram C.pneumoniae ['I5930', 'I7969', 'J2207', 'K1530', 'J9919'] virology CRP_Concentration ['A0248', 'E6332', 'F5581', 'J7381', 'F2631'] inflammatory_panel Calcium_Blood_Concentration ['C0543', 'D2359', 'A0038', 'H5041', 'F2625', 'L5047', 'A2512', 'A2512', 'A0607'] ionogram Chloride_Blood_Concentration ['A0079', 'J1179', 'F2619'] ionogram Coronavirus ['I5916', 'I5917', 'I5918', 'I5919', 'I7953', 'I7954', 'I7955', 'I7956', 'J2199', 'J2996', 'J2994', 'J2993', 'J2992', 'J8829', 'J8828', 'J8823', 'J8822', 'K1525', 'K1524', 'K1522', 'K1523', 'J9969'] virology Creatine Kinase ['A0090', 'G0171', 'E6330'] other D-Dimers_Concentration ['C7882', 'C7882', 'I8765', 'A0124', 'C0474', 'C0474', 'C0474', 'B4199', 'F5402'] coagulation EPP_Blood_Concentration ['A0250', 'C9874', 'A3758', 'A0004', 'F9978', 'A0005', 'H8137', 'C7087', 'A0003', 'C7088', 'B9456', 'B9455', 'A0008', 'H8138', 'C7089', 'A0007', 'C7090', 'A0010', 'H8139', 'C7091', 'A0009', 'C7092', 'C6525', 'C6524', 'A0415', 'C7093', 'A0414', 'C7094', 'B9458', 'B9457', 'A2113', 'H8140', 'E5327', 'A2112', 'E5328', 'C6536', 'C6535', 'E2398', 'E2399', 'A2115', 'H8141', 'E5329', 'A2114', 'E5330', 'C6538', 'C6537', 'E2400', 'E2401', 'A0130', 'H8142', 'C7100', 'A0129', 'C7101', 'G6942', 'G6941', 'B9460', 'B9459', 'C6596', 'C6595', 'C6598', 'C6597', 'E2402', 'E2403', 'K4483', 'A2118', 'A2117', 'E1847', 'B8047', 'A2127', 'I8076', 'A2126', 'I8077', 'X5093', 'X5094', 'X5091', 'X5092', 'A2279', 'L7258', 'A1361', 'L7259', 'C6909', 'B1727', 'B1725', 'C6924', 'I5139', 'X5097', 'X5098', 'X5095', 'X5096', 'A2278', 'L7260', 'A2277', 'L7261', 'D0265', 'B1728', 'B1726', 'D0267', 'I5140', 'X5101', 'X5102', 'X5099', 'X5100', 'H6397', 'L7262', 'H6396', 'L7263', 'D0266', 'C0616', 'D0268', 'I5141', 'A8775', 'A7816', 'A8776', 'A8777', 'A8778', 'A8779', 'A8780', 'A7330', 'F0748', 'F0749', 'H9656', 'H9657', 'H9658', 'H9659', 'H9660'] proteins Eosinophil_Polymorphonuclears_Blood_Count ['A0150', 'H6730'] blood_count_cell Ferritin_Concentration ['A0123', 'E9865'] martial_panel Fibrinogen_Concentration ['A0126'] inflammatory_panel Glomerular_Filtration_Rate_EPI_CKD ['G6921', 'F8160', 'F9613', 'F9621', 'F9621'] renal_panel Glomerular_Filtration_Rate_MDRD ['F9622', 'G7835', 'B9964', 'A7456', 'A7455', 'H5609'] renal_panel GGT_Activity ['A0131', 'F8184', 'E9771', 'J7370', 'K7045'] hepatic_panel Glucose_Blood_Concentration ['A0141', 'H7323', 'J7401', 'F2622', 'B9553', 'C7236', 'E7312', 'A7338', 'H7324', 'C0565', 'E9889', 'A8424'] diabete HCO3-_Blood_Concentration ['A0420', 'L5018'] blood_gas HIV Serology ['D2865', 'D2867', 'D2845', 'D2864', 'F4252', 'D2866', 'F3257', 'D2869', 'F1705', 'D2846', 'D2847', 'D2844', 'E8605', 'F5401', 'G0175', 'J5891', 'J2672', 'H7667'] serology HbA1c_Blood_% ['B6983', 'A2228', 'A1271', 'E6632', 'I5968'] diabete Hemoglobin_Blood_Count ['A0163', 'H6738'] blood_count_cell Hepatitis B Serology ['D2729', 'D2730', 'E1524', 'F2075', 'D2725', 'I1903', 'D2728', 'D2726', 'D2726', 'F5613', 'D2731', 'D2727', 'G1197', 'G1199', 'G1199', 'J5887', 'J5890', 'J2697', 'L6883', 'J2695', 'L7877', 'D2653', 'D2660', 'D2654', 'D2661', 'D2649', 'D2656', 'H9686', 'H9687', 'I6579', 'I6580', 'D2652', 'D2659', 'D2650', 'D2650', 'D2657', 'D2655', 'D2662', 'D2651', 'D2651', 'D2658', 'F5615', 'F5616', 'G1209', 'G1209', 'G1210', 'G1210', 'I2927', 'I2928', 'J5885', 'J5886', 'J2699', 'J2700'] serology Hepatitis C Serology ['D2780', 'H5078', 'I1846', 'D2777', 'E6503', 'D2778', 'D3088', 'D3088', 'D2774', 'J1518', 'D2776', 'D2771', 'E8372', 'F7465', 'I3151', 'G0173', 'L1237', 'J2678', 'K3833', 'E8373', 'K4228'] serology IL-1 beta_Blood_Concentration ['C9351', 'B8921', 'G4800', 'K3662', 'L2217', 'J9193', 'K3665', 'K3687', 'K3661', 'L2197'] inflammatory_biomarkers IL-10_Blood_Concentration ['B8922', 'C8763', 'K3478', 'L2210', 'J9187', 'K3481', 'K3472', 'K3475', 'L2198'] inflammatory_biomarkers IL-6_Blood_Concentration ['B8929', 'G4799', 'B1910', 'K3467', 'L2205', 'E6992', 'J9190', 'K3456', 'L2193', 'K3435', 'K3460'] inflammatory_biomarkers Quick_INR_Time ['A0269'] coagulation Influenza A ['I5922', 'I7960', 'J2198', 'J2990', 'J8825', 'K1517'] virology Influenza B ['I5923', 'I7961', 'J2203', 'J2985', 'J8819', 'K1513'] virology L.pneumophila ['J2176', 'J3006', 'J8826', 'J9899'] virology LDH ['A0170', 'H5261', 'J7400', 'C8889', 'J1161'] other Lactate_Gaz_Blood_Concentration ['C8697', 'H7748'] blood_gas Legionella Antigenuria ['D1465', 'H6694', 'J7960'] antigenury Leukocytes_Blood_Count ['A0174', 'H6740', 'C8824'] blood_count_cell Lymphocytes_Blood_Count ['A0198', 'H6743'] blood_count_cell Metapneumovirus ['I5920', 'I7958', 'J2200', 'J2991', 'J8824', 'K1519', 'J9965'] virology Monocytes_Blood_Count ['A0210', 'H6747'] blood_count_cell NTProBNP_Concentration ['A7333', 'J7267', 'J7959'] cardiac_biomarkers Neutrophil_Polymorphonuclears_Blood_Count ['A0155', 'H6732'] blood_count_cell PAL_Activity ['A0227', 'F8187', 'E6331', 'F1844'] hepatic_panel PaCO2_Blood_Concentration ['A7305', 'A0630'] blood_gas PaO2_Blood_Concentration ['A7319', 'H7747'] blood_gas Parainfluenza ['I5924', 'I5925', 'I5926', 'I5927', 'I7962', 'I7963', 'I7964', 'I7965', 'J2204', 'J2979', 'J2977', 'J2983', 'J2981', 'J8861', 'J8862', 'J8821', 'J8820', 'K1509', 'K1508', 'K1510', 'K1535', 'J9964'] virology Phosphates_Blood_Concentration ['A0226', 'F8186', 'F2626'] proteins Platelets_Blood_Count ['A0230', 'H6751', 'A1598', 'A1598', 'A2538', 'A2539', 'A2539', 'J4463'] blood_count_cell Pneumococcal Antigenuria ['D2055', 'J7962', 'A2804'] antigenury Potassium_Blood_Concentration ['A2380', 'E2073', 'F2618', 'E2337', 'J1178'] ionogram Procalcitonin_Blood_Concentration ['A1661', 'H5267', 'F2632'] inflammatory_biomarkers Proteins_Urine_24h_Concentration ['A1695', 'A1694', 'A1696', 'C9990', 'C9991', 'J7268', 'J7269', 'C3941', 'E4745', 'G4187', 'F6060'] proteins Proteins_Blood_Concentration ['A7347', 'F5122', 'F2624', 'B9417', 'A0249', 'B3990'] inflammatory_panel Quick_Prothrombin_Time ['A1805', 'E9993'] coagulation RSV ['I5928', 'I7966', 'J2201', 'J2974', 'J8859', 'K1534'] virology Rhino/Enterovirus ['I5921', 'I7959', 'J2197', 'J2973', 'J8858', 'K1515'] virology SARS-CoV-2 ['K1108', 'J9791', 'J8706', 'J8827', 'K1520'] virology SaO2_Blood_Concentration ['A7334', 'L5021'] blood_gas Sodium_Blood_Concentration ['A0262', 'J1177', 'F8162', 'F2617'] ionogram TNF alpha_Blood_Concentration ['B8931', 'G4801', 'C9393', 'K3505', 'L2203', 'E6993', 'J9194', 'K3658', 'K3502', 'K3504', 'L2191'] inflammatory_biomarkers TSH_Concentration ['A1831', 'F2150', 'I8385', 'C2666'] diabete Total_Bilirubin_Concentration ['A0029', 'H5264', 'D0189'] hepatic_panel Transferrin_Saturation_Coefficient ['A0278'] martial_panel Troponine_Concentration ['A0283', 'C5560', 'F9934', 'E6954', 'L3534', 'G7716', 'J5184', 'A3832', 'E7249'] cardiac_biomarkers Urea_Blood_Concentration ['A0286', 'G3350', 'J7372', 'F2620'] renal_panel Venous_Lactate ['A0173', 'B9146', 'A9995'] other pH_Blood ['A0221', 'L5017', 'A0219'] blood_gas Link You can see the dataset here Note The concept codes are expressed in the AnaBio and LOINC standard vocabularies (for more information about the vocabularies see the Vocabulary page).","title":"Concept sets"},{"location":"datasets/concepts-sets/#concepts-sets","text":"A concepts-set is a generic concept that has been deemed appropriate for most biological analyses. It is a group of several biological concepts representing the same biological entity. Concepts-sets This dataset is listing common biological entities in AP-HP's Data Warehouse. Below, one can see the list of default concepts-set provided by the library.","title":"Concepts-sets"},{"location":"datasets/concepts-sets/#preview","text":"concepts_set_name GLIMS_ANABIO_concept_code concepts_set_category ALAT_Activity ['A0002', 'G1804', 'J7373', 'E2067', 'F2629'] hepatic_panel ASAT_Activity ['A0022', 'G1800', 'E2068', 'F2628'] hepatic_panel Activated_Partial_Thromboplastin_Time ['A1792', 'L7286', 'A7748'] coagulation Adenovirus ['I5915', 'I7952', 'J2229', 'J2988', 'J8817', 'K1527', 'J9968'] virology Albumine_Blood_Concentration ['D2358', 'C6841', 'C2102', 'G6616', 'L2260', 'A0006', 'E4799', 'I2013'] proteins B-HCG_Blood_Concentration ['A7426', 'F2353', 'A0164', 'L2277'] diabete B.pertussis ['I7748', 'I7968', 'K1531'] virology BNP_Concentration ['C8189', 'B5596', 'A2128'] cardiac_biomarkers BNP_and_NTProBNP_Concentration ['C8189', 'B5596', 'A2128', 'A7333', 'J7267', 'J7959'] cardiac_biomarkers Bicarbonate_Blood_Concentration ['A0422', 'H9622', 'C6408', 'F4161', 'A2136', 'J7371', 'G2031'] ionogram C.pneumoniae ['I5930', 'I7969', 'J2207', 'K1530', 'J9919'] virology CRP_Concentration ['A0248', 'E6332', 'F5581', 'J7381', 'F2631'] inflammatory_panel Calcium_Blood_Concentration ['C0543', 'D2359', 'A0038', 'H5041', 'F2625', 'L5047', 'A2512', 'A2512', 'A0607'] ionogram Chloride_Blood_Concentration ['A0079', 'J1179', 'F2619'] ionogram Coronavirus ['I5916', 'I5917', 'I5918', 'I5919', 'I7953', 'I7954', 'I7955', 'I7956', 'J2199', 'J2996', 'J2994', 'J2993', 'J2992', 'J8829', 'J8828', 'J8823', 'J8822', 'K1525', 'K1524', 'K1522', 'K1523', 'J9969'] virology Creatine Kinase ['A0090', 'G0171', 'E6330'] other D-Dimers_Concentration ['C7882', 'C7882', 'I8765', 'A0124', 'C0474', 'C0474', 'C0474', 'B4199', 'F5402'] coagulation EPP_Blood_Concentration ['A0250', 'C9874', 'A3758', 'A0004', 'F9978', 'A0005', 'H8137', 'C7087', 'A0003', 'C7088', 'B9456', 'B9455', 'A0008', 'H8138', 'C7089', 'A0007', 'C7090', 'A0010', 'H8139', 'C7091', 'A0009', 'C7092', 'C6525', 'C6524', 'A0415', 'C7093', 'A0414', 'C7094', 'B9458', 'B9457', 'A2113', 'H8140', 'E5327', 'A2112', 'E5328', 'C6536', 'C6535', 'E2398', 'E2399', 'A2115', 'H8141', 'E5329', 'A2114', 'E5330', 'C6538', 'C6537', 'E2400', 'E2401', 'A0130', 'H8142', 'C7100', 'A0129', 'C7101', 'G6942', 'G6941', 'B9460', 'B9459', 'C6596', 'C6595', 'C6598', 'C6597', 'E2402', 'E2403', 'K4483', 'A2118', 'A2117', 'E1847', 'B8047', 'A2127', 'I8076', 'A2126', 'I8077', 'X5093', 'X5094', 'X5091', 'X5092', 'A2279', 'L7258', 'A1361', 'L7259', 'C6909', 'B1727', 'B1725', 'C6924', 'I5139', 'X5097', 'X5098', 'X5095', 'X5096', 'A2278', 'L7260', 'A2277', 'L7261', 'D0265', 'B1728', 'B1726', 'D0267', 'I5140', 'X5101', 'X5102', 'X5099', 'X5100', 'H6397', 'L7262', 'H6396', 'L7263', 'D0266', 'C0616', 'D0268', 'I5141', 'A8775', 'A7816', 'A8776', 'A8777', 'A8778', 'A8779', 'A8780', 'A7330', 'F0748', 'F0749', 'H9656', 'H9657', 'H9658', 'H9659', 'H9660'] proteins Eosinophil_Polymorphonuclears_Blood_Count ['A0150', 'H6730'] blood_count_cell Ferritin_Concentration ['A0123', 'E9865'] martial_panel Fibrinogen_Concentration ['A0126'] inflammatory_panel Glomerular_Filtration_Rate_EPI_CKD ['G6921', 'F8160', 'F9613', 'F9621', 'F9621'] renal_panel Glomerular_Filtration_Rate_MDRD ['F9622', 'G7835', 'B9964', 'A7456', 'A7455', 'H5609'] renal_panel GGT_Activity ['A0131', 'F8184', 'E9771', 'J7370', 'K7045'] hepatic_panel Glucose_Blood_Concentration ['A0141', 'H7323', 'J7401', 'F2622', 'B9553', 'C7236', 'E7312', 'A7338', 'H7324', 'C0565', 'E9889', 'A8424'] diabete HCO3-_Blood_Concentration ['A0420', 'L5018'] blood_gas HIV Serology ['D2865', 'D2867', 'D2845', 'D2864', 'F4252', 'D2866', 'F3257', 'D2869', 'F1705', 'D2846', 'D2847', 'D2844', 'E8605', 'F5401', 'G0175', 'J5891', 'J2672', 'H7667'] serology HbA1c_Blood_% ['B6983', 'A2228', 'A1271', 'E6632', 'I5968'] diabete Hemoglobin_Blood_Count ['A0163', 'H6738'] blood_count_cell Hepatitis B Serology ['D2729', 'D2730', 'E1524', 'F2075', 'D2725', 'I1903', 'D2728', 'D2726', 'D2726', 'F5613', 'D2731', 'D2727', 'G1197', 'G1199', 'G1199', 'J5887', 'J5890', 'J2697', 'L6883', 'J2695', 'L7877', 'D2653', 'D2660', 'D2654', 'D2661', 'D2649', 'D2656', 'H9686', 'H9687', 'I6579', 'I6580', 'D2652', 'D2659', 'D2650', 'D2650', 'D2657', 'D2655', 'D2662', 'D2651', 'D2651', 'D2658', 'F5615', 'F5616', 'G1209', 'G1209', 'G1210', 'G1210', 'I2927', 'I2928', 'J5885', 'J5886', 'J2699', 'J2700'] serology Hepatitis C Serology ['D2780', 'H5078', 'I1846', 'D2777', 'E6503', 'D2778', 'D3088', 'D3088', 'D2774', 'J1518', 'D2776', 'D2771', 'E8372', 'F7465', 'I3151', 'G0173', 'L1237', 'J2678', 'K3833', 'E8373', 'K4228'] serology IL-1 beta_Blood_Concentration ['C9351', 'B8921', 'G4800', 'K3662', 'L2217', 'J9193', 'K3665', 'K3687', 'K3661', 'L2197'] inflammatory_biomarkers IL-10_Blood_Concentration ['B8922', 'C8763', 'K3478', 'L2210', 'J9187', 'K3481', 'K3472', 'K3475', 'L2198'] inflammatory_biomarkers IL-6_Blood_Concentration ['B8929', 'G4799', 'B1910', 'K3467', 'L2205', 'E6992', 'J9190', 'K3456', 'L2193', 'K3435', 'K3460'] inflammatory_biomarkers Quick_INR_Time ['A0269'] coagulation Influenza A ['I5922', 'I7960', 'J2198', 'J2990', 'J8825', 'K1517'] virology Influenza B ['I5923', 'I7961', 'J2203', 'J2985', 'J8819', 'K1513'] virology L.pneumophila ['J2176', 'J3006', 'J8826', 'J9899'] virology LDH ['A0170', 'H5261', 'J7400', 'C8889', 'J1161'] other Lactate_Gaz_Blood_Concentration ['C8697', 'H7748'] blood_gas Legionella Antigenuria ['D1465', 'H6694', 'J7960'] antigenury Leukocytes_Blood_Count ['A0174', 'H6740', 'C8824'] blood_count_cell Lymphocytes_Blood_Count ['A0198', 'H6743'] blood_count_cell Metapneumovirus ['I5920', 'I7958', 'J2200', 'J2991', 'J8824', 'K1519', 'J9965'] virology Monocytes_Blood_Count ['A0210', 'H6747'] blood_count_cell NTProBNP_Concentration ['A7333', 'J7267', 'J7959'] cardiac_biomarkers Neutrophil_Polymorphonuclears_Blood_Count ['A0155', 'H6732'] blood_count_cell PAL_Activity ['A0227', 'F8187', 'E6331', 'F1844'] hepatic_panel PaCO2_Blood_Concentration ['A7305', 'A0630'] blood_gas PaO2_Blood_Concentration ['A7319', 'H7747'] blood_gas Parainfluenza ['I5924', 'I5925', 'I5926', 'I5927', 'I7962', 'I7963', 'I7964', 'I7965', 'J2204', 'J2979', 'J2977', 'J2983', 'J2981', 'J8861', 'J8862', 'J8821', 'J8820', 'K1509', 'K1508', 'K1510', 'K1535', 'J9964'] virology Phosphates_Blood_Concentration ['A0226', 'F8186', 'F2626'] proteins Platelets_Blood_Count ['A0230', 'H6751', 'A1598', 'A1598', 'A2538', 'A2539', 'A2539', 'J4463'] blood_count_cell Pneumococcal Antigenuria ['D2055', 'J7962', 'A2804'] antigenury Potassium_Blood_Concentration ['A2380', 'E2073', 'F2618', 'E2337', 'J1178'] ionogram Procalcitonin_Blood_Concentration ['A1661', 'H5267', 'F2632'] inflammatory_biomarkers Proteins_Urine_24h_Concentration ['A1695', 'A1694', 'A1696', 'C9990', 'C9991', 'J7268', 'J7269', 'C3941', 'E4745', 'G4187', 'F6060'] proteins Proteins_Blood_Concentration ['A7347', 'F5122', 'F2624', 'B9417', 'A0249', 'B3990'] inflammatory_panel Quick_Prothrombin_Time ['A1805', 'E9993'] coagulation RSV ['I5928', 'I7966', 'J2201', 'J2974', 'J8859', 'K1534'] virology Rhino/Enterovirus ['I5921', 'I7959', 'J2197', 'J2973', 'J8858', 'K1515'] virology SARS-CoV-2 ['K1108', 'J9791', 'J8706', 'J8827', 'K1520'] virology SaO2_Blood_Concentration ['A7334', 'L5021'] blood_gas Sodium_Blood_Concentration ['A0262', 'J1177', 'F8162', 'F2617'] ionogram TNF alpha_Blood_Concentration ['B8931', 'G4801', 'C9393', 'K3505', 'L2203', 'E6993', 'J9194', 'K3658', 'K3502', 'K3504', 'L2191'] inflammatory_biomarkers TSH_Concentration ['A1831', 'F2150', 'I8385', 'C2666'] diabete Total_Bilirubin_Concentration ['A0029', 'H5264', 'D0189'] hepatic_panel Transferrin_Saturation_Coefficient ['A0278'] martial_panel Troponine_Concentration ['A0283', 'C5560', 'F9934', 'E6954', 'L3534', 'G7716', 'J5184', 'A3832', 'E7249'] cardiac_biomarkers Urea_Blood_Concentration ['A0286', 'G3350', 'J7372', 'F2620'] renal_panel Venous_Lactate ['A0173', 'B9146', 'A9995'] other pH_Blood ['A0221', 'L5017', 'A0219'] blood_gas","title":"Preview"},{"location":"datasets/concepts-sets/#link","text":"You can see the dataset here Note The concept codes are expressed in the AnaBio and LOINC standard vocabularies (for more information about the vocabularies see the Vocabulary page).","title":"Link"},{"location":"datasets/private-resources/","text":"Resources eds-scikit contains some resources that are stored as is , either because it comes from manual work, or because its generation might be time and computationally intensive. Private data A lot of those resources are specific to AP-HP's CDW, thus are stored on a private repository. If you work on AP-HP's ecosystem, you can install those resources along with eds-scikit via pip install 'eds_scikit[aphp]' . For each resource listed bellow, you will find: A short description If relevant, a way to register your function in order to use your own data If relevant, a link to the generation function Available resources Care site hierarchy Care site emergency Default concepts-sets for Biology Default configuration for Biology","title":"A note on private resources"},{"location":"datasets/private-resources/#resources","text":"eds-scikit contains some resources that are stored as is , either because it comes from manual work, or because its generation might be time and computationally intensive. Private data A lot of those resources are specific to AP-HP's CDW, thus are stored on a private repository. If you work on AP-HP's ecosystem, you can install those resources along with eds-scikit via pip install 'eds_scikit[aphp]' . For each resource listed bellow, you will find: A short description If relevant, a way to register your function in order to use your own data If relevant, a link to the generation function","title":"Resources"},{"location":"datasets/private-resources/#available-resources","text":"Care site hierarchy Care site emergency Default concepts-sets for Biology Default configuration for Biology","title":"Available resources"},{"location":"datasets/synthetic-data/","text":"Small Datasets for testing functionalities Presentation eds-scikit was build to work seamlessly on a pre-existing OMOP database. However, the library also provides some toy datasets so that you can try out some features even without having access to a database. Usage First, you can display all availables synthetic datasets: from eds_scikit import datasets datasets . list_all_synthetics () # Out: ['load_ccam', 'load_consultation_dates', 'load_hierarchy', 'load_icd10', 'load_visit_merging', 'load_stay_duration', 'load_suicide_attempt', 'load_tagging', 'load_biology_data', 'load_event_sequences'] To load a specific dataset, simply run: data = datasets . load_icd10 () data # Out: ICD10Dataset(condition_occurrence, visit_occurrence) The data object is similar to objects available in eds_scikit.io , namely: HiveData PostgresData PandasData For instance, tables are available as attributes: data . condition_occurrence | | person_id | condition_source_value | condition_start_datetime | condition_status_source_value | visit_occurrence_id | |---|-----------|------------------------|--------------------------|-------------------------------|---------------------| | 0 | 1 | C10 | 2010-01-01 | DP | 11 | | 1 | 1 | E112 | 2010-01-01 | DAS | 12 | | 2 | 1 | D20 | 2012-01-01 | DAS | 13 | | 3 | 1 | A20 | 2020-01-01 | DP | 14 | | 4 | 1 | A21 | 2000-01-01 | DP | 15 | | 5 | 1 | X20 | 2000-01-01 | DP | 16 | | 6 | 1 | C10 | 2010-01-01 | DP | 16 | | 7 | 1 | C10 | 2010-01-01 | DP | 17 | As shown in the tutorial , you can now try out the corresponding conditions_from_icd10() function.","title":"Synthetic data"},{"location":"datasets/synthetic-data/#small-datasets-for-testing-functionalities","text":"","title":"Small Datasets for testing functionalities"},{"location":"datasets/synthetic-data/#presentation","text":"eds-scikit was build to work seamlessly on a pre-existing OMOP database. However, the library also provides some toy datasets so that you can try out some features even without having access to a database.","title":"Presentation"},{"location":"datasets/synthetic-data/#usage","text":"First, you can display all availables synthetic datasets: from eds_scikit import datasets datasets . list_all_synthetics () # Out: ['load_ccam', 'load_consultation_dates', 'load_hierarchy', 'load_icd10', 'load_visit_merging', 'load_stay_duration', 'load_suicide_attempt', 'load_tagging', 'load_biology_data', 'load_event_sequences'] To load a specific dataset, simply run: data = datasets . load_icd10 () data # Out: ICD10Dataset(condition_occurrence, visit_occurrence) The data object is similar to objects available in eds_scikit.io , namely: HiveData PostgresData PandasData For instance, tables are available as attributes: data . condition_occurrence | | person_id | condition_source_value | condition_start_datetime | condition_status_source_value | visit_occurrence_id | |---|-----------|------------------------|--------------------------|-------------------------------|---------------------| | 0 | 1 | C10 | 2010-01-01 | DP | 11 | | 1 | 1 | E112 | 2010-01-01 | DAS | 12 | | 2 | 1 | D20 | 2012-01-01 | DAS | 13 | | 3 | 1 | A20 | 2020-01-01 | DP | 14 | | 4 | 1 | A21 | 2000-01-01 | DP | 15 | | 5 | 1 | X20 | 2000-01-01 | DP | 16 | | 6 | 1 | C10 | 2010-01-01 | DP | 16 | | 7 | 1 | C10 | 2010-01-01 | DP | 17 | As shown in the tutorial , you can now try out the corresponding conditions_from_icd10() function.","title":"Usage"},{"location":"functionalities/biology/","text":"Biology The biology module of eds-scikit supports data scientists working on biological data. Its main objectives are to: Provide predefined biology concept sets based on AP-HP coding system Facilitate codes mapping between different terminologies and referentials Provide data visualization tools and statistic summary Allows automatic units conversion from heterogenous units system Quick use Simple mapping of measurement table codes to ANABIO terminology. Visualizing measurements Useful measurement visualizations. Detailed use Preparing measurements workflow : codes mapping, units conversion, outliers detection. Terminologies relationships Explore concept codes relationships between different terminologies. Predefined concept sets Explore predefined concept sets. About measurement table Knowledge about measurement table.","title":"Biology"},{"location":"functionalities/biology/#biology","text":"The biology module of eds-scikit supports data scientists working on biological data. Its main objectives are to: Provide predefined biology concept sets based on AP-HP coding system Facilitate codes mapping between different terminologies and referentials Provide data visualization tools and statistic summary Allows automatic units conversion from heterogenous units system Quick use Simple mapping of measurement table codes to ANABIO terminology. Visualizing measurements Useful measurement visualizations. Detailed use Preparing measurements workflow : codes mapping, units conversion, outliers detection. Terminologies relationships Explore concept codes relationships between different terminologies. Predefined concept sets Explore predefined concept sets. About measurement table Knowledge about measurement table.","title":"Biology"},{"location":"functionalities/biology/about_measurement/","text":"About measurements table The BioClean module focuses on three OMOP terms: Measurement is a record obtained through the standardized testing or examination of a person or person's sample. Concept is a semantic notion that uniquely identify a clinical event. It can group several measurements. Concept Relationship is a semantic relation between terminologies, allowing to map codes from different terminologies. A fourht term was created to ease the use of the two above: concepts-set is a generic concept that has been deemed appropriate for most biological analyses. It is a group of several biological concepts representing the same biological entity. Example: Let's imagine the laboratory X tests the creatinine of Mister A and Mister B in mg/dL and the laboratory Y tests the creatinine of Mister C in \u00b5mol/L. In this context, the dataset will contain: 3 measurements (one for each conducted test) 2 concepts (one concept for the creatinine tested in mg/dL and another one for the creatinine tested in \u00b5mol/L) 1 concepts-set (it groups the 2 concepts because they are the same biological entity) Vocabulary A vocabulary is a terminology system that associates a code to a specific clinical event. One may distinguish two types of vocabularies: Source vocabulary The source vocabulary is the vocabulary used in the LIMS (Laboratory Information Management System) software. It is specific to the LIMS and may be different in each laboratory. Standard vocabulary The standard vocabulary is a unified vocabulary that allows data analysis on a larger scale. It is classified in chapter. It has a bigger granularity than the source vocabulary, multiple source codes may be associated to one standard code. Vocabulary flowchart in OMOP","title":"About measurement"},{"location":"functionalities/biology/about_measurement/#about-measurements-table","text":"The BioClean module focuses on three OMOP terms: Measurement is a record obtained through the standardized testing or examination of a person or person's sample. Concept is a semantic notion that uniquely identify a clinical event. It can group several measurements. Concept Relationship is a semantic relation between terminologies, allowing to map codes from different terminologies. A fourht term was created to ease the use of the two above: concepts-set is a generic concept that has been deemed appropriate for most biological analyses. It is a group of several biological concepts representing the same biological entity. Example: Let's imagine the laboratory X tests the creatinine of Mister A and Mister B in mg/dL and the laboratory Y tests the creatinine of Mister C in \u00b5mol/L. In this context, the dataset will contain: 3 measurements (one for each conducted test) 2 concepts (one concept for the creatinine tested in mg/dL and another one for the creatinine tested in \u00b5mol/L) 1 concepts-set (it groups the 2 concepts because they are the same biological entity)","title":"About measurements table"},{"location":"functionalities/biology/about_measurement/#vocabulary","text":"A vocabulary is a terminology system that associates a code to a specific clinical event. One may distinguish two types of vocabularies:","title":"Vocabulary"},{"location":"functionalities/biology/about_measurement/#source-vocabulary","text":"The source vocabulary is the vocabulary used in the LIMS (Laboratory Information Management System) software. It is specific to the LIMS and may be different in each laboratory.","title":"Source vocabulary"},{"location":"functionalities/biology/about_measurement/#standard-vocabulary","text":"The standard vocabulary is a unified vocabulary that allows data analysis on a larger scale. It is classified in chapter. It has a bigger granularity than the source vocabulary, multiple source codes may be associated to one standard code.","title":"Standard vocabulary"},{"location":"functionalities/biology/about_measurement/#vocabulary-flowchart-in-omop","text":"","title":"Vocabulary flowchart in OMOP"},{"location":"functionalities/biology/concepts_sets/","text":"Predefined concepts sets Those concept sets can be found in eds_scikit.datasets.default_concepts_sets . How were the code selected ? Each concept set codes were selected by coding system expert and validated through statistical analysis. However new codes may appear or become outdated. Feel free to propose new concept sets or concept codes ! Blood Count Cell Ionogram Blood Gas Hepatic Panel Cardiac Biomarkers Inflammatory Panel Martial Panel Renal Panel Coagulation Proteins Diabete Inflammatory Biomarkers concepts_set_name GLIMS_ANABIO_concept_code Hemoglobin_Blood_Count ['A0163', 'H6738'] Leukocytes_Blood_Count ['A0174', 'H6740', 'C8824'] Platelets_Blood_Count ['A0230', 'H6751', 'A1598', 'A1598', 'A2538', 'A2539', 'A2539', 'J4463'] Lymphocytes_Blood_Count ['A0198', 'H6743'] Monocytes_Blood_Count ['A0210', 'H6747'] Neutrophil_Polymorphonuclears_Blood_Count ['A0155', 'H6732'] Eosinophil_Polymorphonuclears_Blood_Count ['A0150', 'H6730'] concepts_set_name GLIMS_ANABIO_concept_code Bicarbonate_Blood_Concentration ['A0422', 'H9622', 'C6408', 'F4161', 'A2136', 'J7371', 'G2031'] Calcium_Blood_Concentration ['C0543', 'D2359', 'A0038', 'H5041', 'F2625', 'L5047', 'A2512', 'A2512', 'A0607'] Chloride_Blood_Concentration ['A0079', 'J1179', 'F2619'] Potassium_Blood_Concentration ['A2380', 'E2073', 'F2618', 'E2337', 'J1178'] Sodium_Blood_Concentration ['A0262', 'J1177', 'F8162', 'F2617'] concepts_set_name GLIMS_ANABIO_concept_code HCO3-_Blood_Concentration ['A0420', 'L5018'] Lactate_Gaz_Blood_Concentration ['C8697', 'H7748'] PaCO2_Blood_Concentration ['A7305', 'A0630'] PaO2_Blood_Concentration ['A7319', 'H7747'] SaO2_Blood_Concentration ['A7334', 'L5021'] pH_Blood ['A0221', 'L5017', 'A0219'] concepts_set_name GLIMS_ANABIO_concept_code ALAT_Activity ['A0002', 'G1804', 'J7373', 'E2067', 'F2629'] ASAT_Activity ['A0022', 'G1800', 'E2068', 'F2628'] GGT_Activity ['A0131', 'F8184', 'E9771', 'J7370', 'K7045'] PAL_Activity ['A0227', 'F8187', 'E6331', 'F1844'] Total_Bilirubin_Concentration ['A0029', 'H5264', 'D0189'] concepts_set_name GLIMS_ANABIO_concept_code BNP_Concentration ['C8189', 'B5596', 'A2128'] BNP_and_NTProBNP_Concentration ['C8189', 'B5596', 'A2128', 'A7333', 'J7267', 'J7959'] NTProBNP_Concentration ['A7333', 'J7267', 'J7959'] Troponine_Concentration ['A0283', 'C5560', 'F9934', 'E6954', 'L3534', 'G7716', 'J5184', 'A3832', 'E7249'] concepts_set_name GLIMS_ANABIO_concept_code CRP_Concentration ['A0248', 'E6332', 'F5581', 'J7381', 'F2631'] Fibrinogen_Concentration ['A0126'] Proteins_Blood_Concentration ['A7347', 'F5122', 'F2624', 'B9417', 'A0249', 'B3990'] concepts_set_name GLIMS_ANABIO_concept_code Ferritin_Concentration ['A0123', 'E9865'] Transferrin_Saturation_Coefficient ['A0278'] concepts_set_name GLIMS_ANABIO_concept_code Glomerular_Filtration_Rate_EPI_CKD ['G6921', 'F8160', 'F9613', 'F9621', 'F9621'] Glomerular_Filtration_Rate_MDRD ['F9622', 'G7835', 'B9964', 'A7456', 'A7455', 'H5609'] Urea_Blood_Concentration ['A0286', 'G3350', 'J7372', 'F2620'] concepts_set_name GLIMS_ANABIO_concept_code Activated_Partial_Thromboplastin_Time ['A1792', 'L7286', 'A7748'] D-Dimers_Concentration ['C7882', 'C7882', 'I8765', 'A0124', 'C0474', 'C0474', 'C0474', 'B4199', 'F5402'] Quick_INR_Time ['A0269'] Quick_Prothrombin_Time ['A1805', 'E9993'] concepts_set_name GLIMS_ANABIO_concept_code Albumine_Blood_Concentration ['D2358', 'C6841', 'C2102', 'G6616', 'L2260', 'A0006', 'E4799', 'I2013'] EPP_Blood_Concentration ['A0250', 'C9874', 'A3758', 'A0004', 'F9978', 'A0005', 'H8137', 'C7087', 'A0003', 'C7088', 'B9456', 'B9455', 'A0008', 'H8138', 'C7089', 'A0007'... Phosphates_Blood_Concentration ['A0226', 'F8186', 'F2626'] Proteins_Urine_24h_Concentration ['A1695', 'A1694', 'A1696', 'C9990', 'C9991', 'J7268', 'J7269', 'C3941', 'E4745', 'G4187', 'F6060'] concepts_set_name GLIMS_ANABIO_concept_code B-HCG_Blood_Concentration ['A7426', 'F2353', 'A0164', 'L2277'] Glucose_Blood_Concentration ['A0141', 'H7323', 'J7401', 'F2622', 'B9553', 'C7236', 'E7312', 'A7338', 'H7324', 'C0565', 'E9889', 'A8424'] HbA1c_Blood_% ['B6983', 'A2228', 'A1271', 'E6632', 'I5968'] TSH_Concentration ['A1831', 'F2150', 'I8385', 'C2666'] concepts_set_name GLIMS_ANABIO_concept_code IL-1 beta_Blood_Concentration ['C9351', 'B8921', 'G4800', 'K3662', 'L2217', 'J9193', 'K3665', 'K3687', 'K3661', 'L2197'] IL-10_Blood_Concentration ['B8922', 'C8763', 'K3478', 'L2210', 'J9187', 'K3481', 'K3472', 'K3475', 'L2198'] IL-6_Blood_Concentration ['B8929', 'G4799', 'B1910', 'K3467', 'L2205', 'E6992', 'J9190', 'K3456', 'L2193', 'K3435', 'K3460'] Procalcitonin_Blood_Concentration ['A1661', 'H5267', 'F2632'] TNF alpha_Blood_Concentration ['B8931', 'G4801', 'C9393', 'K3505', 'L2203', 'E6993', 'J9194', 'K3658', 'K3502', 'K3504', 'L2191']","title":"Predefined concepts sets"},{"location":"functionalities/biology/concepts_sets/#predefined-concepts-sets","text":"Those concept sets can be found in eds_scikit.datasets.default_concepts_sets . How were the code selected ? Each concept set codes were selected by coding system expert and validated through statistical analysis. However new codes may appear or become outdated. Feel free to propose new concept sets or concept codes ! Blood Count Cell Ionogram Blood Gas Hepatic Panel Cardiac Biomarkers Inflammatory Panel Martial Panel Renal Panel Coagulation Proteins Diabete Inflammatory Biomarkers concepts_set_name GLIMS_ANABIO_concept_code Hemoglobin_Blood_Count ['A0163', 'H6738'] Leukocytes_Blood_Count ['A0174', 'H6740', 'C8824'] Platelets_Blood_Count ['A0230', 'H6751', 'A1598', 'A1598', 'A2538', 'A2539', 'A2539', 'J4463'] Lymphocytes_Blood_Count ['A0198', 'H6743'] Monocytes_Blood_Count ['A0210', 'H6747'] Neutrophil_Polymorphonuclears_Blood_Count ['A0155', 'H6732'] Eosinophil_Polymorphonuclears_Blood_Count ['A0150', 'H6730'] concepts_set_name GLIMS_ANABIO_concept_code Bicarbonate_Blood_Concentration ['A0422', 'H9622', 'C6408', 'F4161', 'A2136', 'J7371', 'G2031'] Calcium_Blood_Concentration ['C0543', 'D2359', 'A0038', 'H5041', 'F2625', 'L5047', 'A2512', 'A2512', 'A0607'] Chloride_Blood_Concentration ['A0079', 'J1179', 'F2619'] Potassium_Blood_Concentration ['A2380', 'E2073', 'F2618', 'E2337', 'J1178'] Sodium_Blood_Concentration ['A0262', 'J1177', 'F8162', 'F2617'] concepts_set_name GLIMS_ANABIO_concept_code HCO3-_Blood_Concentration ['A0420', 'L5018'] Lactate_Gaz_Blood_Concentration ['C8697', 'H7748'] PaCO2_Blood_Concentration ['A7305', 'A0630'] PaO2_Blood_Concentration ['A7319', 'H7747'] SaO2_Blood_Concentration ['A7334', 'L5021'] pH_Blood ['A0221', 'L5017', 'A0219'] concepts_set_name GLIMS_ANABIO_concept_code ALAT_Activity ['A0002', 'G1804', 'J7373', 'E2067', 'F2629'] ASAT_Activity ['A0022', 'G1800', 'E2068', 'F2628'] GGT_Activity ['A0131', 'F8184', 'E9771', 'J7370', 'K7045'] PAL_Activity ['A0227', 'F8187', 'E6331', 'F1844'] Total_Bilirubin_Concentration ['A0029', 'H5264', 'D0189'] concepts_set_name GLIMS_ANABIO_concept_code BNP_Concentration ['C8189', 'B5596', 'A2128'] BNP_and_NTProBNP_Concentration ['C8189', 'B5596', 'A2128', 'A7333', 'J7267', 'J7959'] NTProBNP_Concentration ['A7333', 'J7267', 'J7959'] Troponine_Concentration ['A0283', 'C5560', 'F9934', 'E6954', 'L3534', 'G7716', 'J5184', 'A3832', 'E7249'] concepts_set_name GLIMS_ANABIO_concept_code CRP_Concentration ['A0248', 'E6332', 'F5581', 'J7381', 'F2631'] Fibrinogen_Concentration ['A0126'] Proteins_Blood_Concentration ['A7347', 'F5122', 'F2624', 'B9417', 'A0249', 'B3990'] concepts_set_name GLIMS_ANABIO_concept_code Ferritin_Concentration ['A0123', 'E9865'] Transferrin_Saturation_Coefficient ['A0278'] concepts_set_name GLIMS_ANABIO_concept_code Glomerular_Filtration_Rate_EPI_CKD ['G6921', 'F8160', 'F9613', 'F9621', 'F9621'] Glomerular_Filtration_Rate_MDRD ['F9622', 'G7835', 'B9964', 'A7456', 'A7455', 'H5609'] Urea_Blood_Concentration ['A0286', 'G3350', 'J7372', 'F2620'] concepts_set_name GLIMS_ANABIO_concept_code Activated_Partial_Thromboplastin_Time ['A1792', 'L7286', 'A7748'] D-Dimers_Concentration ['C7882', 'C7882', 'I8765', 'A0124', 'C0474', 'C0474', 'C0474', 'B4199', 'F5402'] Quick_INR_Time ['A0269'] Quick_Prothrombin_Time ['A1805', 'E9993'] concepts_set_name GLIMS_ANABIO_concept_code Albumine_Blood_Concentration ['D2358', 'C6841', 'C2102', 'G6616', 'L2260', 'A0006', 'E4799', 'I2013'] EPP_Blood_Concentration ['A0250', 'C9874', 'A3758', 'A0004', 'F9978', 'A0005', 'H8137', 'C7087', 'A0003', 'C7088', 'B9456', 'B9455', 'A0008', 'H8138', 'C7089', 'A0007'... Phosphates_Blood_Concentration ['A0226', 'F8186', 'F2626'] Proteins_Urine_24h_Concentration ['A1695', 'A1694', 'A1696', 'C9990', 'C9991', 'J7268', 'J7269', 'C3941', 'E4745', 'G4187', 'F6060'] concepts_set_name GLIMS_ANABIO_concept_code B-HCG_Blood_Concentration ['A7426', 'F2353', 'A0164', 'L2277'] Glucose_Blood_Concentration ['A0141', 'H7323', 'J7401', 'F2622', 'B9553', 'C7236', 'E7312', 'A7338', 'H7324', 'C0565', 'E9889', 'A8424'] HbA1c_Blood_% ['B6983', 'A2228', 'A1271', 'E6632', 'I5968'] TSH_Concentration ['A1831', 'F2150', 'I8385', 'C2666'] concepts_set_name GLIMS_ANABIO_concept_code IL-1 beta_Blood_Concentration ['C9351', 'B8921', 'G4800', 'K3662', 'L2217', 'J9193', 'K3665', 'K3687', 'K3661', 'L2197'] IL-10_Blood_Concentration ['B8922', 'C8763', 'K3478', 'L2210', 'J9187', 'K3481', 'K3472', 'K3475', 'L2198'] IL-6_Blood_Concentration ['B8929', 'G4799', 'B1910', 'K3467', 'L2205', 'E6992', 'J9190', 'K3456', 'L2193', 'K3435', 'K3460'] Procalcitonin_Blood_Concentration ['A1661', 'H5267', 'F2632'] TNF alpha_Blood_Concentration ['B8931', 'G4801', 'C9393', 'K3505', 'L2203', 'E6993', 'J9194', 'K3658', 'K3502', 'K3504', 'L2191']","title":"Predefined concepts sets"},{"location":"functionalities/biology/exploring-relationship/","text":"Terminologies relationships Manipulating different code terminologies through OMOP concept and concept_relationship tables can be tricky. This becomes even more pronounced when working with biological measurements that may encompass multiple terminologies, including laboratory, unified, and international terminologies. Use prepare_biology_relationship_table to preprocess OMOP concept and concept_relationship into a single table and get a better insight on how terminologies are related. Relationship config Terminologies mapping from AP-HP database are used by default. See io.settings.measurement_config for mapping details or to modify it. from eds_scikit.biology import prepare_biology_relationship_table biology_relationship_table = prepare_biology_relationship_table ( data ) biology_relationship_table = biology_relationship_table . to_pandas () biology_relationship_table . head () source_concept_id source_concept_name source_concept_code standard_concept_id standard_concept_name standard_concept_code 3 xxxxxxxxxxxx CX1 4 xxxxxxxxxxxx A1 9 xxxxxxxxxxxx ZY2 5 xxxxxxxxxxxx A2 9 xxxxxxxxxxxx B3F 47 xxxxxxxxxxxx D3 7 xxxxxxxxxxxx T32 4 xxxxxxxxxxxx F82 5 xxxxxxxxxxxx S23 1 xxxxxxxxxxxx A432","title":"Terminologies relationships"},{"location":"functionalities/biology/exploring-relationship/#terminologies-relationships","text":"Manipulating different code terminologies through OMOP concept and concept_relationship tables can be tricky. This becomes even more pronounced when working with biological measurements that may encompass multiple terminologies, including laboratory, unified, and international terminologies. Use prepare_biology_relationship_table to preprocess OMOP concept and concept_relationship into a single table and get a better insight on how terminologies are related. Relationship config Terminologies mapping from AP-HP database are used by default. See io.settings.measurement_config for mapping details or to modify it. from eds_scikit.biology import prepare_biology_relationship_table biology_relationship_table = prepare_biology_relationship_table ( data ) biology_relationship_table = biology_relationship_table . to_pandas () biology_relationship_table . head () source_concept_id source_concept_name source_concept_code standard_concept_id standard_concept_name standard_concept_code 3 xxxxxxxxxxxx CX1 4 xxxxxxxxxxxx A1 9 xxxxxxxxxxxx ZY2 5 xxxxxxxxxxxx A2 9 xxxxxxxxxxxx B3F 47 xxxxxxxxxxxx D3 7 xxxxxxxxxxxx T32 4 xxxxxxxxxxxx F82 5 xxxxxxxxxxxx S23 1 xxxxxxxxxxxx A432","title":"Terminologies relationships"},{"location":"functionalities/biology/prepare_measurement/","text":"Prepare measurement Prepare measurement flowchart The pipeline is structured in 3 stages: Basic preprocessing Codes mapping Units conversion","title":"Prepare measurement"},{"location":"functionalities/biology/prepare_measurement/#prepare-measurement","text":"","title":"Prepare measurement"},{"location":"functionalities/biology/prepare_measurement/#prepare-measurement-flowchart","text":"The pipeline is structured in 3 stages: Basic preprocessing Codes mapping Units conversion","title":"Prepare measurement flowchart"},{"location":"functionalities/biology/preparing-measurement/","text":"Detailed use This tutorial demonstrates the workflow to prepare the measurement table. Big volume Measurement table can be large. Do not forget to set proper spark config before loading data. Mapping measurement table to ANABIO codes Defining Concept-Set Here we work with the Glucose pre-defined concept set . See quick-use for an example on how to create a custom concept set. from eds_scikit.biology import prepare_measurement_table , ConceptsSet glucose_blood = ConceptsSet ( \"Glucose_Blood\" ) Preparing measurement table First, we prepare measurements with convert_units = False (as we do not yet know which units are contained in the table). from eds_scikit.biology import measurement_values_summary measurement = prepare_measurement_table ( data , start_date = \"2022-01-01\" , end_date = \"2022-05-01\" , concept_sets = [ glucose_blood ], convert_units = False , get_all_terminologies = False , ) Statistical summary A statistical summary by codes allows us to gain insight into value distributions and detect possible heterogeneous units. from eds_scikit.biology import measurement_values_summary stats_summary = measurement_values_summary ( measurement , category_cols = [ \"concept_set\" , \"GLIMS_ANABIO_concept_code\" ], value_column = \"value_as_number\" , unit_column = \"unit_source_value\" , ) stats_summary concept_set ANABIO_concept_code no_units unit_source_value range_low_anomaly_count range_high_anomaly_count measurement_count value_as_number_count value_as_number_mean value_as_number_std value_as_number_min value_as_number_25% value_as_number_50% value_as_number_75% value_as_number_max Glucose_Blood XXXXX 100 mmol/l 15 5 1000 1000 5 2 0 2 5 8 9 Glucose_Blood YYYYY 50 mg/ml 20 10 5000 5000 25 10 0 20 25 37 45 Glucose_Blood ZZZZZ 10 mmol/l 5 18 1000 1000 6 1 0 4 6 7 10 Units correction To map all units to a common unit base we can use add_conversion and add_target_unit from ConceptSet class. glucose_blood . add_conversion ( \"mol\" , \"g\" , 180 ) glucose_blood . add_target_unit ( \"mmol/l\" ) We can check the new summary table after units conversion. stats_summary = measurement_values_summary ( measurement , category_cols = [ \"concept_set\" , \"GLIMS_ANABIO_concept_code\" ], value_column = \"value_as_number_normalized\" , unit_column = \"unit_source_value_normalized\" , ) stats_summary concept_set ANABIO_concept_code no_units unit_source_value range_low_anomaly_count range_high_anomaly_count measurement_count value_as_number_count value_as_number_mean value_as_number_std value_as_number_min value_as_number_25% value_as_number_50% value_as_number_75% value_as_number_max Glucose_Blood XXXXX 100 mmol/l 15 5 1000 1000 5 2 0 2 5 8 9 Glucose_Blood YYYYY 50 mmol/l 20 10 5000 5000 5 2 0 4 5 7 9 Glucose_Blood ZZZZZ 10 mmol/l 5 18 1000 1000 6 1 0 4 6 7 10 Plot summary Once all units are homogeneous, we can generate more detailed dashboard for biology investigation. from eds_scikit.biology import plot_biology_summary measurement = measurement . merge ( data . visit_occurrence [[ \"care_site_id\" , \"visit_occurrence_id\" ]], on = \"visit_occurrence_id\" , ) measurement = measurement . merge ( data . care_site [[ \"care_site_id\" , \"care_site_short_name\" ]], on = \"care_site_id\" ) plot_biology_summary ( measurement , value_column = \"value_as_number_normalized\" ) Volumetry dashboard Distribution dashboard","title":"Detailed use"},{"location":"functionalities/biology/preparing-measurement/#detailed-use","text":"This tutorial demonstrates the workflow to prepare the measurement table. Big volume Measurement table can be large. Do not forget to set proper spark config before loading data.","title":"Detailed use"},{"location":"functionalities/biology/preparing-measurement/#mapping-measurement-table-to-anabio-codes","text":"","title":"Mapping measurement table to ANABIO codes"},{"location":"functionalities/biology/preparing-measurement/#defining-concept-set","text":"Here we work with the Glucose pre-defined concept set . See quick-use for an example on how to create a custom concept set. from eds_scikit.biology import prepare_measurement_table , ConceptsSet glucose_blood = ConceptsSet ( \"Glucose_Blood\" )","title":"Defining Concept-Set"},{"location":"functionalities/biology/preparing-measurement/#preparing-measurement-table","text":"First, we prepare measurements with convert_units = False (as we do not yet know which units are contained in the table). from eds_scikit.biology import measurement_values_summary measurement = prepare_measurement_table ( data , start_date = \"2022-01-01\" , end_date = \"2022-05-01\" , concept_sets = [ glucose_blood ], convert_units = False , get_all_terminologies = False , )","title":"Preparing measurement table"},{"location":"functionalities/biology/preparing-measurement/#statistical-summary","text":"A statistical summary by codes allows us to gain insight into value distributions and detect possible heterogeneous units. from eds_scikit.biology import measurement_values_summary stats_summary = measurement_values_summary ( measurement , category_cols = [ \"concept_set\" , \"GLIMS_ANABIO_concept_code\" ], value_column = \"value_as_number\" , unit_column = \"unit_source_value\" , ) stats_summary concept_set ANABIO_concept_code no_units unit_source_value range_low_anomaly_count range_high_anomaly_count measurement_count value_as_number_count value_as_number_mean value_as_number_std value_as_number_min value_as_number_25% value_as_number_50% value_as_number_75% value_as_number_max Glucose_Blood XXXXX 100 mmol/l 15 5 1000 1000 5 2 0 2 5 8 9 Glucose_Blood YYYYY 50 mg/ml 20 10 5000 5000 25 10 0 20 25 37 45 Glucose_Blood ZZZZZ 10 mmol/l 5 18 1000 1000 6 1 0 4 6 7 10","title":"Statistical summary"},{"location":"functionalities/biology/preparing-measurement/#units-correction","text":"To map all units to a common unit base we can use add_conversion and add_target_unit from ConceptSet class. glucose_blood . add_conversion ( \"mol\" , \"g\" , 180 ) glucose_blood . add_target_unit ( \"mmol/l\" ) We can check the new summary table after units conversion. stats_summary = measurement_values_summary ( measurement , category_cols = [ \"concept_set\" , \"GLIMS_ANABIO_concept_code\" ], value_column = \"value_as_number_normalized\" , unit_column = \"unit_source_value_normalized\" , ) stats_summary concept_set ANABIO_concept_code no_units unit_source_value range_low_anomaly_count range_high_anomaly_count measurement_count value_as_number_count value_as_number_mean value_as_number_std value_as_number_min value_as_number_25% value_as_number_50% value_as_number_75% value_as_number_max Glucose_Blood XXXXX 100 mmol/l 15 5 1000 1000 5 2 0 2 5 8 9 Glucose_Blood YYYYY 50 mmol/l 20 10 5000 5000 5 2 0 4 5 7 9 Glucose_Blood ZZZZZ 10 mmol/l 5 18 1000 1000 6 1 0 4 6 7 10","title":"Units correction"},{"location":"functionalities/biology/preparing-measurement/#plot-summary","text":"Once all units are homogeneous, we can generate more detailed dashboard for biology investigation. from eds_scikit.biology import plot_biology_summary measurement = measurement . merge ( data . visit_occurrence [[ \"care_site_id\" , \"visit_occurrence_id\" ]], on = \"visit_occurrence_id\" , ) measurement = measurement . merge ( data . care_site [[ \"care_site_id\" , \"care_site_short_name\" ]], on = \"care_site_id\" ) plot_biology_summary ( measurement , value_column = \"value_as_number_normalized\" ) Volumetry dashboard Distribution dashboard","title":"Plot summary"},{"location":"functionalities/biology/quick-use/","text":"Quick use This tutorial demonstrates how the biology module can be quickly used to map measurement codes. Big volume Measurement table can be large. Do not forget to set proper spark config before loading data. Mapping measurement table to ANABIO codes Defining Concept-Set To define a concept-set variable you just need to specify a terminology and a set of codes. from eds_scikit.biology import prepare_measurement_table , ConceptsSet custom_leukocytes = ConceptsSet ( \"Custom_Leukocytes\" ) custom_leukocytes . add_concept_codes ( concept_codes = [ \"A0174\" , \"H6740\" ], terminology = \"GLIMS_ANABIO\" # (1) ) custom_leukocytes . add_concept_codes ( concept_codes = [ \"6690-2\" ], terminology = \"ITM_LOINC\" # (2) ) Codes must be given with terminology. Available terminologies can be accessed with eds_scikit.io.settings.measurement_config['source_terminologies'] . See. AP-HP biology for details on the AP-HP setting. Codes must be given with terminology. Available terminologies can be accessed with eds_scikit.io.settings.measurement_config['source_terminologies'] . See. AP-HP biology for details on the AP-HP setting. Preparing measurement table Then, simply run prepare_measurement_table to select the measurements from your concept set. measurement = prepare_measurement_table ( data , start_date = \"2022-01-01\" , end_date = \"2022-05-01\" , concept_sets = [ custom_leukocytes ], convert_units = False , get_all_terminologies = True , )","title":"Quick use"},{"location":"functionalities/biology/quick-use/#quick-use","text":"This tutorial demonstrates how the biology module can be quickly used to map measurement codes. Big volume Measurement table can be large. Do not forget to set proper spark config before loading data.","title":"Quick use"},{"location":"functionalities/biology/quick-use/#mapping-measurement-table-to-anabio-codes","text":"","title":"Mapping measurement table to ANABIO codes"},{"location":"functionalities/biology/quick-use/#defining-concept-set","text":"To define a concept-set variable you just need to specify a terminology and a set of codes. from eds_scikit.biology import prepare_measurement_table , ConceptsSet custom_leukocytes = ConceptsSet ( \"Custom_Leukocytes\" ) custom_leukocytes . add_concept_codes ( concept_codes = [ \"A0174\" , \"H6740\" ], terminology = \"GLIMS_ANABIO\" # (1) ) custom_leukocytes . add_concept_codes ( concept_codes = [ \"6690-2\" ], terminology = \"ITM_LOINC\" # (2) ) Codes must be given with terminology. Available terminologies can be accessed with eds_scikit.io.settings.measurement_config['source_terminologies'] . See. AP-HP biology for details on the AP-HP setting. Codes must be given with terminology. Available terminologies can be accessed with eds_scikit.io.settings.measurement_config['source_terminologies'] . See. AP-HP biology for details on the AP-HP setting.","title":"Defining Concept-Set"},{"location":"functionalities/biology/quick-use/#preparing-measurement-table","text":"Then, simply run prepare_measurement_table to select the measurements from your concept set. measurement = prepare_measurement_table ( data , start_date = \"2022-01-01\" , end_date = \"2022-05-01\" , concept_sets = [ custom_leukocytes ], convert_units = False , get_all_terminologies = True , )","title":"Preparing measurement table"},{"location":"functionalities/biology/tutorial/","text":"(function() { function addWidgetsRenderer() { var requireJsScript = document.createElement('script'); requireJsScript.src = 'https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js'; var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var jupyterWidgetsScript = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} jupyterWidgetsScript.src = widgetRendererSrc; document.body.appendChild(requireJsScript); document.body.appendChild(jupyterWidgetsScript); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); You can download this notebook directly here Tutorial - Preparing measurement table This tutorial takes you through the entire workflow of the Biology module. import eds_scikit import pandas as pd 1 - Load data Big volume Measurement table can be large. Do not forget to set proper spark config. to_add_conf = [ ( \"master\" , \"yarn\" ), ( \"deploy_mode\" , \"client\" ), ( \"spark.driver.memory\" , ... ), ( \"spark.executor.memory\" , ... ), ( \"spark.executor.cores\" , ... ), ( \"spark.executor.memoryOverhead\" , ... ), ( \"spark.driver.maxResultSize\" , ... ) ... ] spark , sc , sql = eds_scikit . improve_performances ( to_add_conf = to_add_conf ) from eds_scikit.io.hive import HiveData data = HiveData ( spark_session = spark , database_name = \"cse_xxxxxxx_xxxxxxx\" , tables_to_load = [ \"care_site\" , \"concept\" , \"visit_occurrence\" , \"measurement\" , \"concept_relationship\" , ], ) 2 - Quick use : Preparing measurement table a) Define biology concept-sets In order to work on the measurements of interest, you can extract a list of concepts-sets by: Selecting default concepts-sets provided in the library. Modifying the codes of a selected default concepts-set. Creating a concepts-set from scratch. Code selection can be tricky. See Concept codes relationships exploration section for more details on how to select them. from eds_scikit.biology import ConceptsSet # Creating Concept-Set custom_leukocytes = ConceptsSet ( \"Custom_Leukocytes\" ) custom_leukocytes . add_concept_codes ( concept_codes = [ 'A0174' , 'H6740' , 'C8824' ], terminology = 'GLIMS_ANABIO' ) custom_leukocytes . add_concept_codes ( concept_codes = [ '6690-2' ], terminology = 'ITM_LOINC' ) # Importing Concept-Set (see. 4.b for details on existing concepts sets) glucose_blood = ConceptsSet ( \"Glucose_Blood_Concentration\" ) concepts_sets = [ custom_leukocytes , glucose_blood ] b) Prepare measurements Lazy execution Execution will be lazy, except if convert_units=True . from eds_scikit.biology import prepare_measurement_table measurement = prepare_measurement_table ( data , start_date = \"2022-01-01\" , end_date = \"2022-05-01\" , concept_sets = concepts_sets , convert_units = False , get_all_terminologies = True ) Now you have your measurement table mapped with concept set terminology. Next sections are about measurement codes analysis, units and plots. 3 - Detailed use : Analysing measurement table a) Measurements statistics table from eds_scikit.biology import measurement_values_summary stats_summary = measurement_values_summary ( measurement , category_cols = [ \"concept_set\" , \"GLIMS_ANABIO_concept_code\" , \"GLIMS_LOINC_concept_code\" ], value_column = \"value_as_number\" , unit_column = \"unit_source_value\" ) stats_summary b) Measurements units correction glucose_blood . add_conversion ( \"mol\" , \"g\" , 180 ) glucose_blood . add_target_unit ( \"mmol/l\" ) concepts_sets = [ glucose_blood , custom_leukocytes ] measurement = prepare_measurement_table ( data , start_date = \"2022-01-01\" , end_date = \"2022-05-01\" , concept_sets = concepts_sets , convert_units = True , get_all_terminologies = False ) stats_summary = measurement_values_summary ( measurement , category_cols = [ \"concept_set\" , \"GLIMS_ANABIO_concept_code\" ], value_column = \"value_as_number_normalized\" , #converted unit_column = \"unit_source_value_normalized\" ) stats_summary c) Plot biology summary Applying plot_biology_summary to computed measurement dataframe, merged with care sites, allows to generate nice exploration plots such as : Interactive volumetry Interactive distribution from eds_scikit.biology import plot_biology_summary measurement = measurement . merge ( data . visit_occurrence [[ \"care_site_id\" , \"visit_occurrence_id\" ]], on = \"visit_occurrence_id\" ) measurement = measurement . merge ( data . care_site [[ \"care_site_id\" , \"care_site_short_name\" ]], on = \"care_site_id\" ) plot_biology_summary ( measurement , value_column = \"value_as_number_normalized\" ) 4 - Further : Concept Codes, Concepts Sets and Units 1 - Concept codes relationships exploration Concept codes relationships can be tricky to understand and to manipulate. Function prepare_biology_relationship_table allows to build mapping dataframe between main AP-HP biology referential . See io.settings.measurement_config[\"mapping\"] and io.settings.measurement_config[\"source_terminologies\"] configurations for mapping details. from eds_scikit.biology import prepare_biology_relationship_table biology_relationship_table = prepare_biology_relationship_table ( data ) biology_relationship_table = biology_relationship_table . to_pandas () Relationship between codes from different referentials. columns = [ col for col in biology_relationship_table . columns if \"concept_code\" in col ] biology_relationship_table [ biology_relationship_table . GLIMS_ANABIO_concept_code . isin ([ 'A0174' , 'H6740' , 'C8824' ])][ columns ] . drop_duplicates () ANALYSES_LABORATOIRE_concept_code GLIMS_ANABIO_concept_code GLIMS_LOINC_concept_code ITM_ANABIO_concept_code ITM_LOINC_concept_code 0 C8824 33256-9 Unknown Unknown 1 A0174 6690-2 A0174 6690-2 1 A0174 26464-8 A0174 6690-2 biology_relationship_table [ biology_relationship_table . GLIMS_LOINC_concept_code . isin ([ '33256-9' , '6690-2' , '26464-8' ])][ columns ] . drop_duplicates () ANALYSES_LABORATOIRE_concept_code GLIMS_ANABIO_concept_code GLIMS_LOINC_concept_code ITM_ANABIO_concept_code ITM_LOINC_concept_code 4 E4358 6690-2 Unknown Unknown 2 C9097 26464-8 Unknown Unknown 6 K3232 6690-2 Unknown Unknown 5 E6953 26464-8 Unknown Unknown 1 C8824 33256-9 Unknown Unknown 4 E4358 26464-8 Unknown Unknown 5 E6953 6690-2 Unknown Unknown 7 K6094 6690-2 Unknown Unknown 0 C9784 6690-2 C9784 6690-2 0 C9784 26464-8 C9784 6690-2 3 A0174 6690-2 A0174 6690-2 3 A0174 26464-8 A0174 6690-2 2 - Concepts-Sets To get all availables concepts sets see datasets.default_concepts_sets . More details about their definition and how they are build can be found in this section . from eds_scikit import datasets from eds_scikit.biology import ConceptsSet print ( ConceptsSet ( \"Glucose_Blood_Concentration\" ) . concept_codes ) datasets . default_concepts_sets 3 - Units Units module makes conversion between units easier. It uses configuration files datasets.units and datasets.elements . from eds_scikit import datasets from eds_scikit.biology import Units units = Units () print ( \"L to ml : \" , units . convert_unit ( \"L\" , \"ml\" )) print ( \"m/s to m/h : \" , units . convert_unit ( \"m/s\" , \"m/h\" )) print ( \"g to mol : \" , units . convert_unit ( \"g\" , \"mol\" )) units . add_conversion ( \"mol\" , \"g\" , 180 ) print ( \"g to mol : \" , units . convert_unit ( \"g\" , \"mol\" ))","title":"Tutorial"},{"location":"functionalities/biology/tutorial/#tutorial-preparing-measurement-table","text":"This tutorial takes you through the entire workflow of the Biology module. import eds_scikit import pandas as pd","title":"Tutorial - Preparing measurement table"},{"location":"functionalities/biology/tutorial/#1-load-data","text":"Big volume Measurement table can be large. Do not forget to set proper spark config. to_add_conf = [ ( \"master\" , \"yarn\" ), ( \"deploy_mode\" , \"client\" ), ( \"spark.driver.memory\" , ... ), ( \"spark.executor.memory\" , ... ), ( \"spark.executor.cores\" , ... ), ( \"spark.executor.memoryOverhead\" , ... ), ( \"spark.driver.maxResultSize\" , ... ) ... ] spark , sc , sql = eds_scikit . improve_performances ( to_add_conf = to_add_conf ) from eds_scikit.io.hive import HiveData data = HiveData ( spark_session = spark , database_name = \"cse_xxxxxxx_xxxxxxx\" , tables_to_load = [ \"care_site\" , \"concept\" , \"visit_occurrence\" , \"measurement\" , \"concept_relationship\" , ], )","title":"1 - Load data "},{"location":"functionalities/biology/tutorial/#2-quick-use-preparing-measurement-table","text":"","title":"2 - Quick use : Preparing measurement table "},{"location":"functionalities/biology/tutorial/#a-define-biology-concept-sets","text":"In order to work on the measurements of interest, you can extract a list of concepts-sets by: Selecting default concepts-sets provided in the library. Modifying the codes of a selected default concepts-set. Creating a concepts-set from scratch. Code selection can be tricky. See Concept codes relationships exploration section for more details on how to select them. from eds_scikit.biology import ConceptsSet # Creating Concept-Set custom_leukocytes = ConceptsSet ( \"Custom_Leukocytes\" ) custom_leukocytes . add_concept_codes ( concept_codes = [ 'A0174' , 'H6740' , 'C8824' ], terminology = 'GLIMS_ANABIO' ) custom_leukocytes . add_concept_codes ( concept_codes = [ '6690-2' ], terminology = 'ITM_LOINC' ) # Importing Concept-Set (see. 4.b for details on existing concepts sets) glucose_blood = ConceptsSet ( \"Glucose_Blood_Concentration\" ) concepts_sets = [ custom_leukocytes , glucose_blood ]","title":"a) Define biology concept-sets "},{"location":"functionalities/biology/tutorial/#b-prepare-measurements","text":"Lazy execution Execution will be lazy, except if convert_units=True . from eds_scikit.biology import prepare_measurement_table measurement = prepare_measurement_table ( data , start_date = \"2022-01-01\" , end_date = \"2022-05-01\" , concept_sets = concepts_sets , convert_units = False , get_all_terminologies = True ) Now you have your measurement table mapped with concept set terminology. Next sections are about measurement codes analysis, units and plots.","title":"b) Prepare measurements "},{"location":"functionalities/biology/tutorial/#3-detailed-use-analysing-measurement-table","text":"","title":"3 - Detailed use : Analysing measurement table"},{"location":"functionalities/biology/tutorial/#a-measurements-statistics-table","text":"from eds_scikit.biology import measurement_values_summary stats_summary = measurement_values_summary ( measurement , category_cols = [ \"concept_set\" , \"GLIMS_ANABIO_concept_code\" , \"GLIMS_LOINC_concept_code\" ], value_column = \"value_as_number\" , unit_column = \"unit_source_value\" ) stats_summary","title":"a) Measurements statistics table "},{"location":"functionalities/biology/tutorial/#b-measurements-units-correction","text":"glucose_blood . add_conversion ( \"mol\" , \"g\" , 180 ) glucose_blood . add_target_unit ( \"mmol/l\" ) concepts_sets = [ glucose_blood , custom_leukocytes ] measurement = prepare_measurement_table ( data , start_date = \"2022-01-01\" , end_date = \"2022-05-01\" , concept_sets = concepts_sets , convert_units = True , get_all_terminologies = False ) stats_summary = measurement_values_summary ( measurement , category_cols = [ \"concept_set\" , \"GLIMS_ANABIO_concept_code\" ], value_column = \"value_as_number_normalized\" , #converted unit_column = \"unit_source_value_normalized\" ) stats_summary","title":"b) Measurements units correction "},{"location":"functionalities/biology/tutorial/#c-plot-biology-summary","text":"Applying plot_biology_summary to computed measurement dataframe, merged with care sites, allows to generate nice exploration plots such as : Interactive volumetry Interactive distribution from eds_scikit.biology import plot_biology_summary measurement = measurement . merge ( data . visit_occurrence [[ \"care_site_id\" , \"visit_occurrence_id\" ]], on = \"visit_occurrence_id\" ) measurement = measurement . merge ( data . care_site [[ \"care_site_id\" , \"care_site_short_name\" ]], on = \"care_site_id\" ) plot_biology_summary ( measurement , value_column = \"value_as_number_normalized\" )","title":"c) Plot biology summary "},{"location":"functionalities/biology/tutorial/#4-further-concept-codes-concepts-sets-and-units","text":"","title":"4 - Further : Concept Codes, Concepts Sets and Units "},{"location":"functionalities/biology/tutorial/#1-concept-codes-relationships-exploration","text":"Concept codes relationships can be tricky to understand and to manipulate. Function prepare_biology_relationship_table allows to build mapping dataframe between main AP-HP biology referential . See io.settings.measurement_config[\"mapping\"] and io.settings.measurement_config[\"source_terminologies\"] configurations for mapping details. from eds_scikit.biology import prepare_biology_relationship_table biology_relationship_table = prepare_biology_relationship_table ( data ) biology_relationship_table = biology_relationship_table . to_pandas () Relationship between codes from different referentials. columns = [ col for col in biology_relationship_table . columns if \"concept_code\" in col ] biology_relationship_table [ biology_relationship_table . GLIMS_ANABIO_concept_code . isin ([ 'A0174' , 'H6740' , 'C8824' ])][ columns ] . drop_duplicates () ANALYSES_LABORATOIRE_concept_code GLIMS_ANABIO_concept_code GLIMS_LOINC_concept_code ITM_ANABIO_concept_code ITM_LOINC_concept_code 0 C8824 33256-9 Unknown Unknown 1 A0174 6690-2 A0174 6690-2 1 A0174 26464-8 A0174 6690-2 biology_relationship_table [ biology_relationship_table . GLIMS_LOINC_concept_code . isin ([ '33256-9' , '6690-2' , '26464-8' ])][ columns ] . drop_duplicates () ANALYSES_LABORATOIRE_concept_code GLIMS_ANABIO_concept_code GLIMS_LOINC_concept_code ITM_ANABIO_concept_code ITM_LOINC_concept_code 4 E4358 6690-2 Unknown Unknown 2 C9097 26464-8 Unknown Unknown 6 K3232 6690-2 Unknown Unknown 5 E6953 26464-8 Unknown Unknown 1 C8824 33256-9 Unknown Unknown 4 E4358 26464-8 Unknown Unknown 5 E6953 6690-2 Unknown Unknown 7 K6094 6690-2 Unknown Unknown 0 C9784 6690-2 C9784 6690-2 0 C9784 26464-8 C9784 6690-2 3 A0174 6690-2 A0174 6690-2 3 A0174 26464-8 A0174 6690-2","title":"1 - Concept codes relationships exploration "},{"location":"functionalities/biology/tutorial/#2-concepts-sets","text":"To get all availables concepts sets see datasets.default_concepts_sets . More details about their definition and how they are build can be found in this section . from eds_scikit import datasets from eds_scikit.biology import ConceptsSet print ( ConceptsSet ( \"Glucose_Blood_Concentration\" ) . concept_codes ) datasets . default_concepts_sets","title":"2 - Concepts-Sets "},{"location":"functionalities/biology/tutorial/#3-units","text":"Units module makes conversion between units easier. It uses configuration files datasets.units and datasets.elements . from eds_scikit import datasets from eds_scikit.biology import Units units = Units () print ( \"L to ml : \" , units . convert_unit ( \"L\" , \"ml\" )) print ( \"m/s to m/h : \" , units . convert_unit ( \"m/s\" , \"m/h\" )) print ( \"g to mol : \" , units . convert_unit ( \"g\" , \"mol\" )) units . add_conversion ( \"mol\" , \"g\" , 180 ) print ( \"g to mol : \" , units . convert_unit ( \"g\" , \"mol\" ))","title":"3 - Units "},{"location":"functionalities/biology/visualization/","text":"Visualizing measurements Once the measurement table has been computed, biology module provides measurement_values_summary and plot_biology_summary to gain better insights into their distribution and volumetry across codes, care sites, and time. Statistical summary measurement_values_summary generates useful statistics to identify anomalies in measurements associated with a concept set. from eds_scikit.biology import measurement_values_summary stats_summary = measurement_values_summary ( measurement , category_cols = [ \"concept_set\" , \"GLIMS_ANABIO_concept_code\" , \"GLIMS_LOINC_concept_code\" , ], value_column = \"value_as_number\" , unit_column = \"unit_source_value\" , ) stats_summary concept_set ANABIO_concept_code no_units unit_source_value range_low_anomaly_count range_high_anomaly_count measurement_count value_as_number_count value_as_number_mean value_as_number_std value_as_number_min value_as_number_25% value_as_number_50% value_as_number_75% value_as_number_max Custom_Leukocytes A0174 148 x10*9/l 813 1099 11857 11857 21 18 0 25 50 75 100 Custom_Leukocytes C8824 121 x10*9/l 1166 1196 11821 11821 20 20 0 25 50 75 100 Custom_Leukocytes C9784 83 x10*9/l 935 902 11082 11082 10 16 0 25 50 75 100 Plot summary plot_biology_summary generates a useful dashboard to better understand the volumetry and distribution of codes within the same concept set. The purpose is to identify and correct possible biases associated with sets of codes, time periods, or specific care sites. from eds_scikit.biology import plot_biology_summary # First add 'care_site_short_name' column to measurement table measurement = measurement . merge ( data . visit_occurrence [[ \"care_site_id\" , \"visit_occurrence_id\" ]], on = \"visit_occurrence_id\" , ) measurement = measurement . merge ( data . care_site [[ \"care_site_id\" , \"care_site_short_name\" ]], on = \"care_site_id\" ) plot_biology_summary ( measurement , value_column = \"value_as_number\" ) Volumetry dashboard Distribution dashboard","title":"Visualizing measurements"},{"location":"functionalities/biology/visualization/#visualizing-measurements","text":"Once the measurement table has been computed, biology module provides measurement_values_summary and plot_biology_summary to gain better insights into their distribution and volumetry across codes, care sites, and time.","title":"Visualizing measurements"},{"location":"functionalities/biology/visualization/#statistical-summary","text":"measurement_values_summary generates useful statistics to identify anomalies in measurements associated with a concept set. from eds_scikit.biology import measurement_values_summary stats_summary = measurement_values_summary ( measurement , category_cols = [ \"concept_set\" , \"GLIMS_ANABIO_concept_code\" , \"GLIMS_LOINC_concept_code\" , ], value_column = \"value_as_number\" , unit_column = \"unit_source_value\" , ) stats_summary concept_set ANABIO_concept_code no_units unit_source_value range_low_anomaly_count range_high_anomaly_count measurement_count value_as_number_count value_as_number_mean value_as_number_std value_as_number_min value_as_number_25% value_as_number_50% value_as_number_75% value_as_number_max Custom_Leukocytes A0174 148 x10*9/l 813 1099 11857 11857 21 18 0 25 50 75 100 Custom_Leukocytes C8824 121 x10*9/l 1166 1196 11821 11821 20 20 0 25 50 75 100 Custom_Leukocytes C9784 83 x10*9/l 935 902 11082 11082 10 16 0 25 50 75 100","title":"Statistical summary"},{"location":"functionalities/biology/visualization/#plot-summary","text":"plot_biology_summary generates a useful dashboard to better understand the volumetry and distribution of codes within the same concept set. The purpose is to identify and correct possible biases associated with sets of codes, time periods, or specific care sites. from eds_scikit.biology import plot_biology_summary # First add 'care_site_short_name' column to measurement table measurement = measurement . merge ( data . visit_occurrence [[ \"care_site_id\" , \"visit_occurrence_id\" ]], on = \"visit_occurrence_id\" , ) measurement = measurement . merge ( data . care_site [[ \"care_site_id\" , \"care_site_short_name\" ]], on = \"care_site_id\" ) plot_biology_summary ( measurement , value_column = \"value_as_number\" ) Volumetry dashboard Distribution dashboard","title":"Plot summary"},{"location":"functionalities/generic/introduction/","text":"(function() { function addWidgetsRenderer() { var requireJsScript = document.createElement('script'); requireJsScript.src = 'https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js'; var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var jupyterWidgetsScript = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} jupyterWidgetsScript.src = widgetRendererSrc; document.body.appendChild(requireJsScript); document.body.appendChild(jupyterWidgetsScript); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); You can download this notebook directly here A gentle demo import datetime import pandas as pd import eds_scikit spark , sc , sql = eds_scikit . improve_performances () # (1) See the welcome page for an explanation of this line Loading data Data loading is made easy by using the HiveData object. Simply give it the name of the database you want to use: database_name = \"MY_DATABASE_NAME\" from eds_scikit.io import HiveData data = HiveData ( database_name = \"database_name\" , ) Now your tables are available as Koalas DataFrames: Those are basically Spark DataFrames which allows for the Pandas API to be used on top (see the Project description of eds-scikit's documentation for more informations.) What we need to extract: Patients with diabetes Patients with Covid-19 Visits from those patients, and their ICU/Non-ICU status Let us import what's necessary from eds-scikit : from eds_scikit.event import conditions_from_icd10 from eds_scikit.event.diabetes import ( diabetes_from_icd10 , DEFAULT_DIABETE_FROM_ICD10_CONFIG , ) from eds_scikit.icu import tag_icu_visit DATE_MIN = datetime . datetime ( 2020 , 1 , 1 ) DATE_MAX = datetime . datetime ( 2021 , 6 , 1 ) Extracting the diabetic status Luckily, a function is available to extract diabetic patients from ICD-10: diabetes = diabetes_from_icd10 ( condition_occurrence = data . condition_occurrence , visit_occurrence = data . visit_occurrence , date_min = DATE_MIN , date_max = DATE_MAX , ) We can check the default parameters used here: DEFAULT_DIABETE_FROM_ICD10_CONFIG {'additional_filtering': {'condition_status_source_value': {'DP', 'DAS'}}, 'codes': {'DIABETES_INSIPIDUS': {'code_list': ['E232', 'N251'], 'code_type': 'exact'}, 'DIABETES_IN_PREGNANCY': {'code_list': ['O24'], 'code_type': 'prefix'}, 'DIABETES_MALNUTRITION': {'code_list': ['E12'], 'code_type': 'prefix'}, 'DIABETES_TYPE_I': {'code_list': ['E10'], 'code_type': 'prefix'}, 'DIABETES_TYPE_II': {'code_list': ['E11'], 'code_type': 'prefix'}, 'OTHER_DIABETES_MELLITUS': {'code_list': ['E13', 'E14'], 'code_type': 'prefix'}}, 'date_from_visit': True, 'default_code_type': 'prefix'} We are only interested in diabetes mellitus , although we extracted other types of diabetes: diabetes . concept . value_counts () DIABETES_TYPE_II 117843 DIABETES_TYPE_I 10597 OTHER_DIABETES_MELLITUS 6031 DIABETES_IN_PREGNANCY 2597 DIABETES_INSIPIDUS 1089 DIABETES_MALNUTRITION 199 Name: concept, dtype: int64 We will restrict the types of diabetes used here: diabetes_cohort = ( diabetes [ diabetes . concept . isin ( { \"DIABETES_TYPE_I\" , \"DIABETES_TYPE_II\" , \"OTHER_DIABETES_MELLITUS\" , } ) ] . person_id . unique () . reset_index () ) diabetes_cohort . loc [:, \"HAS_DIABETE\" ] = True Extracting the Covid status Using the conditions_from_icd10 function, we will extract visits linked to COVID-19: codes = dict ( COVID = dict ( code_list = r \"U071[0145]\" , code_type = \"regex\" , ) ) covid = conditions_from_icd10 ( condition_occurrence = data . condition_occurrence , visit_occurrence = data . visit_occurrence , codes = codes , date_min = DATE_MIN , date_max = DATE_MAX , ) Now we can go from the visit_occurrence level to the visit_detail level. visit_detail_covid = data . visit_detail . merge ( covid [[ \"visit_occurrence_id\" ]], on = \"visit_occurrence_id\" , how = \"inner\" , ) Extracting ICU visits What is left to do is to tag each visit as occurring in an ICU or not. This is achieved with the tag_icu_visit . Like many functions in eds-scikit , this function exposes an algo argument, allowing you to choose how the tagging is done. You can check the corresponding documentation to see the availables algos . visit_detail_covid = tag_icu_visit ( visit_detail = visit_detail_covid , care_site = data . care_site , algo = \"from_authorisation_type\" , ) visit_detail_covid = visit_detail_covid . merge ( diabetes_cohort , on = \"person_id\" , how = \"left\" ) visit_detail_covid [ \"HAS_DIABETE\" ] . fillna ( False , inplace = True ) visit_detail_covid [ \"IS_ICU\" ] . fillna ( False , inplace = True ) Finishing the analysis Adding patient's age We will add the patient's age at each visit_detail : from eds_scikit.utils import datetime_helpers visit_detail_covid = visit_detail_covid . merge ( data . person [[ 'person_id' , 'birth_datetime' ]], on = 'person_id' , how = 'inner' ) visit_detail_covid [ \"age\" ] = ( datetime_helpers . substract_datetime ( visit_detail_covid [ \"visit_detail_start_datetime\" ], visit_detail_covid [ \"birth_datetime\" ], out = \"hours\" , ) / ( 24 * 365.25 ) ) From distributed Koalas to local Pandas All the computing above was done using Koalas DataFrames, which are distributed. Now that we limited our cohort to a manageable size, we can switch to Pandas to finish our analysis. visit_detail_covid_pd = visit_detail_covid [ [ \"person_id\" , \"age\" , \"HAS_DIABETE\" , \"IS_ICU\" ] ] . to_pandas () Grouping by patient stats = ( visit_detail_covid_pd [[ \"person_id\" , \"age\" , \"HAS_DIABETE\" , \"IS_ICU\" ]] . groupby ( \"person_id\" ) . agg ( HAS_DIABETE = ( \"HAS_DIABETE\" , \"any\" ), IS_ICU = ( \"IS_ICU\" , \"any\" ), age = ( \"age\" , \"min\" ), ) ) Binning the age into intervals stats [ \"age\" ] = pd . cut ( stats . age , bins = [ 0 , 40 , 50 , 60 , 70 , 120 ], labels = [ \"(0, 40]\" , \"(40, 50]\" , \"(50, 60]\" , \"(60, 70]\" , \"(70, 120]\" ], ) Computing the ratio of patients that had an ICU visit stats = stats . groupby ([ \"age\" , \"HAS_DIABETE\" ], as_index = False ) . apply ( lambda x : x [ \"IS_ICU\" ] . sum () / len ( x ) ) stats . columns = [ \"age\" , \"cohorte\" , \"percent_icu\" ] stats [ \"cohorte\" ] = stats [ \"cohorte\" ] . replace ({ True : \"Diab.\" , False : \"Control\" }) Results stats .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } age cohorte percent_icu 0 (0, 40] Control 0.327988 1 (0, 40] Diab. 0.445578 2 (40, 50] Control 0.263667 3 (40, 50] Diab. 0.427203 4 (50, 60] Control 0.315931 5 (50, 60] Diab. 0.464736 6 (60, 70] Control 0.356808 7 (60, 70] Diab. 0.474766 8 (70, 120] Control 0.159337 9 (70, 120] Diab. 0.230180 We can finally plot our results using Altair : import altair as alt bars = ( alt . Chart ( stats , title = [ \"Percentage of patients who went through ICU during their COVID stay, \" , \"as a function of their age range and diabetic status\" , \" \" , ], ) . mark_bar () . encode ( x = alt . X ( \"cohorte:N\" , title = \"\" ), y = alt . Y ( \"percent_icu\" , title = \" % o f patients who went through ICU.\" , axis = alt . Axis ( format = \"%\" ), ), color = alt . Color ( \"cohorte:N\" , title = \"Cohort\" ), column = alt . Column ( \"age:N\" , title = \"Age range\" ), ) ) bars = bars . configure_title ( anchor = \"middle\" , baseline = \"bottom\" ) bars (function(spec, embedOpt){ let outputDiv = document.currentScript.previousElementSibling; if (outputDiv.id !== \"altair-viz-679a8662c76643b1ae8af86ce4171d2c\") { outputDiv = document.getElementById(\"altair-viz-679a8662c76643b1ae8af86ce4171d2c\"); } const paths = { \"vega\": \"https://cdn.jsdelivr.net/npm//vega@5?noext\", \"vega-lib\": \"https://cdn.jsdelivr.net/npm//vega-lib?noext\", \"vega-lite\": \"https://cdn.jsdelivr.net/npm//vega-lite@4.8.1?noext\", \"vega-embed\": \"https://cdn.jsdelivr.net/npm//vega-embed@6?noext\", }; function loadScript(lib) { return new Promise(function(resolve, reject) { var s = document.createElement('script'); s.src = paths[lib]; s.async = true; s.onload = () => resolve(paths[lib]); s.onerror = () => reject(`Error loading script: ${paths[lib]}`); document.getElementsByTagName(\"head\")[0].appendChild(s); }); } function showError(err) { outputDiv.innerHTML = `<div class=\"error\" style=\"color:red;\">${err}</div>`; throw err; } function displayChart(vegaEmbed) { vegaEmbed(outputDiv, spec, embedOpt) .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`)); } if(typeof define === \"function\" && define.amd) { requirejs.config({paths}); require([\"vega-embed\"], displayChart, err => showError(`Error loading script: ${err.message}`)); } else if (typeof vegaEmbed === \"function\") { displayChart(vegaEmbed); } else { loadScript(\"vega\") .then(() => loadScript(\"vega-lite\")) .then(() => loadScript(\"vega-embed\")) .catch(showError) .then(() => displayChart(vegaEmbed)); } })({\"config\": {\"view\": {\"continuousWidth\": 400, \"continuousHeight\": 300}, \"title\": {\"anchor\": \"middle\", \"baseline\": \"bottom\"}}, \"data\": {\"name\": \"data-55507a07f81645e51f63eaba5b403390\"}, \"mark\": \"bar\", \"encoding\": {\"color\": {\"type\": \"nominal\", \"field\": \"cohorte\", \"title\": \"Cohort\"}, \"column\": {\"type\": \"nominal\", \"field\": \"age\", \"title\": \"Age range\"}, \"x\": {\"type\": \"nominal\", \"field\": \"cohorte\", \"title\": \"\"}, \"y\": {\"type\": \"quantitative\", \"axis\": {\"format\": \"%\"}, \"field\": \"percent_icu\", \"title\": \"% of patients who went through ICU.\"}}, \"title\": [\"Percentage of patients who went through ICU during their COVID stay, \", \"as a function of their age range and diabetic status\", \" \"], \"$schema\": \"https://vega.github.io/schema/vega-lite/v4.8.1.json\", \"datasets\": {\"data-55507a07f81645e51f63eaba5b403390\": [{\"age\": \"(0, 40]\", \"cohorte\": \"Control\", \"percent_icu\": 0.32798833819241985}, {\"age\": \"(0, 40]\", \"cohorte\": \"Diab.\", \"percent_icu\": 0.445578231292517}, {\"age\": \"(40, 50]\", \"cohorte\": \"Control\", \"percent_icu\": 0.26366666666666666}, {\"age\": \"(40, 50]\", \"cohorte\": \"Diab.\", \"percent_icu\": 0.4272030651340996}, {\"age\": \"(50, 60]\", \"cohorte\": \"Control\", \"percent_icu\": 0.31593098812457987}, {\"age\": \"(50, 60]\", \"cohorte\": \"Diab.\", \"percent_icu\": 0.4647364513734224}, {\"age\": \"(60, 70]\", \"cohorte\": \"Control\", \"percent_icu\": 0.3568075117370892}, {\"age\": \"(60, 70]\", \"cohorte\": \"Diab.\", \"percent_icu\": 0.47476552032157215}, {\"age\": \"(70, 120]\", \"cohorte\": \"Control\", \"percent_icu\": 0.15933694181326116}, {\"age\": \"(70, 120]\", \"cohorte\": \"Diab.\", \"percent_icu\": 0.23017958826106}]}}, {\"mode\": \"vega-lite\"}); {\"state\": {}, \"version_major\": 2, \"version_minor\": 0}","title":"A gentle demo"},{"location":"functionalities/generic/introduction/#a-gentle-demo","text":"import datetime import pandas as pd import eds_scikit spark , sc , sql = eds_scikit . improve_performances () # (1) See the welcome page for an explanation of this line","title":"A gentle demo"},{"location":"functionalities/generic/introduction/#loading-data","text":"Data loading is made easy by using the HiveData object. Simply give it the name of the database you want to use: database_name = \"MY_DATABASE_NAME\" from eds_scikit.io import HiveData data = HiveData ( database_name = \"database_name\" , ) Now your tables are available as Koalas DataFrames: Those are basically Spark DataFrames which allows for the Pandas API to be used on top (see the Project description of eds-scikit's documentation for more informations.) What we need to extract: Patients with diabetes Patients with Covid-19 Visits from those patients, and their ICU/Non-ICU status Let us import what's necessary from eds-scikit : from eds_scikit.event import conditions_from_icd10 from eds_scikit.event.diabetes import ( diabetes_from_icd10 , DEFAULT_DIABETE_FROM_ICD10_CONFIG , ) from eds_scikit.icu import tag_icu_visit DATE_MIN = datetime . datetime ( 2020 , 1 , 1 ) DATE_MAX = datetime . datetime ( 2021 , 6 , 1 )","title":"Loading data"},{"location":"functionalities/generic/introduction/#extracting-the-diabetic-status","text":"Luckily, a function is available to extract diabetic patients from ICD-10: diabetes = diabetes_from_icd10 ( condition_occurrence = data . condition_occurrence , visit_occurrence = data . visit_occurrence , date_min = DATE_MIN , date_max = DATE_MAX , ) We can check the default parameters used here: DEFAULT_DIABETE_FROM_ICD10_CONFIG {'additional_filtering': {'condition_status_source_value': {'DP', 'DAS'}}, 'codes': {'DIABETES_INSIPIDUS': {'code_list': ['E232', 'N251'], 'code_type': 'exact'}, 'DIABETES_IN_PREGNANCY': {'code_list': ['O24'], 'code_type': 'prefix'}, 'DIABETES_MALNUTRITION': {'code_list': ['E12'], 'code_type': 'prefix'}, 'DIABETES_TYPE_I': {'code_list': ['E10'], 'code_type': 'prefix'}, 'DIABETES_TYPE_II': {'code_list': ['E11'], 'code_type': 'prefix'}, 'OTHER_DIABETES_MELLITUS': {'code_list': ['E13', 'E14'], 'code_type': 'prefix'}}, 'date_from_visit': True, 'default_code_type': 'prefix'} We are only interested in diabetes mellitus , although we extracted other types of diabetes: diabetes . concept . value_counts () DIABETES_TYPE_II 117843 DIABETES_TYPE_I 10597 OTHER_DIABETES_MELLITUS 6031 DIABETES_IN_PREGNANCY 2597 DIABETES_INSIPIDUS 1089 DIABETES_MALNUTRITION 199 Name: concept, dtype: int64 We will restrict the types of diabetes used here: diabetes_cohort = ( diabetes [ diabetes . concept . isin ( { \"DIABETES_TYPE_I\" , \"DIABETES_TYPE_II\" , \"OTHER_DIABETES_MELLITUS\" , } ) ] . person_id . unique () . reset_index () ) diabetes_cohort . loc [:, \"HAS_DIABETE\" ] = True","title":"Extracting the diabetic status"},{"location":"functionalities/generic/introduction/#extracting-the-covid-status","text":"Using the conditions_from_icd10 function, we will extract visits linked to COVID-19: codes = dict ( COVID = dict ( code_list = r \"U071[0145]\" , code_type = \"regex\" , ) ) covid = conditions_from_icd10 ( condition_occurrence = data . condition_occurrence , visit_occurrence = data . visit_occurrence , codes = codes , date_min = DATE_MIN , date_max = DATE_MAX , ) Now we can go from the visit_occurrence level to the visit_detail level. visit_detail_covid = data . visit_detail . merge ( covid [[ \"visit_occurrence_id\" ]], on = \"visit_occurrence_id\" , how = \"inner\" , )","title":"Extracting the Covid status"},{"location":"functionalities/generic/introduction/#extracting-icu-visits","text":"What is left to do is to tag each visit as occurring in an ICU or not. This is achieved with the tag_icu_visit . Like many functions in eds-scikit , this function exposes an algo argument, allowing you to choose how the tagging is done. You can check the corresponding documentation to see the availables algos . visit_detail_covid = tag_icu_visit ( visit_detail = visit_detail_covid , care_site = data . care_site , algo = \"from_authorisation_type\" , ) visit_detail_covid = visit_detail_covid . merge ( diabetes_cohort , on = \"person_id\" , how = \"left\" ) visit_detail_covid [ \"HAS_DIABETE\" ] . fillna ( False , inplace = True ) visit_detail_covid [ \"IS_ICU\" ] . fillna ( False , inplace = True )","title":"Extracting ICU visits"},{"location":"functionalities/generic/introduction/#finishing-the-analysis","text":"","title":"Finishing the analysis"},{"location":"functionalities/generic/introduction/#adding-patients-age","text":"We will add the patient's age at each visit_detail : from eds_scikit.utils import datetime_helpers visit_detail_covid = visit_detail_covid . merge ( data . person [[ 'person_id' , 'birth_datetime' ]], on = 'person_id' , how = 'inner' ) visit_detail_covid [ \"age\" ] = ( datetime_helpers . substract_datetime ( visit_detail_covid [ \"visit_detail_start_datetime\" ], visit_detail_covid [ \"birth_datetime\" ], out = \"hours\" , ) / ( 24 * 365.25 ) )","title":"Adding patient's age"},{"location":"functionalities/generic/introduction/#from-distributed-koalas-to-local-pandas","text":"All the computing above was done using Koalas DataFrames, which are distributed. Now that we limited our cohort to a manageable size, we can switch to Pandas to finish our analysis. visit_detail_covid_pd = visit_detail_covid [ [ \"person_id\" , \"age\" , \"HAS_DIABETE\" , \"IS_ICU\" ] ] . to_pandas ()","title":"From distributed Koalas to local Pandas"},{"location":"functionalities/generic/introduction/#grouping-by-patient","text":"stats = ( visit_detail_covid_pd [[ \"person_id\" , \"age\" , \"HAS_DIABETE\" , \"IS_ICU\" ]] . groupby ( \"person_id\" ) . agg ( HAS_DIABETE = ( \"HAS_DIABETE\" , \"any\" ), IS_ICU = ( \"IS_ICU\" , \"any\" ), age = ( \"age\" , \"min\" ), ) )","title":"Grouping by patient"},{"location":"functionalities/generic/introduction/#binning-the-age-into-intervals","text":"stats [ \"age\" ] = pd . cut ( stats . age , bins = [ 0 , 40 , 50 , 60 , 70 , 120 ], labels = [ \"(0, 40]\" , \"(40, 50]\" , \"(50, 60]\" , \"(60, 70]\" , \"(70, 120]\" ], )","title":"Binning the age into intervals"},{"location":"functionalities/generic/introduction/#computing-the-ratio-of-patients-that-had-an-icu-visit","text":"stats = stats . groupby ([ \"age\" , \"HAS_DIABETE\" ], as_index = False ) . apply ( lambda x : x [ \"IS_ICU\" ] . sum () / len ( x ) ) stats . columns = [ \"age\" , \"cohorte\" , \"percent_icu\" ] stats [ \"cohorte\" ] = stats [ \"cohorte\" ] . replace ({ True : \"Diab.\" , False : \"Control\" })","title":"Computing the ratio of patients that had an ICU visit"},{"location":"functionalities/generic/introduction/#results","text":"stats .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } age cohorte percent_icu 0 (0, 40] Control 0.327988 1 (0, 40] Diab. 0.445578 2 (40, 50] Control 0.263667 3 (40, 50] Diab. 0.427203 4 (50, 60] Control 0.315931 5 (50, 60] Diab. 0.464736 6 (60, 70] Control 0.356808 7 (60, 70] Diab. 0.474766 8 (70, 120] Control 0.159337 9 (70, 120] Diab. 0.230180 We can finally plot our results using Altair : import altair as alt bars = ( alt . Chart ( stats , title = [ \"Percentage of patients who went through ICU during their COVID stay, \" , \"as a function of their age range and diabetic status\" , \" \" , ], ) . mark_bar () . encode ( x = alt . X ( \"cohorte:N\" , title = \"\" ), y = alt . Y ( \"percent_icu\" , title = \" % o f patients who went through ICU.\" , axis = alt . Axis ( format = \"%\" ), ), color = alt . Color ( \"cohorte:N\" , title = \"Cohort\" ), column = alt . Column ( \"age:N\" , title = \"Age range\" ), ) ) bars = bars . configure_title ( anchor = \"middle\" , baseline = \"bottom\" ) bars (function(spec, embedOpt){ let outputDiv = document.currentScript.previousElementSibling; if (outputDiv.id !== \"altair-viz-679a8662c76643b1ae8af86ce4171d2c\") { outputDiv = document.getElementById(\"altair-viz-679a8662c76643b1ae8af86ce4171d2c\"); } const paths = { \"vega\": \"https://cdn.jsdelivr.net/npm//vega@5?noext\", \"vega-lib\": \"https://cdn.jsdelivr.net/npm//vega-lib?noext\", \"vega-lite\": \"https://cdn.jsdelivr.net/npm//vega-lite@4.8.1?noext\", \"vega-embed\": \"https://cdn.jsdelivr.net/npm//vega-embed@6?noext\", }; function loadScript(lib) { return new Promise(function(resolve, reject) { var s = document.createElement('script'); s.src = paths[lib]; s.async = true; s.onload = () => resolve(paths[lib]); s.onerror = () => reject(`Error loading script: ${paths[lib]}`); document.getElementsByTagName(\"head\")[0].appendChild(s); }); } function showError(err) { outputDiv.innerHTML = `<div class=\"error\" style=\"color:red;\">${err}</div>`; throw err; } function displayChart(vegaEmbed) { vegaEmbed(outputDiv, spec, embedOpt) .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`)); } if(typeof define === \"function\" && define.amd) { requirejs.config({paths}); require([\"vega-embed\"], displayChart, err => showError(`Error loading script: ${err.message}`)); } else if (typeof vegaEmbed === \"function\") { displayChart(vegaEmbed); } else { loadScript(\"vega\") .then(() => loadScript(\"vega-lite\")) .then(() => loadScript(\"vega-embed\")) .catch(showError) .then(() => displayChart(vegaEmbed)); } })({\"config\": {\"view\": {\"continuousWidth\": 400, \"continuousHeight\": 300}, \"title\": {\"anchor\": \"middle\", \"baseline\": \"bottom\"}}, \"data\": {\"name\": \"data-55507a07f81645e51f63eaba5b403390\"}, \"mark\": \"bar\", \"encoding\": {\"color\": {\"type\": \"nominal\", \"field\": \"cohorte\", \"title\": \"Cohort\"}, \"column\": {\"type\": \"nominal\", \"field\": \"age\", \"title\": \"Age range\"}, \"x\": {\"type\": \"nominal\", \"field\": \"cohorte\", \"title\": \"\"}, \"y\": {\"type\": \"quantitative\", \"axis\": {\"format\": \"%\"}, \"field\": \"percent_icu\", \"title\": \"% of patients who went through ICU.\"}}, \"title\": [\"Percentage of patients who went through ICU during their COVID stay, \", \"as a function of their age range and diabetic status\", \" \"], \"$schema\": \"https://vega.github.io/schema/vega-lite/v4.8.1.json\", \"datasets\": {\"data-55507a07f81645e51f63eaba5b403390\": [{\"age\": \"(0, 40]\", \"cohorte\": \"Control\", \"percent_icu\": 0.32798833819241985}, {\"age\": \"(0, 40]\", \"cohorte\": \"Diab.\", \"percent_icu\": 0.445578231292517}, {\"age\": \"(40, 50]\", \"cohorte\": \"Control\", \"percent_icu\": 0.26366666666666666}, {\"age\": \"(40, 50]\", \"cohorte\": \"Diab.\", \"percent_icu\": 0.4272030651340996}, {\"age\": \"(50, 60]\", \"cohorte\": \"Control\", \"percent_icu\": 0.31593098812457987}, {\"age\": \"(50, 60]\", \"cohorte\": \"Diab.\", \"percent_icu\": 0.4647364513734224}, {\"age\": \"(60, 70]\", \"cohorte\": \"Control\", \"percent_icu\": 0.3568075117370892}, {\"age\": \"(60, 70]\", \"cohorte\": \"Diab.\", \"percent_icu\": 0.47476552032157215}, {\"age\": \"(70, 120]\", \"cohorte\": \"Control\", \"percent_icu\": 0.15933694181326116}, {\"age\": \"(70, 120]\", \"cohorte\": \"Diab.\", \"percent_icu\": 0.23017958826106}]}}, {\"mode\": \"vega-lite\"}); {\"state\": {}, \"version_major\": 2, \"version_minor\": 0}","title":"Results"},{"location":"functionalities/generic/io/","text":"(function() { function addWidgetsRenderer() { var requireJsScript = document.createElement('script'); requireJsScript.src = 'https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js'; var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var jupyterWidgetsScript = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} jupyterWidgetsScript.src = widgetRendererSrc; document.body.appendChild(requireJsScript); document.body.appendChild(jupyterWidgetsScript); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); You can download this notebook directly here IO: Getting Data 3 classes are available to facilitate data access: HiveData : Getting data from a Hive cluster, returning Koalas DataFrames PandasData : Getting data from tables saved on disk, returning Pandas DataFrames PostgresData : Getting data from a PostGreSQL DB, returning Pandas DataFrames from eds_scikit.io import HiveData , PandasData , PostgresData Loading from Hive: HiveData The HiveData class expects two parameters: A SparkSession variable The name of the Database to connect to Using Spark kernels All kernels designed to use Spark are configured to expose 3 variables at startup: spark , the current SparkSession sc , the current SparkContext sql , a function to execute SQL code on the Hive Database. In this case you can just provide the spark variable to HiveData ! Working with an I2B2 database To use a built-in I2B2 to OMOP connector, specify database_type=\"I2b2\" when instantiating HiveData If needed, the following snippet allows to create the necessary variables: from pyspark import SparkConf , SparkContext from pyspark.sql.session import SparkSession conf = SparkConf () sc = SparkContext ( conf = conf ) spark = SparkSession . builder \\ . enableHiveSupport () \\ . getOrCreate () sql = spark . sql The class HiveData provides a convenient interface to OMOP data stored in Hive. The OMOP tables can be accessed as attribute and they are represented as Koalas DataFrames . You simply need to mention your Hive database name. data = HiveData ( \"cse_210038_20221219\" , #DB_NAME, spark , database_type = \"I2B2\" , ) By default, only a subset of tables are added as attributes: data . available_tables ['concept', 'visit_detail', 'note_deid', 'person', 'care_site', 'visit_occurrence', 'measurement', 'procedure_occurrence', 'condition_occurrence', 'fact_relationship', 'concept_relationship'] Koalas DataFrames, like Spark DataFrames, rely on a lazy execution plan: As long as no data needs to be specifically collected, saved or displayed, no code is executed. It is simply saved for a later execution. The main interest of Koalas DataFrames is that you can use (most of) the Pandas API: person = data . person person . drop ( columns = [ 'person_id' ]) . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } birth_datetime death_datetime gender_source_value cdm_source 0 1946-06-04 NaT m ORBIS 1 1940-01-21 2018-05-07 m ORBIS 2 1979-04-25 NaT m ORBIS 3 2007-10-13 NaT f ORBIS 4 1964-12-27 NaT f ORBIS from datetime import datetime person [ 'is_over_50' ] = ( person [ 'birth_datetime' ] >= datetime ( 1971 , 1 , 1 )) stats = ( person . groupby ( 'is_over_50' ) . person_id . count () ) Once data has been sufficiently aggregated, it can be converted back to Pandas, e.g. for plotting. stats_pd = stats . to_pandas () stats_pd is_over_50 True 132794 False 66808 Name: person_id, dtype: int64 Similarily, if you want to work on the Spark DataFrame instead, a similar method is available: person_spark = person . to_spark () Persisting/Reading a sample to/from disk: PandasData Working with Pandas DataFrame is, when possible, more convenient. You have the possibility to save your database or at least a subset of it. Doing so allows you to work on it later without having to go through Spark again. Careful with cohort size Do not save it if your cohort is big : This saves all available tables on disk. For instance, let us define a dummy subset of 1000 patients: visits = data . visit_occurrence selected_visits = ( visits . loc [ visits [ \"visit_source_value\" ] == \"urgence\" ] ) sample_patients = ( selected_visits [ \"person_id\" ] . drop_duplicates () . head ( 1000 ) ) And save every table restricted to this small cohort as a parquet file: MY_FOLDER_PATH = \"./test_cohort\" import os folder = os . path . abspath ( MY_FOLDER_PATH ) tables_to_save = [ \"person\" , \"visit_detail\" , \"visit_occurrence\" ] data . persist_tables_to_folder ( folder , tables = tables_to_save , person_ids = sample_patients ) Once you saved some data to disk, a dedicated class can be used to access it: The class PandasData can be used to load OMOP data from a folder containing several parquet files. The tables are accessed as attributes and are returned as Pandas DataFrame. Warning : in this case, the whole table will be loaded into memory on a single jupyter server. Consequently it is advised to only use this for small datasets. data = PandasData ( folder ) data . available_tables ['visit_detail', 'visit_occurrence', 'person'] person = data . person print ( f \"type: { type ( person ) } \" ) print ( f \"shape: { person . shape } \" ) type: <class 'pandas.core.frame.DataFrame'> shape: (1000, 5) Loading from PostGres: PostgresData OMOP data can be stored in a PostgreSQL database. The PostgresData class provides a convinient interface to it. Note : this class relies on the file ~/.pgpass that contains your identifiers for several databases. data = PostgresData ( dbname = DB , schema = \"omop\" , user = USER , ) data . read_sql ( \"select count(*) from person\" ) {\"state\": {}, \"version_major\": 2, \"version_minor\": 0}","title":"Connectors"},{"location":"functionalities/generic/io/#io-getting-data","text":"3 classes are available to facilitate data access: HiveData : Getting data from a Hive cluster, returning Koalas DataFrames PandasData : Getting data from tables saved on disk, returning Pandas DataFrames PostgresData : Getting data from a PostGreSQL DB, returning Pandas DataFrames from eds_scikit.io import HiveData , PandasData , PostgresData","title":"IO: Getting Data"},{"location":"functionalities/generic/io/#loading-from-hive-hivedata","text":"The HiveData class expects two parameters: A SparkSession variable The name of the Database to connect to Using Spark kernels All kernels designed to use Spark are configured to expose 3 variables at startup: spark , the current SparkSession sc , the current SparkContext sql , a function to execute SQL code on the Hive Database. In this case you can just provide the spark variable to HiveData ! Working with an I2B2 database To use a built-in I2B2 to OMOP connector, specify database_type=\"I2b2\" when instantiating HiveData If needed, the following snippet allows to create the necessary variables: from pyspark import SparkConf , SparkContext from pyspark.sql.session import SparkSession conf = SparkConf () sc = SparkContext ( conf = conf ) spark = SparkSession . builder \\ . enableHiveSupport () \\ . getOrCreate () sql = spark . sql The class HiveData provides a convenient interface to OMOP data stored in Hive. The OMOP tables can be accessed as attribute and they are represented as Koalas DataFrames . You simply need to mention your Hive database name. data = HiveData ( \"cse_210038_20221219\" , #DB_NAME, spark , database_type = \"I2B2\" , ) By default, only a subset of tables are added as attributes: data . available_tables ['concept', 'visit_detail', 'note_deid', 'person', 'care_site', 'visit_occurrence', 'measurement', 'procedure_occurrence', 'condition_occurrence', 'fact_relationship', 'concept_relationship'] Koalas DataFrames, like Spark DataFrames, rely on a lazy execution plan: As long as no data needs to be specifically collected, saved or displayed, no code is executed. It is simply saved for a later execution. The main interest of Koalas DataFrames is that you can use (most of) the Pandas API: person = data . person person . drop ( columns = [ 'person_id' ]) . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } birth_datetime death_datetime gender_source_value cdm_source 0 1946-06-04 NaT m ORBIS 1 1940-01-21 2018-05-07 m ORBIS 2 1979-04-25 NaT m ORBIS 3 2007-10-13 NaT f ORBIS 4 1964-12-27 NaT f ORBIS from datetime import datetime person [ 'is_over_50' ] = ( person [ 'birth_datetime' ] >= datetime ( 1971 , 1 , 1 )) stats = ( person . groupby ( 'is_over_50' ) . person_id . count () ) Once data has been sufficiently aggregated, it can be converted back to Pandas, e.g. for plotting. stats_pd = stats . to_pandas () stats_pd is_over_50 True 132794 False 66808 Name: person_id, dtype: int64 Similarily, if you want to work on the Spark DataFrame instead, a similar method is available: person_spark = person . to_spark ()","title":"Loading from Hive: HiveData"},{"location":"functionalities/generic/io/#persistingreading-a-sample-tofrom-disk-pandasdata","text":"Working with Pandas DataFrame is, when possible, more convenient. You have the possibility to save your database or at least a subset of it. Doing so allows you to work on it later without having to go through Spark again. Careful with cohort size Do not save it if your cohort is big : This saves all available tables on disk. For instance, let us define a dummy subset of 1000 patients: visits = data . visit_occurrence selected_visits = ( visits . loc [ visits [ \"visit_source_value\" ] == \"urgence\" ] ) sample_patients = ( selected_visits [ \"person_id\" ] . drop_duplicates () . head ( 1000 ) ) And save every table restricted to this small cohort as a parquet file: MY_FOLDER_PATH = \"./test_cohort\" import os folder = os . path . abspath ( MY_FOLDER_PATH ) tables_to_save = [ \"person\" , \"visit_detail\" , \"visit_occurrence\" ] data . persist_tables_to_folder ( folder , tables = tables_to_save , person_ids = sample_patients ) Once you saved some data to disk, a dedicated class can be used to access it: The class PandasData can be used to load OMOP data from a folder containing several parquet files. The tables are accessed as attributes and are returned as Pandas DataFrame. Warning : in this case, the whole table will be loaded into memory on a single jupyter server. Consequently it is advised to only use this for small datasets. data = PandasData ( folder ) data . available_tables ['visit_detail', 'visit_occurrence', 'person'] person = data . person print ( f \"type: { type ( person ) } \" ) print ( f \"shape: { person . shape } \" ) type: <class 'pandas.core.frame.DataFrame'> shape: (1000, 5)","title":"Persisting/Reading a sample to/from disk: PandasData"},{"location":"functionalities/generic/io/#loading-from-postgres-postgresdata","text":"OMOP data can be stored in a PostgreSQL database. The PostgresData class provides a convinient interface to it. Note : this class relies on the file ~/.pgpass that contains your identifiers for several databases. data = PostgresData ( dbname = DB , schema = \"omop\" , user = USER , ) data . read_sql ( \"select count(*) from person\" ) {\"state\": {}, \"version_major\": 2, \"version_minor\": 0}","title":"Loading from PostGres: PostgresData"},{"location":"functionalities/omop-explorer/","text":"OMOP Explorer The OMOP Explorer module of eds-scikit supports data scientists working on OMOP data. Its main objective is to allow quick OMOP tables exploration to get familiar with table values and their associated distributions. Quick use - OMOP Quick use for OMOP dataset exploration. Quick use - general Quick use for any dataframe exploration.","title":"OMOP Explorer"},{"location":"functionalities/omop-explorer/#omop-explorer","text":"The OMOP Explorer module of eds-scikit supports data scientists working on OMOP data. Its main objective is to allow quick OMOP tables exploration to get familiar with table values and their associated distributions. Quick use - OMOP Quick use for OMOP dataset exploration. Quick use - general Quick use for any dataframe exploration.","title":"OMOP Explorer"},{"location":"functionalities/omop-explorer/quick-use-general/","text":"Quick use General This tutorial demonstrates how the biology module can be quickly used to map measurement codes. Big volume Measurement table can be large. Do not forget to set proper spark config before loading data. Mapping measurement table to ANABIO codes Defining Concept-Set To define a concept-set variable you just need to specify a terminology and a set of codes. from eds_scikit.biology import prepare_measurement_table , ConceptsSet custom_leukocytes = ConceptsSet ( \"Custom_Leukocytes\" ) custom_leukocytes . add_concept_codes ( concept_codes = [ \"A0174\" , \"H6740\" ], terminology = \"GLIMS_ANABIO\" # (1) ) custom_leukocytes . add_concept_codes ( concept_codes = [ \"6690-2\" ], terminology = \"ITM_LOINC\" # (2) ) Codes must be given with terminology. Available terminologies can be accessed with eds_scikit.io.settings.measurement_config['source_terminologies'] . See. AP-HP biology for details on the AP-HP setting. Codes must be given with terminology. Available terminologies can be accessed with eds_scikit.io.settings.measurement_config['source_terminologies'] . See. AP-HP biology for details on the AP-HP setting. Preparing measurement table Then, simply run prepare_measurement_table to select the measurements from your concept set. measurement = prepare_measurement_table ( data , start_date = \"2022-01-01\" , end_date = \"2022-05-01\" , concept_sets = [ custom_leukocytes ], convert_units = False , get_all_terminologies = True , )","title":"Quick use General"},{"location":"functionalities/omop-explorer/quick-use-general/#quick-use-general","text":"This tutorial demonstrates how the biology module can be quickly used to map measurement codes. Big volume Measurement table can be large. Do not forget to set proper spark config before loading data.","title":"Quick use General"},{"location":"functionalities/omop-explorer/quick-use-general/#mapping-measurement-table-to-anabio-codes","text":"","title":"Mapping measurement table to ANABIO codes"},{"location":"functionalities/omop-explorer/quick-use-general/#defining-concept-set","text":"To define a concept-set variable you just need to specify a terminology and a set of codes. from eds_scikit.biology import prepare_measurement_table , ConceptsSet custom_leukocytes = ConceptsSet ( \"Custom_Leukocytes\" ) custom_leukocytes . add_concept_codes ( concept_codes = [ \"A0174\" , \"H6740\" ], terminology = \"GLIMS_ANABIO\" # (1) ) custom_leukocytes . add_concept_codes ( concept_codes = [ \"6690-2\" ], terminology = \"ITM_LOINC\" # (2) ) Codes must be given with terminology. Available terminologies can be accessed with eds_scikit.io.settings.measurement_config['source_terminologies'] . See. AP-HP biology for details on the AP-HP setting. Codes must be given with terminology. Available terminologies can be accessed with eds_scikit.io.settings.measurement_config['source_terminologies'] . See. AP-HP biology for details on the AP-HP setting.","title":"Defining Concept-Set"},{"location":"functionalities/omop-explorer/quick-use-general/#preparing-measurement-table","text":"Then, simply run prepare_measurement_table to select the measurements from your concept set. measurement = prepare_measurement_table ( data , start_date = \"2022-01-01\" , end_date = \"2022-05-01\" , concept_sets = [ custom_leukocytes ], convert_units = False , get_all_terminologies = True , )","title":"Preparing measurement table"},{"location":"functionalities/omop-explorer/quick-use-omop/","text":"Quick use OMOP This tutorial demonstrates how the biology module can be quickly used to map measurement codes. Big volume Measurement table can be large. Do not forget to set proper spark config before loading data. Mapping measurement table to ANABIO codes Defining Concept-Set To define a concept-set variable you just need to specify a terminology and a set of codes. from eds_scikit.biology import prepare_measurement_table , ConceptsSet custom_leukocytes = ConceptsSet ( \"Custom_Leukocytes\" ) custom_leukocytes . add_concept_codes ( concept_codes = [ \"A0174\" , \"H6740\" ], terminology = \"GLIMS_ANABIO\" # (1) ) custom_leukocytes . add_concept_codes ( concept_codes = [ \"6690-2\" ], terminology = \"ITM_LOINC\" # (2) ) Codes must be given with terminology. Available terminologies can be accessed with eds_scikit.io.settings.measurement_config['source_terminologies'] . See. AP-HP biology for details on the AP-HP setting. Codes must be given with terminology. Available terminologies can be accessed with eds_scikit.io.settings.measurement_config['source_terminologies'] . See. AP-HP biology for details on the AP-HP setting. Preparing measurement table Then, simply run prepare_measurement_table to select the measurements from your concept set. measurement = prepare_measurement_table ( data , start_date = \"2022-01-01\" , end_date = \"2022-05-01\" , concept_sets = [ custom_leukocytes ], convert_units = False , get_all_terminologies = True , )","title":"Quick use OMOP"},{"location":"functionalities/omop-explorer/quick-use-omop/#quick-use-omop","text":"This tutorial demonstrates how the biology module can be quickly used to map measurement codes. Big volume Measurement table can be large. Do not forget to set proper spark config before loading data.","title":"Quick use OMOP"},{"location":"functionalities/omop-explorer/quick-use-omop/#mapping-measurement-table-to-anabio-codes","text":"","title":"Mapping measurement table to ANABIO codes"},{"location":"functionalities/omop-explorer/quick-use-omop/#defining-concept-set","text":"To define a concept-set variable you just need to specify a terminology and a set of codes. from eds_scikit.biology import prepare_measurement_table , ConceptsSet custom_leukocytes = ConceptsSet ( \"Custom_Leukocytes\" ) custom_leukocytes . add_concept_codes ( concept_codes = [ \"A0174\" , \"H6740\" ], terminology = \"GLIMS_ANABIO\" # (1) ) custom_leukocytes . add_concept_codes ( concept_codes = [ \"6690-2\" ], terminology = \"ITM_LOINC\" # (2) ) Codes must be given with terminology. Available terminologies can be accessed with eds_scikit.io.settings.measurement_config['source_terminologies'] . See. AP-HP biology for details on the AP-HP setting. Codes must be given with terminology. Available terminologies can be accessed with eds_scikit.io.settings.measurement_config['source_terminologies'] . See. AP-HP biology for details on the AP-HP setting.","title":"Defining Concept-Set"},{"location":"functionalities/omop-explorer/quick-use-omop/#preparing-measurement-table","text":"Then, simply run prepare_measurement_table to select the measurements from your concept set. measurement = prepare_measurement_table ( data , start_date = \"2022-01-01\" , end_date = \"2022-05-01\" , concept_sets = [ custom_leukocytes ], convert_units = False , get_all_terminologies = True , )","title":"Preparing measurement table"},{"location":"functionalities/patients-course/consultation_dates/","text":"When a patient comes multiple times for consultations, it is often represented as a single visit_occurrence in the CDW. If a clear history of a patient's course is needed, it is then necessary to use proxies in order to access this information. An available proxy to get those consultation dates is to check for the existence of consultation reports and use the associated reports dates. To this extend, two methods are available. They can be combined or used separately: Use the note_datetime field associated to each consultation report Extract the consultation report date by using NLP An important remark Be careful when using the note_datetime field as it can represent the date of modification of a document (i.e. it can be modified if the clinician adds some information in it in the future). from eds_scikit.io import HiveData data = HiveData ( DBNAME ) from eds_scikit.event import get_consultation_dates get_consultation_dates ( data . visit_occurrence , note = data . note , note_nlp = note_nlp , algo = [ \"nlp\" ], ) The snippet above required us to generate a note_nlp with a consultation_date column (see below for more informations). Consultation pipe A consultation date pipeline exists and is particulary suited for this task. Moreover, methods are available to run an EDS-NLP pipeline on a Pandas, Spark or even Koalas DataFrame ! We can check the various exposed parameters if needed: Extract consultation dates. See the implementation details of the algo(s) you want to use PARAMETER DESCRIPTION vo visit_occurrence DataFrame TYPE: DataFrame note note DataFrame TYPE: DataFrame note_nlp note_nlp DataFrame, used only with the \"nlp\" algo TYPE: Optional [ DataFrame ] DEFAULT: None algo Algorithm(s) to use to determine consultation dates. Multiple algorithms can be provided as a list. Accepted values are: \"structured\" : See get_consultation_dates_structured() \"nlp\" : See get_consultation_dates_nlp() TYPE: Union [ str , List [ str ]] DEFAULT: ['nlp'] max_timedelta If two extracted consultations are spaced by less than max_timedelta , we consider that they correspond to the same event and only keep the first one. TYPE: timedelta DEFAULT: timedelta(days=7) structured_config A dictionnary of parameters when using the structured algorithm TYPE: Dict [ str , Any ] DEFAULT: dict() nlp_config A dictionnary of parameters when using the nlp algorithm TYPE: Dict [ str , Any ] DEFAULT: dict() RETURNS DESCRIPTION DataFrame Event type DataFrame with the following columns: person_id visit_occurrence_id CONSULTATION_DATE : corresponds to the note_datetime value of a consultation report coming from the considered visit. CONSULTATION_NOTE_ID : the note_id of the corresponding report. CONSULTATION_DATE_EXTRACTION : the method of extraction Availables algorithms (values for \"algo\" ) 'nlp' 'structured' Uses consultation dates extracted a priori in consultation reports to infer true consultation dates PARAMETER DESCRIPTION note_nlp A DataFrame with (at least) the following columns: note_id consultation_date end if using dates_to_keep=first : end should store the character offset of the extracted date. TYPE: DataFrame dates_to_keep How to handle multiple consultation dates found in the document: min : keep the oldest one first : keep the occurrence that appeared first in the text all : keep all date TYPE: str , optional DEFAULT: 'min' RETURNS DESCRIPTION Dataframe With 2 added columns corresponding to the following concept: CONSULTATION_DATE , containing the date CONSULTATION_DATE_EXTRACTION , containing \"NLP\" Source code in eds_scikit/event/consultations.py 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 def get_consultation_dates_nlp ( note_nlp : DataFrame , dates_to_keep : str = \"min\" , ) -> DataFrame : \"\"\" Uses consultation dates extracted *a priori* in consultation reports to infer *true* consultation dates Parameters ---------- note_nlp : DataFrame A DataFrame with (at least) the following columns: - `note_id` - `consultation_date` - `end` **if** using `dates_to_keep=first`: `end` should store the character offset of the extracted date. dates_to_keep : str, optional How to handle multiple consultation dates found in the document: - `min`: keep the oldest one - `first`: keep the occurrence that appeared first in the text - `all`: keep all date Returns ------- Dataframe With 2 added columns corresponding to the following concept: - `CONSULTATION_DATE`, containing the date - `CONSULTATION_DATE_EXTRACTION`, containing `\"NLP\"` \"\"\" if dates_to_keep == \"min\" : dates_per_note = note_nlp . groupby ( \"note_id\" ) . agg ( CONSULTATION_DATE = ( \"consultation_date\" , \"min\" ), ) elif dates_to_keep == \"first\" : dates_per_note = ( note_nlp . sort_values ( by = \"start\" ) . groupby ( \"note_id\" ) . agg ( CONSULTATION_DATE = ( \"consultation_date\" , \"first\" )) ) elif dates_to_keep == \"all\" : dates_per_note = note_nlp [[ \"consultation_date\" , \"note_id\" ]] . set_index ( \"note_id\" ) dates_per_note = dates_per_note . rename ( columns = { \"consultation_date\" : \"CONSULTATION_DATE\" } ) dates_per_note [ \"CONSULTATION_DATE_EXTRACTION\" ] = \"NLP\" return dates_per_note Uses note_datetime value to infer true consultation dates PARAMETER DESCRIPTION note A note DataFrame with at least the following columns: note_id note_datetime note_source_value if kept_note_class_source_value is not None visit_occurrence_id if kept_visit_source_value is not None TYPE: DataFrame vo A visit_occurrence DataFrame to provide if kept_visit_source_value is not None , with at least the following columns: visit_occurrence_id visit_source_value if kept_visit_source_value is not None TYPE: Optional [ DataFrame ] DEFAULT: None kept_note_class_source_value Value(s) allowed for the note_class_source_value column. TYPE: Optional [ Union [ str , List [ str ]]] DEFAULT: 'CR-CONS' kept_visit_source_value Value(s) allowed for the visit_source_value column. TYPE: Optional [ Union [ str , List [ str ]]], optional DEFAULT: 'consultation externe' RETURNS DESCRIPTION Dataframe With 2 added columns corresponding to the following concept: CONSULTATION_DATE , containing the date CONSULTATION_DATE_EXTRACTION , containing \"STRUCTURED\" Source code in eds_scikit/event/consultations.py 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 def get_consultation_dates_structured ( note : DataFrame , vo : Optional [ DataFrame ] = None , kept_note_class_source_value : Optional [ Union [ str , List [ str ]]] = \"CR-CONS\" , kept_visit_source_value : Optional [ Union [ str , List [ str ]]] = \"consultation externe\" , ) -> DataFrame : \"\"\" Uses `note_datetime` value to infer *true* consultation dates Parameters ---------- note : DataFrame A `note` DataFrame with at least the following columns: - `note_id` - `note_datetime` - `note_source_value` **if** `kept_note_class_source_value is not None` - `visit_occurrence_id` **if** `kept_visit_source_value is not None` vo : Optional[DataFrame] A visit_occurrence DataFrame to provide **if** `kept_visit_source_value is not None`, with at least the following columns: - `visit_occurrence_id` - `visit_source_value` **if** `kept_visit_source_value is not None` kept_note_class_source_value : Optional[Union[str, List[str]]] Value(s) allowed for the `note_class_source_value` column. kept_visit_source_value : Optional[Union[str, List[str]]], optional Value(s) allowed for the `visit_source_value` column. Returns ------- Dataframe With 2 added columns corresponding to the following concept: - `CONSULTATION_DATE`, containing the date - `CONSULTATION_DATE_EXTRACTION`, containing `\"STRUCTURED\"` \"\"\" kept_note = note if kept_note_class_source_value is not None : if type ( kept_note_class_source_value ) == str : kept_note_class_source_value = [ kept_note_class_source_value ] kept_note = note [ note . note_class_source_value . isin ( set ( kept_note_class_source_value )) ] if kept_visit_source_value is not None : if type ( kept_visit_source_value ) == str : kept_visit_source_value = [ kept_visit_source_value ] kept_note = kept_note . merge ( vo [ [ \"visit_occurrence_id\" , \"visit_source_value\" , ] ][ vo . visit_source_value . isin ( set ( kept_visit_source_value ))], on = \"visit_occurrence_id\" , ) dates_per_note = kept_note [[ \"note_datetime\" , \"note_id\" ]] . rename ( columns = { \"note_datetime\" : \"CONSULTATION_DATE\" , } ) dates_per_note [ \"CONSULTATION_DATE_EXTRACTION\" ] = \"STRUCTURED\" return dates_per_note . set_index ( \"note_id\" )","title":"Consultation dates"},{"location":"functionalities/patients-course/is_emergency/","text":"eds-scikit provides a function to tag care sites as being medical emergency units . It also provides a higher-level function to directly tag visits. from eds_scikit.io import HiveData data = HiveData ( DBNAME ) Tagging care sites Tagging is done using the tag_emergency_care_site function: from eds_scikit.emergency import tag_emergency_care_site Tag care sites that correspond to medical emergency units . The tagging is done by adding a \"IS_EMERGENCY\" column to the provided DataFrame. Some algos can add an additional \"EMERGENCY_TYPE\" column to the provided DataFrame, providing a more detailled classification. PARAMETER DESCRIPTION care_site TYPE: DataFrame algo Possible values are: \"from_mapping\" relies on a list of care_site_source_value extracted by Judith LEBLANC, Ariel COHEN and validated by an ER doctor. The emergency care sites are here further labelled to distinguish the different types of emergency \"from_regex_on_care_site_description\" : relies on a specific list of RegEx applied on the description (= simplified care site name) of each care site. \"from_regex_on_parent_UF\" : relies on a specific list of regular expressions applied on the description (= simplified care site name) of each UF (Unit\u00e9 Fonctionnelle). The obtained tag is then propagated to every UF's children. TYPE: str DEFAULT: 'from_mapping' RETURNS DESCRIPTION care_site Dataframe with 1 to 2 added columns corresponding to the following concepts: \"IS_EMERGENCY\" \"EMERGENCY_TYPE\" (if using algo \"from_mapping\" ) TYPE: DataFrame Simply call the function by providing the necessary data (see below) and by picking the algo care_site = tag_emergency_care_site ( care_site = data . care_site , algo = \"from_mapping\" , ) Availables algorithms (values for \"algo\" ) 'from_mapping' 'from_regex_on_parent_UF' 'from_regex_on_care_site_description' This algo uses a labelled list of 201 emergency care sites. Those care sites were extracted and verified by Ariel COHEN, Judith LEBLANC, and an ER doctor validated them. Those emergency care sites are further divised into different categories, as defined in the concept 'EMERGENCY_TYPE'. The different categories are: Urgences sp\u00e9cialis\u00e9es UHCD + Post-urgences Urgences p\u00e9diatriques Urgences g\u00e9n\u00e9rales adulte Consultation urgences SAMU / SMUR See the dataset here PARAMETER DESCRIPTION care_site Should at least contains the care_site_source_value column TYPE: DataFrame version Optional version string for the mapping TYPE: Optional [ str ] DEFAULT: None RETURNS DESCRIPTION care_site Dataframe with 2 added columns corresponding to the following concepts: \"IS_EMERGENCY\" \"EMERGENCY_TYPE\" TYPE: DataFrame Source code in eds_scikit/emergency/emergency_care_site.py 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 @concept_checker ( concepts = [ \"IS_EMERGENCY\" , \"EMERGENCY_TYPE\" ]) def from_mapping ( care_site : DataFrame , version : Optional [ str ] = None , ) -> DataFrame : \"\"\"This algo uses a labelled list of 201 emergency care sites. Those care sites were extracted and verified by Ariel COHEN, Judith LEBLANC, and an ER doctor validated them. Those emergency care sites are further divised into different categories, as defined in the concept 'EMERGENCY_TYPE'. The different categories are: - Urgences sp\u00e9cialis\u00e9es - UHCD + Post-urgences - Urgences p\u00e9diatriques - Urgences g\u00e9n\u00e9rales adulte - Consultation urgences - SAMU / SMUR See the dataset [here](/datasets/care-site-emergency) Parameters ---------- care_site: DataFrame Should at least contains the `care_site_source_value` column version: Optional[str] Optional version string for the mapping Returns ------- care_site: DataFrame Dataframe with 2 added columns corresponding to the following concepts: - `\"IS_EMERGENCY\"` - `\"EMERGENCY_TYPE\"` \"\"\" function_name = \"get_care_site_emergency_mapping\" if version is not None : function_name += f \". { version } \" mapping = registry . get ( \"data\" , function_name = function_name )() # Getting the right framework fw = framework . get_framework ( care_site ) mapping = framework . to ( fw , mapping ) care_site = care_site . merge ( mapping , how = \"left\" , on = \"care_site_source_value\" , ) care_site [ \"IS_EMERGENCY\" ] = care_site [ \"EMERGENCY_TYPE\" ] . notna () return care_site Use regular expressions on parent UF (Unit\u00e9 Fonctionnelle) to classify emergency care site. This relies on this function . The regular expression used to detect emergency status is r\"\bURG|\bSAU\b|\bUHCD\b|\bZHTCD\b\" PARAMETER DESCRIPTION care_site Should at least contains the care_site_name column TYPE: DataFrame RETURNS DESCRIPTION care_site Dataframe with 1 added column corresponding to the following concept: 'IS_EMERGENCY' TYPE: DataFrame Source code in eds_scikit/emergency/emergency_care_site.py 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 @concept_checker ( concepts = [ \"IS_EMERGENCY\" ]) def from_regex_on_parent_UF ( care_site : DataFrame ) -> DataFrame : \"\"\"Use regular expressions on parent UF (Unit\u00e9 Fonctionnelle) to classify emergency care site. This relies on [this function][eds_scikit.structures.attributes.get_parent_attributes]. The regular expression used to detect emergency status is `r\"\\bURG|\\bSAU\\b|\\bUHCD\\b|\\bZHTCD\\b\"` Parameters ---------- care_site: DataFrame Should at least contains the `care_site_name` column Returns ------- care_site: DataFrame Dataframe with 1 added column corresponding to the following concept: - 'IS_EMERGENCY' \"\"\" return attributes . get_parent_attributes ( care_site , only_attributes = [ \"IS_EMERGENCY\" ], parent_type = \"Unit\u00e9 Fonctionnelle (UF)\" , ) Use regular expressions on care_site_name to decide if it an emergency care site. This relies on this function . The regular expression used to detect emergency status is r\"\bURG|\bSAU\b|\bUHCDb\b|\bZHTCD\b\" PARAMETER DESCRIPTION care_site Should at least contains the care_site_name column TYPE: DataFrame RETURNS DESCRIPTION care_site Dataframe with 1 added column corresponding to the following concept: \"IS_EMERGENCY\" TYPE: DataFrame Source code in eds_scikit/emergency/emergency_care_site.py 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 def from_regex_on_care_site_description ( care_site : DataFrame ) -> DataFrame : \"\"\"Use regular expressions on `care_site_name` to decide if it an emergency care site. This relies on [this function][eds_scikit.structures.attributes.add_care_site_attributes]. The regular expression used to detect emergency status is `r\"\\bURG|\\bSAU\\b|\\bUHCDb\\b|\\bZHTCD\\b\"` Parameters ---------- care_site: DataFrame Should at least contains the `care_site_name` column Returns ------- care_site: DataFrame Dataframe with 1 added column corresponding to the following concept: - `\"IS_EMERGENCY\"` \"\"\" return attributes . add_care_site_attributes ( care_site , only_attributes = [ \"IS_EMERGENCY\" ] ) Tagging visits Tagging is done using the tag_emergency_visit function: from eds_scikit.emergency import tag_emergency_visit Tag visits that correspond to medical emergency units . The tagging is done by adding a \"IS_EMERGENCY\" column to the provided DataFrame. Some algos can add an additional \"EMERGENCY_TYPE\" column to the provided DataFrame, providing a more detailled classification. It works by either tagging each visit detail's care site , or by using the visit_occurrence 's \"visit_source_value\" . PARAMETER DESCRIPTION visit_detail TYPE: DataFrame care_site Isn't necessary if the algo \"from_vo_visit_source_value\" is used TYPE: Optional [ DataFrame ] DEFAULT: None visit_occurrence Is mandatory if the algo \"from_vo_visit_source_value\" is used TYPE: Optional [ DataFrame ] DEFAULT: None algo Possible values are: \"from_mapping\" relies on a list of care_site_source_value extracted by Judith LEBLANC, Ariel COHEN and validated by an ER doctor. The emergency care sites are here further labelled to distinguish the different types of emergency \"from_regex_on_care_site_description\" : relies on a specific list of RegEx applied on the description (= simplified care site name) of each care site. \"from_regex_on_parent_UF\" : relies on a specific list of regular expressions applied on the description (= simplified care site name) of each UF (Unit\u00e9 Fonctionnelle). The obtained tag is then propagated to every UF's children. \"from_vo_visit_source_value\" : relies on the parent visit occurrence of each visit detail: A visit detail will be tagged as emergency if it belongs to a visit occurrence where visit_occurrence.visit_source_value=='urgence' . TYPE: str DEFAULT: 'from_mapping' RETURNS DESCRIPTION care_site Dataframe with 1 to 2 added columns corresponding to the following concepts: \"IS_EMERGENCY\" \"EMERGENCY_TYPE\" (if using algo \"from_mapping\" ) TYPE: DataFrame Simply call the function by providing the necessary data (see below) and by picking the algo visit_detail = tag_emergency_visit ( visit_detail = data . visit_detail , algo = \"from_mapping\" , ) Availables algorithms (values for \"algo\" ) 'from_mapping' 'from_regex_on_parent_UF' 'from_regex_on_care_site_description' 'from_vo_visit_source_value' This algo uses a labelled list of 201 emergency care sites. Those care sites were extracted and verified by Ariel COHEN, Judith LEBLANC, and an ER doctor validated them. Those emergency care sites are further divised into different categories, as defined in the concept 'EMERGENCY_TYPE'. The different categories are: Urgences sp\u00e9cialis\u00e9es UHCD + Post-urgences Urgences p\u00e9diatriques Urgences g\u00e9n\u00e9rales adulte Consultation urgences SAMU / SMUR See the dataset here PARAMETER DESCRIPTION care_site Should at least contains the care_site_source_value column TYPE: DataFrame version Optional version string for the mapping TYPE: Optional [ str ] DEFAULT: None RETURNS DESCRIPTION care_site Dataframe with 2 added columns corresponding to the following concepts: \"IS_EMERGENCY\" \"EMERGENCY_TYPE\" TYPE: DataFrame Source code in eds_scikit/emergency/emergency_care_site.py 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 @concept_checker ( concepts = [ \"IS_EMERGENCY\" , \"EMERGENCY_TYPE\" ]) def from_mapping ( care_site : DataFrame , version : Optional [ str ] = None , ) -> DataFrame : \"\"\"This algo uses a labelled list of 201 emergency care sites. Those care sites were extracted and verified by Ariel COHEN, Judith LEBLANC, and an ER doctor validated them. Those emergency care sites are further divised into different categories, as defined in the concept 'EMERGENCY_TYPE'. The different categories are: - Urgences sp\u00e9cialis\u00e9es - UHCD + Post-urgences - Urgences p\u00e9diatriques - Urgences g\u00e9n\u00e9rales adulte - Consultation urgences - SAMU / SMUR See the dataset [here](/datasets/care-site-emergency) Parameters ---------- care_site: DataFrame Should at least contains the `care_site_source_value` column version: Optional[str] Optional version string for the mapping Returns ------- care_site: DataFrame Dataframe with 2 added columns corresponding to the following concepts: - `\"IS_EMERGENCY\"` - `\"EMERGENCY_TYPE\"` \"\"\" function_name = \"get_care_site_emergency_mapping\" if version is not None : function_name += f \". { version } \" mapping = registry . get ( \"data\" , function_name = function_name )() # Getting the right framework fw = framework . get_framework ( care_site ) mapping = framework . to ( fw , mapping ) care_site = care_site . merge ( mapping , how = \"left\" , on = \"care_site_source_value\" , ) care_site [ \"IS_EMERGENCY\" ] = care_site [ \"EMERGENCY_TYPE\" ] . notna () return care_site Use regular expressions on parent UF (Unit\u00e9 Fonctionnelle) to classify emergency care site. This relies on this function . The regular expression used to detect emergency status is r\"\bURG|\bSAU\b|\bUHCD\b|\bZHTCD\b\" PARAMETER DESCRIPTION care_site Should at least contains the care_site_name column TYPE: DataFrame RETURNS DESCRIPTION care_site Dataframe with 1 added column corresponding to the following concept: 'IS_EMERGENCY' TYPE: DataFrame Source code in eds_scikit/emergency/emergency_care_site.py 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 @concept_checker ( concepts = [ \"IS_EMERGENCY\" ]) def from_regex_on_parent_UF ( care_site : DataFrame ) -> DataFrame : \"\"\"Use regular expressions on parent UF (Unit\u00e9 Fonctionnelle) to classify emergency care site. This relies on [this function][eds_scikit.structures.attributes.get_parent_attributes]. The regular expression used to detect emergency status is `r\"\\bURG|\\bSAU\\b|\\bUHCD\\b|\\bZHTCD\\b\"` Parameters ---------- care_site: DataFrame Should at least contains the `care_site_name` column Returns ------- care_site: DataFrame Dataframe with 1 added column corresponding to the following concept: - 'IS_EMERGENCY' \"\"\" return attributes . get_parent_attributes ( care_site , only_attributes = [ \"IS_EMERGENCY\" ], parent_type = \"Unit\u00e9 Fonctionnelle (UF)\" , ) Use regular expressions on care_site_name to decide if it an emergency care site. This relies on this function . The regular expression used to detect emergency status is r\"\bURG|\bSAU\b|\bUHCDb\b|\bZHTCD\b\" PARAMETER DESCRIPTION care_site Should at least contains the care_site_name column TYPE: DataFrame RETURNS DESCRIPTION care_site Dataframe with 1 added column corresponding to the following concept: \"IS_EMERGENCY\" TYPE: DataFrame Source code in eds_scikit/emergency/emergency_care_site.py 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 def from_regex_on_care_site_description ( care_site : DataFrame ) -> DataFrame : \"\"\"Use regular expressions on `care_site_name` to decide if it an emergency care site. This relies on [this function][eds_scikit.structures.attributes.add_care_site_attributes]. The regular expression used to detect emergency status is `r\"\\bURG|\\bSAU\\b|\\bUHCDb\\b|\\bZHTCD\\b\"` Parameters ---------- care_site: DataFrame Should at least contains the `care_site_name` column Returns ------- care_site: DataFrame Dataframe with 1 added column corresponding to the following concept: - `\"IS_EMERGENCY\"` \"\"\" return attributes . add_care_site_attributes ( care_site , only_attributes = [ \"IS_EMERGENCY\" ] ) This algo uses the \"Type de dossier\" of each visit detail's parent visit occurrence. Thus, a visit_detail will be tagged with IS_EMERGENCY=True iff the visit occurrence it belongs to is an emergency-type visit (meaning that visit_occurrence.visit_source_value=='urgence' ) Admission through ICU At AP-HP, when a patient is hospitalized after coming to the ICU, its visit_source_value is set from \"urgence\" to \"hospitalisation compl\u00e8te\" . So you should keep in mind that this method doesn't tag those visits as ICU. PARAMETER DESCRIPTION visit_detail TYPE: DataFrame visit_occurrence TYPE: DataFrame RETURNS DESCRIPTION visit_detail Dataframe with added columns corresponding to the following conceps: \"IS_EMERGENCY\" TYPE: DataFrame Source code in eds_scikit/emergency/emergency_visit.py 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 @concept_checker ( concepts = [ \"IS_EMERGENCY\" ]) def from_vo_visit_source_value ( visit_detail : DataFrame , visit_occurrence : DataFrame , ) -> DataFrame : \"\"\" This algo uses the *\"Type de dossier\"* of each visit detail's parent visit occurrence. Thus, a visit_detail will be tagged with `IS_EMERGENCY=True` iff the visit occurrence it belongs to is an emergency-type visit (meaning that `visit_occurrence.visit_source_value=='urgence'`) !!! aphp \"Admission through ICU\" At AP-HP, when a patient is hospitalized after coming to the ICU, its `visit_source_value` is set from `\"urgence\"` to `\"hospitalisation compl\u00e8te\"`. So you should keep in mind that this method doesn't tag those visits as ICU. Parameters ---------- visit_detail: DataFrame visit_occurrence: DataFrame Returns ------- visit_detail: DataFrame Dataframe with added columns corresponding to the following conceps: - `\"IS_EMERGENCY\"` \"\"\" vo_emergency = visit_occurrence [[ \"visit_occurrence_id\" , \"visit_source_value\" ]] vo_emergency [ \"IS_EMERGENCY\" ] = visit_occurrence . visit_source_value == \"urgence\" return visit_detail . merge ( vo_emergency [[ \"visit_occurrence_id\" , \"IS_EMERGENCY\" ]], on = \"visit_occurrence_id\" , how = \"left\" , )","title":"Emergency Units"},{"location":"functionalities/patients-course/is_emergency/#tagging-care-sites","text":"Tagging is done using the tag_emergency_care_site function: from eds_scikit.emergency import tag_emergency_care_site Tag care sites that correspond to medical emergency units . The tagging is done by adding a \"IS_EMERGENCY\" column to the provided DataFrame. Some algos can add an additional \"EMERGENCY_TYPE\" column to the provided DataFrame, providing a more detailled classification. PARAMETER DESCRIPTION care_site TYPE: DataFrame algo Possible values are: \"from_mapping\" relies on a list of care_site_source_value extracted by Judith LEBLANC, Ariel COHEN and validated by an ER doctor. The emergency care sites are here further labelled to distinguish the different types of emergency \"from_regex_on_care_site_description\" : relies on a specific list of RegEx applied on the description (= simplified care site name) of each care site. \"from_regex_on_parent_UF\" : relies on a specific list of regular expressions applied on the description (= simplified care site name) of each UF (Unit\u00e9 Fonctionnelle). The obtained tag is then propagated to every UF's children. TYPE: str DEFAULT: 'from_mapping' RETURNS DESCRIPTION care_site Dataframe with 1 to 2 added columns corresponding to the following concepts: \"IS_EMERGENCY\" \"EMERGENCY_TYPE\" (if using algo \"from_mapping\" ) TYPE: DataFrame Simply call the function by providing the necessary data (see below) and by picking the algo care_site = tag_emergency_care_site ( care_site = data . care_site , algo = \"from_mapping\" , ) Availables algorithms (values for \"algo\" ) 'from_mapping' 'from_regex_on_parent_UF' 'from_regex_on_care_site_description' This algo uses a labelled list of 201 emergency care sites. Those care sites were extracted and verified by Ariel COHEN, Judith LEBLANC, and an ER doctor validated them. Those emergency care sites are further divised into different categories, as defined in the concept 'EMERGENCY_TYPE'. The different categories are: Urgences sp\u00e9cialis\u00e9es UHCD + Post-urgences Urgences p\u00e9diatriques Urgences g\u00e9n\u00e9rales adulte Consultation urgences SAMU / SMUR See the dataset here PARAMETER DESCRIPTION care_site Should at least contains the care_site_source_value column TYPE: DataFrame version Optional version string for the mapping TYPE: Optional [ str ] DEFAULT: None RETURNS DESCRIPTION care_site Dataframe with 2 added columns corresponding to the following concepts: \"IS_EMERGENCY\" \"EMERGENCY_TYPE\" TYPE: DataFrame Source code in eds_scikit/emergency/emergency_care_site.py 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 @concept_checker ( concepts = [ \"IS_EMERGENCY\" , \"EMERGENCY_TYPE\" ]) def from_mapping ( care_site : DataFrame , version : Optional [ str ] = None , ) -> DataFrame : \"\"\"This algo uses a labelled list of 201 emergency care sites. Those care sites were extracted and verified by Ariel COHEN, Judith LEBLANC, and an ER doctor validated them. Those emergency care sites are further divised into different categories, as defined in the concept 'EMERGENCY_TYPE'. The different categories are: - Urgences sp\u00e9cialis\u00e9es - UHCD + Post-urgences - Urgences p\u00e9diatriques - Urgences g\u00e9n\u00e9rales adulte - Consultation urgences - SAMU / SMUR See the dataset [here](/datasets/care-site-emergency) Parameters ---------- care_site: DataFrame Should at least contains the `care_site_source_value` column version: Optional[str] Optional version string for the mapping Returns ------- care_site: DataFrame Dataframe with 2 added columns corresponding to the following concepts: - `\"IS_EMERGENCY\"` - `\"EMERGENCY_TYPE\"` \"\"\" function_name = \"get_care_site_emergency_mapping\" if version is not None : function_name += f \". { version } \" mapping = registry . get ( \"data\" , function_name = function_name )() # Getting the right framework fw = framework . get_framework ( care_site ) mapping = framework . to ( fw , mapping ) care_site = care_site . merge ( mapping , how = \"left\" , on = \"care_site_source_value\" , ) care_site [ \"IS_EMERGENCY\" ] = care_site [ \"EMERGENCY_TYPE\" ] . notna () return care_site Use regular expressions on parent UF (Unit\u00e9 Fonctionnelle) to classify emergency care site. This relies on this function . The regular expression used to detect emergency status is r\"\bURG|\bSAU\b|\bUHCD\b|\bZHTCD\b\" PARAMETER DESCRIPTION care_site Should at least contains the care_site_name column TYPE: DataFrame RETURNS DESCRIPTION care_site Dataframe with 1 added column corresponding to the following concept: 'IS_EMERGENCY' TYPE: DataFrame Source code in eds_scikit/emergency/emergency_care_site.py 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 @concept_checker ( concepts = [ \"IS_EMERGENCY\" ]) def from_regex_on_parent_UF ( care_site : DataFrame ) -> DataFrame : \"\"\"Use regular expressions on parent UF (Unit\u00e9 Fonctionnelle) to classify emergency care site. This relies on [this function][eds_scikit.structures.attributes.get_parent_attributes]. The regular expression used to detect emergency status is `r\"\\bURG|\\bSAU\\b|\\bUHCD\\b|\\bZHTCD\\b\"` Parameters ---------- care_site: DataFrame Should at least contains the `care_site_name` column Returns ------- care_site: DataFrame Dataframe with 1 added column corresponding to the following concept: - 'IS_EMERGENCY' \"\"\" return attributes . get_parent_attributes ( care_site , only_attributes = [ \"IS_EMERGENCY\" ], parent_type = \"Unit\u00e9 Fonctionnelle (UF)\" , ) Use regular expressions on care_site_name to decide if it an emergency care site. This relies on this function . The regular expression used to detect emergency status is r\"\bURG|\bSAU\b|\bUHCDb\b|\bZHTCD\b\" PARAMETER DESCRIPTION care_site Should at least contains the care_site_name column TYPE: DataFrame RETURNS DESCRIPTION care_site Dataframe with 1 added column corresponding to the following concept: \"IS_EMERGENCY\" TYPE: DataFrame Source code in eds_scikit/emergency/emergency_care_site.py 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 def from_regex_on_care_site_description ( care_site : DataFrame ) -> DataFrame : \"\"\"Use regular expressions on `care_site_name` to decide if it an emergency care site. This relies on [this function][eds_scikit.structures.attributes.add_care_site_attributes]. The regular expression used to detect emergency status is `r\"\\bURG|\\bSAU\\b|\\bUHCDb\\b|\\bZHTCD\\b\"` Parameters ---------- care_site: DataFrame Should at least contains the `care_site_name` column Returns ------- care_site: DataFrame Dataframe with 1 added column corresponding to the following concept: - `\"IS_EMERGENCY\"` \"\"\" return attributes . add_care_site_attributes ( care_site , only_attributes = [ \"IS_EMERGENCY\" ] )","title":"Tagging care sites"},{"location":"functionalities/patients-course/is_emergency/#tagging-visits","text":"Tagging is done using the tag_emergency_visit function: from eds_scikit.emergency import tag_emergency_visit Tag visits that correspond to medical emergency units . The tagging is done by adding a \"IS_EMERGENCY\" column to the provided DataFrame. Some algos can add an additional \"EMERGENCY_TYPE\" column to the provided DataFrame, providing a more detailled classification. It works by either tagging each visit detail's care site , or by using the visit_occurrence 's \"visit_source_value\" . PARAMETER DESCRIPTION visit_detail TYPE: DataFrame care_site Isn't necessary if the algo \"from_vo_visit_source_value\" is used TYPE: Optional [ DataFrame ] DEFAULT: None visit_occurrence Is mandatory if the algo \"from_vo_visit_source_value\" is used TYPE: Optional [ DataFrame ] DEFAULT: None algo Possible values are: \"from_mapping\" relies on a list of care_site_source_value extracted by Judith LEBLANC, Ariel COHEN and validated by an ER doctor. The emergency care sites are here further labelled to distinguish the different types of emergency \"from_regex_on_care_site_description\" : relies on a specific list of RegEx applied on the description (= simplified care site name) of each care site. \"from_regex_on_parent_UF\" : relies on a specific list of regular expressions applied on the description (= simplified care site name) of each UF (Unit\u00e9 Fonctionnelle). The obtained tag is then propagated to every UF's children. \"from_vo_visit_source_value\" : relies on the parent visit occurrence of each visit detail: A visit detail will be tagged as emergency if it belongs to a visit occurrence where visit_occurrence.visit_source_value=='urgence' . TYPE: str DEFAULT: 'from_mapping' RETURNS DESCRIPTION care_site Dataframe with 1 to 2 added columns corresponding to the following concepts: \"IS_EMERGENCY\" \"EMERGENCY_TYPE\" (if using algo \"from_mapping\" ) TYPE: DataFrame Simply call the function by providing the necessary data (see below) and by picking the algo visit_detail = tag_emergency_visit ( visit_detail = data . visit_detail , algo = \"from_mapping\" , ) Availables algorithms (values for \"algo\" ) 'from_mapping' 'from_regex_on_parent_UF' 'from_regex_on_care_site_description' 'from_vo_visit_source_value' This algo uses a labelled list of 201 emergency care sites. Those care sites were extracted and verified by Ariel COHEN, Judith LEBLANC, and an ER doctor validated them. Those emergency care sites are further divised into different categories, as defined in the concept 'EMERGENCY_TYPE'. The different categories are: Urgences sp\u00e9cialis\u00e9es UHCD + Post-urgences Urgences p\u00e9diatriques Urgences g\u00e9n\u00e9rales adulte Consultation urgences SAMU / SMUR See the dataset here PARAMETER DESCRIPTION care_site Should at least contains the care_site_source_value column TYPE: DataFrame version Optional version string for the mapping TYPE: Optional [ str ] DEFAULT: None RETURNS DESCRIPTION care_site Dataframe with 2 added columns corresponding to the following concepts: \"IS_EMERGENCY\" \"EMERGENCY_TYPE\" TYPE: DataFrame Source code in eds_scikit/emergency/emergency_care_site.py 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 @concept_checker ( concepts = [ \"IS_EMERGENCY\" , \"EMERGENCY_TYPE\" ]) def from_mapping ( care_site : DataFrame , version : Optional [ str ] = None , ) -> DataFrame : \"\"\"This algo uses a labelled list of 201 emergency care sites. Those care sites were extracted and verified by Ariel COHEN, Judith LEBLANC, and an ER doctor validated them. Those emergency care sites are further divised into different categories, as defined in the concept 'EMERGENCY_TYPE'. The different categories are: - Urgences sp\u00e9cialis\u00e9es - UHCD + Post-urgences - Urgences p\u00e9diatriques - Urgences g\u00e9n\u00e9rales adulte - Consultation urgences - SAMU / SMUR See the dataset [here](/datasets/care-site-emergency) Parameters ---------- care_site: DataFrame Should at least contains the `care_site_source_value` column version: Optional[str] Optional version string for the mapping Returns ------- care_site: DataFrame Dataframe with 2 added columns corresponding to the following concepts: - `\"IS_EMERGENCY\"` - `\"EMERGENCY_TYPE\"` \"\"\" function_name = \"get_care_site_emergency_mapping\" if version is not None : function_name += f \". { version } \" mapping = registry . get ( \"data\" , function_name = function_name )() # Getting the right framework fw = framework . get_framework ( care_site ) mapping = framework . to ( fw , mapping ) care_site = care_site . merge ( mapping , how = \"left\" , on = \"care_site_source_value\" , ) care_site [ \"IS_EMERGENCY\" ] = care_site [ \"EMERGENCY_TYPE\" ] . notna () return care_site Use regular expressions on parent UF (Unit\u00e9 Fonctionnelle) to classify emergency care site. This relies on this function . The regular expression used to detect emergency status is r\"\bURG|\bSAU\b|\bUHCD\b|\bZHTCD\b\" PARAMETER DESCRIPTION care_site Should at least contains the care_site_name column TYPE: DataFrame RETURNS DESCRIPTION care_site Dataframe with 1 added column corresponding to the following concept: 'IS_EMERGENCY' TYPE: DataFrame Source code in eds_scikit/emergency/emergency_care_site.py 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 @concept_checker ( concepts = [ \"IS_EMERGENCY\" ]) def from_regex_on_parent_UF ( care_site : DataFrame ) -> DataFrame : \"\"\"Use regular expressions on parent UF (Unit\u00e9 Fonctionnelle) to classify emergency care site. This relies on [this function][eds_scikit.structures.attributes.get_parent_attributes]. The regular expression used to detect emergency status is `r\"\\bURG|\\bSAU\\b|\\bUHCD\\b|\\bZHTCD\\b\"` Parameters ---------- care_site: DataFrame Should at least contains the `care_site_name` column Returns ------- care_site: DataFrame Dataframe with 1 added column corresponding to the following concept: - 'IS_EMERGENCY' \"\"\" return attributes . get_parent_attributes ( care_site , only_attributes = [ \"IS_EMERGENCY\" ], parent_type = \"Unit\u00e9 Fonctionnelle (UF)\" , ) Use regular expressions on care_site_name to decide if it an emergency care site. This relies on this function . The regular expression used to detect emergency status is r\"\bURG|\bSAU\b|\bUHCDb\b|\bZHTCD\b\" PARAMETER DESCRIPTION care_site Should at least contains the care_site_name column TYPE: DataFrame RETURNS DESCRIPTION care_site Dataframe with 1 added column corresponding to the following concept: \"IS_EMERGENCY\" TYPE: DataFrame Source code in eds_scikit/emergency/emergency_care_site.py 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 def from_regex_on_care_site_description ( care_site : DataFrame ) -> DataFrame : \"\"\"Use regular expressions on `care_site_name` to decide if it an emergency care site. This relies on [this function][eds_scikit.structures.attributes.add_care_site_attributes]. The regular expression used to detect emergency status is `r\"\\bURG|\\bSAU\\b|\\bUHCDb\\b|\\bZHTCD\\b\"` Parameters ---------- care_site: DataFrame Should at least contains the `care_site_name` column Returns ------- care_site: DataFrame Dataframe with 1 added column corresponding to the following concept: - `\"IS_EMERGENCY\"` \"\"\" return attributes . add_care_site_attributes ( care_site , only_attributes = [ \"IS_EMERGENCY\" ] ) This algo uses the \"Type de dossier\" of each visit detail's parent visit occurrence. Thus, a visit_detail will be tagged with IS_EMERGENCY=True iff the visit occurrence it belongs to is an emergency-type visit (meaning that visit_occurrence.visit_source_value=='urgence' ) Admission through ICU At AP-HP, when a patient is hospitalized after coming to the ICU, its visit_source_value is set from \"urgence\" to \"hospitalisation compl\u00e8te\" . So you should keep in mind that this method doesn't tag those visits as ICU. PARAMETER DESCRIPTION visit_detail TYPE: DataFrame visit_occurrence TYPE: DataFrame RETURNS DESCRIPTION visit_detail Dataframe with added columns corresponding to the following conceps: \"IS_EMERGENCY\" TYPE: DataFrame Source code in eds_scikit/emergency/emergency_visit.py 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 @concept_checker ( concepts = [ \"IS_EMERGENCY\" ]) def from_vo_visit_source_value ( visit_detail : DataFrame , visit_occurrence : DataFrame , ) -> DataFrame : \"\"\" This algo uses the *\"Type de dossier\"* of each visit detail's parent visit occurrence. Thus, a visit_detail will be tagged with `IS_EMERGENCY=True` iff the visit occurrence it belongs to is an emergency-type visit (meaning that `visit_occurrence.visit_source_value=='urgence'`) !!! aphp \"Admission through ICU\" At AP-HP, when a patient is hospitalized after coming to the ICU, its `visit_source_value` is set from `\"urgence\"` to `\"hospitalisation compl\u00e8te\"`. So you should keep in mind that this method doesn't tag those visits as ICU. Parameters ---------- visit_detail: DataFrame visit_occurrence: DataFrame Returns ------- visit_detail: DataFrame Dataframe with added columns corresponding to the following conceps: - `\"IS_EMERGENCY\"` \"\"\" vo_emergency = visit_occurrence [[ \"visit_occurrence_id\" , \"visit_source_value\" ]] vo_emergency [ \"IS_EMERGENCY\" ] = visit_occurrence . visit_source_value == \"urgence\" return visit_detail . merge ( vo_emergency [[ \"visit_occurrence_id\" , \"IS_EMERGENCY\" ]], on = \"visit_occurrence_id\" , how = \"left\" , )","title":"Tagging visits"},{"location":"functionalities/patients-course/is_icu/","text":"eds-scikit provides a function to tag care sites as being Intensive Care Units . It also provides a higher-level function to directly tag visits. from eds_scikit.io import HiveData data = HiveData ( DBNAME ) Tagging care sites Tagging is done using the tag_icu_care_site function: from eds_scikit.icu import tag_icu_care_site Tag care sites that correspond to ICU units . The tagging is done by adding a \"IS_ICU\" column to the provided DataFrame. PARAMETER DESCRIPTION care_site TYPE: DataFrame algo Possible values are: \"from_authorisation_type\" \"from_regex_on_care_site_description\" TYPE: str DEFAULT: 'from_mapping' RETURNS DESCRIPTION care_site Dataframe with 1 added column corresponding to the following concept: \"IS_ICU\" TYPE: DataFrame Simply call the function by providing the necessary data (see below) and by picking the algo care_site = tag_icu_care_site ( care_site = data . care_site , algo = \"from_authorisation_type\" , ) Availables algorithms (values for \"algo\" ) 'from_authorisation_type' 'from_regex_on_care_site_description' This algo uses the care_site.place_of_service_source_value columns to retrieve Intensive Care Units. The following values are used to tag a care site as ICU: \"REA PED\" \"REA\" \"REA ADULTE\" \"REA NEONAT\" \"USI\" \"USI ADULTE\" \"USI NEONAT\" \"SC PED\" \"SC\" \"SC ADULTE\" PARAMETER DESCRIPTION care_site Should at least contains the place_of_service_source_value column TYPE: DataFrame RETURNS DESCRIPTION care_site Dataframe with 1 added column corresponding to the following concepts: \"IS_ICU\" TYPE: DataFrame Source code in eds_scikit/icu/icu_care_site.py 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 @concept_checker ( concepts = [ \"IS_ICU\" ]) def from_authorisation_type ( care_site : DataFrame ) -> DataFrame : \"\"\"This algo uses the `care_site.place_of_service_source_value` columns to retrieve Intensive Care Units. The following values are used to tag a care site as ICU: - `\"REA PED\"` - `\"REA\"` - `\"REA ADULTE\"` - `\"REA NEONAT\"` - `\"USI\"` - `\"USI ADULTE\"` - `\"USI NEONAT\"` - `\"SC PED\"` - `\"SC\"` - `\"SC ADULTE\"` Parameters ---------- care_site: DataFrame Should at least contains the `place_of_service_source_value` column Returns ------- care_site: DataFrame Dataframe with 1 added column corresponding to the following concepts: - `\"IS_ICU\"` \"\"\" icu_units = set ( [ \"REA PED\" , \"USI\" , \"SC PED\" , \"SC\" , \"REA\" , \"SC ADULTE\" , \"USI ADULTE\" , \"REA ADULTE\" , \"USI NEONAT\" , \"REA NEONAT\" , ] ) care_site [ \"IS_ICU\" ] = care_site [ \"place_of_service_source_value\" ] . isin ( icu_units ) return care_site Use regular expressions on care_site_name to decide if it an ICU care site. This relies on this function . The regular expression used to detect ICU is r\"\bUSI|\bREA[N\\s]|\bREA\b|\bUSC\b|SOINS.*INTENSIF|SURV.{0,15}CONT|\bSI\b|\bSC\b\" . Keeping only 'UDS' At AP-HP, all ICU are UDS ( Unit\u00e9 De Soins ). Therefore, this function filters care sites by default to only keep UDS. PARAMETER DESCRIPTION care_site Should at least contains the care_site_name and care_site_type_source_value columns TYPE: DataFrame subset_care_site_type_source_value Acceptable values for care_site_type_source_value TYPE: Union [ list , set ] DEFAULT: {'UDS'} RETURNS DESCRIPTION care_site Dataframe with 1 added column corresponding to the following concept: \"IS_ICU\" TYPE: DataFrame Source code in eds_scikit/icu/icu_care_site.py 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 def from_regex_on_care_site_description ( care_site : DataFrame , subset_care_site_type_source_value : Union [ list , set ] = { \"UDS\" } ) -> DataFrame : \"\"\"Use regular expressions on `care_site_name` to decide if it an ICU care site. This relies on [this function][eds_scikit.structures.attributes.add_care_site_attributes]. The regular expression used to detect ICU is `r\"\\bUSI|\\bREA[N\\s]|\\bREA\\b|\\bUSC\\b|SOINS.*INTENSIF|SURV.{0,15}CONT|\\bSI\\b|\\bSC\\b\"`. !!! aphp \"Keeping only 'UDS'\" At AP-HP, all ICU are **UDS** (*Unit\u00e9 De Soins*). Therefore, this function filters care sites by default to only keep UDS. Parameters ---------- care_site: DataFrame Should at least contains the `care_site_name` and `care_site_type_source_value` columns subset_care_site_type_source_value: Union[list, set] Acceptable values for `care_site_type_source_value` Returns ------- care_site: DataFrame Dataframe with 1 added column corresponding to the following concept: - `\"IS_ICU\"` \"\"\" # noqa care_site = attributes . add_care_site_attributes ( care_site , only_attributes = [ \"IS_ICU\" ] ) # Filtering matches if subset_care_site_type_source_value : care_site [ \"IS_ICU\" ] = care_site [ \"IS_ICU\" ] & ( care_site . care_site_type_source_value . isin ( subset_care_site_type_source_value ) ) return care_site Tagging visits Tagging is done using the tag_icu_visit function: from eds_scikit.icu import tag_icu_visit Tag care_sites that correspond to ICU units . The tagging is done by adding a \"IS_ICU\" column to the provided DataFrame. It works by tagging each visit detail's care site . PARAMETER DESCRIPTION visit_detail TYPE: DataFrame care_site TYPE: DataFrame algo Possible values are: \"from_authorisation_type\" \"from_regex_on_care_site_description\" TYPE: str DEFAULT: 'from_authorisation_type' RETURNS DESCRIPTION visit_detail Dataframe with 1 added column corresponding to the following concept: \"IS_ICU\" TYPE: DataFrame Simply call the function by providing the necessary data (see below) and by picking the algo visit_detail = tag_icu_visit ( visit_detail = data . visit_detail , algo = \"from_mapping\" , ) Availables algorithms (values for \"algo\" ) Those are the same as tag_icu_care_site","title":"ICU"},{"location":"functionalities/patients-course/is_icu/#tagging-care-sites","text":"Tagging is done using the tag_icu_care_site function: from eds_scikit.icu import tag_icu_care_site Tag care sites that correspond to ICU units . The tagging is done by adding a \"IS_ICU\" column to the provided DataFrame. PARAMETER DESCRIPTION care_site TYPE: DataFrame algo Possible values are: \"from_authorisation_type\" \"from_regex_on_care_site_description\" TYPE: str DEFAULT: 'from_mapping' RETURNS DESCRIPTION care_site Dataframe with 1 added column corresponding to the following concept: \"IS_ICU\" TYPE: DataFrame Simply call the function by providing the necessary data (see below) and by picking the algo care_site = tag_icu_care_site ( care_site = data . care_site , algo = \"from_authorisation_type\" , ) Availables algorithms (values for \"algo\" ) 'from_authorisation_type' 'from_regex_on_care_site_description' This algo uses the care_site.place_of_service_source_value columns to retrieve Intensive Care Units. The following values are used to tag a care site as ICU: \"REA PED\" \"REA\" \"REA ADULTE\" \"REA NEONAT\" \"USI\" \"USI ADULTE\" \"USI NEONAT\" \"SC PED\" \"SC\" \"SC ADULTE\" PARAMETER DESCRIPTION care_site Should at least contains the place_of_service_source_value column TYPE: DataFrame RETURNS DESCRIPTION care_site Dataframe with 1 added column corresponding to the following concepts: \"IS_ICU\" TYPE: DataFrame Source code in eds_scikit/icu/icu_care_site.py 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 @concept_checker ( concepts = [ \"IS_ICU\" ]) def from_authorisation_type ( care_site : DataFrame ) -> DataFrame : \"\"\"This algo uses the `care_site.place_of_service_source_value` columns to retrieve Intensive Care Units. The following values are used to tag a care site as ICU: - `\"REA PED\"` - `\"REA\"` - `\"REA ADULTE\"` - `\"REA NEONAT\"` - `\"USI\"` - `\"USI ADULTE\"` - `\"USI NEONAT\"` - `\"SC PED\"` - `\"SC\"` - `\"SC ADULTE\"` Parameters ---------- care_site: DataFrame Should at least contains the `place_of_service_source_value` column Returns ------- care_site: DataFrame Dataframe with 1 added column corresponding to the following concepts: - `\"IS_ICU\"` \"\"\" icu_units = set ( [ \"REA PED\" , \"USI\" , \"SC PED\" , \"SC\" , \"REA\" , \"SC ADULTE\" , \"USI ADULTE\" , \"REA ADULTE\" , \"USI NEONAT\" , \"REA NEONAT\" , ] ) care_site [ \"IS_ICU\" ] = care_site [ \"place_of_service_source_value\" ] . isin ( icu_units ) return care_site Use regular expressions on care_site_name to decide if it an ICU care site. This relies on this function . The regular expression used to detect ICU is r\"\bUSI|\bREA[N\\s]|\bREA\b|\bUSC\b|SOINS.*INTENSIF|SURV.{0,15}CONT|\bSI\b|\bSC\b\" . Keeping only 'UDS' At AP-HP, all ICU are UDS ( Unit\u00e9 De Soins ). Therefore, this function filters care sites by default to only keep UDS. PARAMETER DESCRIPTION care_site Should at least contains the care_site_name and care_site_type_source_value columns TYPE: DataFrame subset_care_site_type_source_value Acceptable values for care_site_type_source_value TYPE: Union [ list , set ] DEFAULT: {'UDS'} RETURNS DESCRIPTION care_site Dataframe with 1 added column corresponding to the following concept: \"IS_ICU\" TYPE: DataFrame Source code in eds_scikit/icu/icu_care_site.py 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 def from_regex_on_care_site_description ( care_site : DataFrame , subset_care_site_type_source_value : Union [ list , set ] = { \"UDS\" } ) -> DataFrame : \"\"\"Use regular expressions on `care_site_name` to decide if it an ICU care site. This relies on [this function][eds_scikit.structures.attributes.add_care_site_attributes]. The regular expression used to detect ICU is `r\"\\bUSI|\\bREA[N\\s]|\\bREA\\b|\\bUSC\\b|SOINS.*INTENSIF|SURV.{0,15}CONT|\\bSI\\b|\\bSC\\b\"`. !!! aphp \"Keeping only 'UDS'\" At AP-HP, all ICU are **UDS** (*Unit\u00e9 De Soins*). Therefore, this function filters care sites by default to only keep UDS. Parameters ---------- care_site: DataFrame Should at least contains the `care_site_name` and `care_site_type_source_value` columns subset_care_site_type_source_value: Union[list, set] Acceptable values for `care_site_type_source_value` Returns ------- care_site: DataFrame Dataframe with 1 added column corresponding to the following concept: - `\"IS_ICU\"` \"\"\" # noqa care_site = attributes . add_care_site_attributes ( care_site , only_attributes = [ \"IS_ICU\" ] ) # Filtering matches if subset_care_site_type_source_value : care_site [ \"IS_ICU\" ] = care_site [ \"IS_ICU\" ] & ( care_site . care_site_type_source_value . isin ( subset_care_site_type_source_value ) ) return care_site","title":"Tagging care sites"},{"location":"functionalities/patients-course/is_icu/#tagging-visits","text":"Tagging is done using the tag_icu_visit function: from eds_scikit.icu import tag_icu_visit Tag care_sites that correspond to ICU units . The tagging is done by adding a \"IS_ICU\" column to the provided DataFrame. It works by tagging each visit detail's care site . PARAMETER DESCRIPTION visit_detail TYPE: DataFrame care_site TYPE: DataFrame algo Possible values are: \"from_authorisation_type\" \"from_regex_on_care_site_description\" TYPE: str DEFAULT: 'from_authorisation_type' RETURNS DESCRIPTION visit_detail Dataframe with 1 added column corresponding to the following concept: \"IS_ICU\" TYPE: DataFrame Simply call the function by providing the necessary data (see below) and by picking the algo visit_detail = tag_icu_visit ( visit_detail = data . visit_detail , algo = \"from_mapping\" , ) Availables algorithms (values for \"algo\" ) Those are the same as tag_icu_care_site","title":"Tagging visits"},{"location":"functionalities/patients-course/visit_merging/","text":"(function() { function addWidgetsRenderer() { var requireJsScript = document.createElement('script'); requireJsScript.src = 'https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js'; var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var jupyterWidgetsScript = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} jupyterWidgetsScript.src = widgetRendererSrc; document.body.appendChild(requireJsScript); document.body.appendChild(jupyterWidgetsScript); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); You can download this notebook directly here Merging visits into stays In order to have a precise view of each patient's course of care, it can be useful to merge together visit occurrences into stays. A crude way of doing so is by using the preceding_visit_occurrence_id column in the visit_occurrence table. However, this columns isn't always filled, and a lot of visits would be missed by using only this method. The method proposed here relies on how close two visits are in order to put them in the same stay. This is the role of the merge_visits() functions: import eds_scikit spark , sc , sql = eds_scikit . improve_performances () from eds_scikit.period.stays import merge_visits Another function, get_stays_duration() , can then be used to extract a stay DataFrame with useful informations: from eds_scikit.period.stays import get_stays_duration % config Completer . use_jedi = False % load_ext autoreload % autoreload 2 Loading data import databricks.koalas as ks import pandas as pd import altair as alt from datetime import datetime , timedelta from eds_scikit.io import HiveData from eds_scikit.utils.datetime_helpers import substract_datetime data = HiveData ( spark , database_name = 'eds_lib_poc' ) vo = data . visit_occurrence This cohort is of reasonnable size, so we can work with Pandas in this case: vo_pd = vo . to_pandas () Getting stays We can now merge visits into stays. Check the corresponding documentation for more informations about each individual parameter vo_pd = merge_visits ( vo_pd , remove_deleted_visits = True , long_stay_threshold = timedelta ( days = 365 ), long_stay_filtering = 'all' , max_timedelta = timedelta ( days = 2 ), merge_different_hospitals = False , merge_different_source_values = [ 'hospitalis\u00e9s' , 'urgence' ], ) This functions will add a 'STAY_ID' column, corresponding to the visit_occurrence_id of the first visit of the stay. We can check that indeed, most stays are composed of a single visit (notice that the Y-axis is in log scale): stats = vo_pd . groupby ( 'STAY_ID' ) . visit_occurrence_id . count () _ = stats . hist ( bins = 20 , log = True ) We can finally display the number of merged visits: stats [ stats > 1 ] . sum () 42738 or in % of the total number of visits: round ( 100 * stats [ stats > 1 ] . sum () / len ( vo_pd ), 2 ) 8.69 Getting stays durations This second function generates an easy-to-use stay DataFrame : We will only focus on emergency and hospitalisation for this part, which is no problem since we only allowed merging those two types of stay (via merge_different_source_values=['hospitalis\u00e9s', 'urgence'] ) vo_pd = vo_pd [ vo_pd . visit_source_value . isin ([ 'hospitalis\u00e9s' , 'urgence' ])] stays = get_stays_duration ( vo_pd , algo = 'visits_date_difference' , missing_end_date_handling = 'coerce' ) stays . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } person_id t_start t_end STAY_DURATION STAY_ID -2.147468e+09 -793380275 2017-08-22 18:45:00 2017-08-25 14:24:00 67.650000 -2.147465e+09 -1632607976 2019-12-03 12:20:00 2019-12-03 13:42:00 1.366667 -2.147450e+09 56307107 2019-04-23 17:18:00 2019-04-25 16:21:00 47.050000 -2.147433e+09 -916176344 2021-04-23 10:40:00 2021-04-23 11:50:00 1.166667 -2.147431e+09 1814301591 2018-06-06 14:25:00 2018-06-06 15:24:00 0.983333 Let us compare the distribution of stay/visit durations. # Extracting visit duration (in hours) vo_pd [ 'VISIT_DURATION' ] = substract_datetime ( vo_pd [ 'visit_end_datetime' ], vo_pd [ 'visit_start_datetime' ], out = 'hours' ) # COnverting to days stays [ 'STAY_DURATION' ] = stays [ 'STAY_DURATION' ] / 24 vo_pd [ 'VISIT_DURATION' ] = vo_pd [ 'VISIT_DURATION' ] / 24 # Keeping only visits/stays less than a month long vo_pd = vo_pd [ vo_pd [ 'VISIT_DURATION' ] <= 31 ] stays = stays [ stays [ 'STAY_DURATION' ] <= 31 ] stays . STAY_DURATION . mean () 2.306892174348691 vo_pd . VISIT_DURATION . mean () 2.2188916112474866 We will aggregate the data into bins of 1 week days = list ( range ( 1 , 32 )) stays_distribution = pd . cut ( stays [ 'STAY_DURATION' ], 31 , labels = days ) . value_counts ( normalize = True ) . sort_index () visits_distribution = pd . cut ( vo_pd [ 'VISIT_DURATION' ], 31 , labels = days ) . value_counts ( normalize = True ) . sort_index () data = pd . concat ([ pd . DataFrame ( data = { 'density' : stays_distribution . values , 'day' : stays_distribution . index , 'type' : 'STAY' }), pd . DataFrame ( data = { 'density' : visits_distribution . values , 'day' : visits_distribution . index , 'type' : 'VISIT' }) ]) diff_distribution = ( stays_distribution - visits_distribution ) . to_frame () . reset_index () diff_distribution . columns = [ 'day' , 'difference' ] alt . Chart ( data ) . mark_bar ( opacity = 0.5 ) . encode ( x = \"day:N\" , y = alt . Y ( \"density:Q\" , stack = None , scale = alt . Scale ( type = 'log' )), color = alt . Color ( \"type:N\" , title = 'Type' ) ) . properties ( width = 800 ) (function(spec, embedOpt){ let outputDiv = document.currentScript.previousElementSibling; if (outputDiv.id !== \"altair-viz-06979841a60142e78d60594f1d1b0722\") { outputDiv = document.getElementById(\"altair-viz-06979841a60142e78d60594f1d1b0722\"); } const paths = { \"vega\": \"https://cdn.jsdelivr.net/npm//vega@5?noext\", \"vega-lib\": \"https://cdn.jsdelivr.net/npm//vega-lib?noext\", \"vega-lite\": \"https://cdn.jsdelivr.net/npm//vega-lite@4.8.1?noext\", \"vega-embed\": \"https://cdn.jsdelivr.net/npm//vega-embed@6?noext\", }; function loadScript(lib) { return new Promise(function(resolve, reject) { var s = document.createElement('script'); s.src = paths[lib]; s.async = true; s.onload = () => resolve(paths[lib]); s.onerror = () => reject(`Error loading script: ${paths[lib]}`); document.getElementsByTagName(\"head\")[0].appendChild(s); }); } function showError(err) { outputDiv.innerHTML = `<div class=\"error\" style=\"color:red;\">${err}</div>`; throw err; } function displayChart(vegaEmbed) { vegaEmbed(outputDiv, spec, embedOpt) .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`)); } if(typeof define === \"function\" && define.amd) { requirejs.config({paths}); require([\"vega-embed\"], displayChart, err => showError(`Error loading script: ${err.message}`)); } else if (typeof vegaEmbed === \"function\") { displayChart(vegaEmbed); } else { loadScript(\"vega\") .then(() => loadScript(\"vega-lite\")) .then(() => loadScript(\"vega-embed\")) .catch(showError) .then(() => displayChart(vegaEmbed)); } })({\"config\": {\"view\": {\"continuousWidth\": 400, \"continuousHeight\": 300}}, \"data\": {\"name\": \"data-b0c53aa5acb801ec3d3cb4990609769b\"}, \"mark\": {\"type\": \"bar\", \"opacity\": 0.5}, \"encoding\": {\"color\": {\"type\": \"nominal\", \"field\": \"type\", \"title\": \"Type\"}, \"x\": {\"type\": \"nominal\", \"field\": \"day\"}, \"y\": {\"type\": \"quantitative\", \"field\": \"density\", \"scale\": {\"type\": \"log\"}, \"stack\": null}}, \"width\": 800, \"$schema\": \"https://vega.github.io/schema/vega-lite/v4.8.1.json\", \"datasets\": {\"data-b0c53aa5acb801ec3d3cb4990609769b\": [{\"density\": 0.6257372293084775, \"day\": 1, \"type\": \"STAY\"}, {\"density\": 0.08317573975454683, \"day\": 2, \"type\": \"STAY\"}, {\"density\": 0.060734788244734876, \"day\": 3, \"type\": \"STAY\"}, {\"density\": 0.05161026032401614, \"day\": 4, \"type\": \"STAY\"}, {\"density\": 0.04074612375912593, \"day\": 5, \"type\": \"STAY\"}, {\"density\": 0.024679996184084264, \"day\": 6, \"type\": \"STAY\"}, {\"density\": 0.01920303477533796, \"day\": 7, \"type\": \"STAY\"}, {\"density\": 0.016015622807952816, \"day\": 8, \"type\": \"STAY\"}, {\"density\": 0.010914641331979058, \"day\": 9, \"type\": \"STAY\"}, {\"density\": 0.008630703531405548, \"day\": 10, \"type\": \"STAY\"}, {\"density\": 0.007272686460794271, \"day\": 11, \"type\": \"STAY\"}, {\"density\": 0.005869776263881797, \"day\": 12, \"type\": \"STAY\"}, {\"density\": 0.005145874602274959, \"day\": 13, \"type\": \"STAY\"}, {\"density\": 0.005173932806213209, \"day\": 14, \"type\": \"STAY\"}, {\"density\": 0.004595933805085269, \"day\": 15, \"type\": \"STAY\"}, {\"density\": 0.003849585580327832, \"day\": 16, \"type\": \"STAY\"}, {\"density\": 0.0031593537634468944, \"day\": 17, \"type\": \"STAY\"}, {\"density\": 0.002676752655709003, \"day\": 18, \"type\": \"STAY\"}, {\"density\": 0.0025364616360177552, \"day\": 19, \"type\": \"STAY\"}, {\"density\": 0.0022895494413611595, \"day\": 20, \"type\": \"STAY\"}, {\"density\": 0.002328830926874709, \"day\": 21, \"type\": \"STAY\"}, {\"density\": 0.002227821392697011, \"day\": 22, \"type\": \"STAY\"}, {\"density\": 0.0016554340323567207, \"day\": 23, \"type\": \"STAY\"}, {\"density\": 0.0014926964495148736, \"day\": 24, \"type\": \"STAY\"}, {\"density\": 0.0011952794877694288, \"day\": 25, \"type\": \"STAY\"}, {\"density\": 0.0012289493324953284, \"day\": 26, \"type\": \"STAY\"}, {\"density\": 0.0011672212838311794, \"day\": 27, \"type\": \"STAY\"}, {\"density\": 0.0014926964495148736, \"day\": 28, \"type\": \"STAY\"}, {\"density\": 0.0012233376917076785, \"day\": 29, \"type\": \"STAY\"}, {\"density\": 0.001015706982564632, \"day\": 30, \"type\": \"STAY\"}, {\"density\": 0.0009539789339004832, \"day\": 31, \"type\": \"STAY\"}, {\"density\": 0.65274397841082, \"day\": 1, \"type\": \"VISIT\"}, {\"density\": 0.07284227492272537, \"day\": 2, \"type\": \"VISIT\"}, {\"density\": 0.05349578236918982, \"day\": 3, \"type\": \"VISIT\"}, {\"density\": 0.04853854188205479, \"day\": 4, \"type\": \"VISIT\"}, {\"density\": 0.038677079522620395, \"day\": 5, \"type\": \"VISIT\"}, {\"density\": 0.02326456818989147, \"day\": 6, \"type\": \"VISIT\"}, {\"density\": 0.018222497919019368, \"day\": 7, \"type\": \"VISIT\"}, {\"density\": 0.015560963454268793, \"day\": 8, \"type\": \"VISIT\"}, {\"density\": 0.010550704352298091, \"day\": 9, \"type\": \"VISIT\"}, {\"density\": 0.008281507637331468, \"day\": 10, \"type\": \"VISIT\"}, {\"density\": 0.006993155296824715, \"day\": 11, \"type\": \"VISIT\"}, {\"density\": 0.005810840185989301, \"day\": 12, \"type\": \"VISIT\"}, {\"density\": 0.005100390747191339, \"day\": 13, \"type\": \"VISIT\"}, {\"density\": 0.005206427976862676, \"day\": 14, \"type\": \"VISIT\"}, {\"density\": 0.0046550343825717206, \"day\": 15, \"type\": \"VISIT\"}, {\"density\": 0.0038226421296517206, \"day\": 16, \"type\": \"VISIT\"}, {\"density\": 0.0030750796604687904, \"day\": 17, \"type\": \"VISIT\"}, {\"density\": 0.0026297232958491725, \"day\": 18, \"type\": \"VISIT\"}, {\"density\": 0.0024812711743093, \"day\": 19, \"type\": \"VISIT\"}, {\"density\": 0.0022373855460652236, \"day\": 20, \"type\": \"VISIT\"}, {\"density\": 0.0023169134683187266, \"day\": 21, \"type\": \"VISIT\"}, {\"density\": 0.0021419520393610196, \"day\": 22, \"type\": \"VISIT\"}, {\"density\": 0.0015852565835864972, \"day\": 23, \"type\": \"VISIT\"}, {\"density\": 0.0014951249383658603, \"day\": 24, \"type\": \"VISIT\"}, {\"density\": 0.001245937448638217, \"day\": 25, \"type\": \"VISIT\"}, {\"density\": 0.0011345983574833124, \"day\": 26, \"type\": \"VISIT\"}, {\"density\": 0.0012194281412203824, \"day\": 27, \"type\": \"VISIT\"}, {\"density\": 0.0015481435532015292, \"day\": 28, \"type\": \"VISIT\"}, {\"density\": 0.001245937448638217, \"day\": 29, \"type\": \"VISIT\"}, {\"density\": 0.0009808443744598729, \"day\": 30, \"type\": \"VISIT\"}, {\"density\": 0.0008960145907228028, \"day\": 31, \"type\": \"VISIT\"}]}}, {\"mode\": \"vega-lite\"}); Plotting the difference between those two distributions shows that - Some short visits ( < 1 week ) seems to be merged into longer stays - This cause stays of duration ~ N weeks to be more frequent than visits of duration ~ N weeks alt . Chart ( diff_distribution ) . mark_bar () . encode ( x = alt . X ( \"day:N\" , title = \"Day\" ), y = alt . Y ( \"difference:Q\" , title = 'Difference between stay and visit distribution' ) ) . properties ( width = 800 ) (function(spec, embedOpt){ let outputDiv = document.currentScript.previousElementSibling; if (outputDiv.id !== \"altair-viz-bfc21174f79b41678f31cbc2a2bdb70d\") { outputDiv = document.getElementById(\"altair-viz-bfc21174f79b41678f31cbc2a2bdb70d\"); } const paths = { \"vega\": \"https://cdn.jsdelivr.net/npm//vega@5?noext\", \"vega-lib\": \"https://cdn.jsdelivr.net/npm//vega-lib?noext\", \"vega-lite\": \"https://cdn.jsdelivr.net/npm//vega-lite@4.8.1?noext\", \"vega-embed\": \"https://cdn.jsdelivr.net/npm//vega-embed@6?noext\", }; function loadScript(lib) { return new Promise(function(resolve, reject) { var s = document.createElement('script'); s.src = paths[lib]; s.async = true; s.onload = () => resolve(paths[lib]); s.onerror = () => reject(`Error loading script: ${paths[lib]}`); document.getElementsByTagName(\"head\")[0].appendChild(s); }); } function showError(err) { outputDiv.innerHTML = `<div class=\"error\" style=\"color:red;\">${err}</div>`; throw err; } function displayChart(vegaEmbed) { vegaEmbed(outputDiv, spec, embedOpt) .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`)); } if(typeof define === \"function\" && define.amd) { requirejs.config({paths}); require([\"vega-embed\"], displayChart, err => showError(`Error loading script: ${err.message}`)); } else if (typeof vegaEmbed === \"function\") { displayChart(vegaEmbed); } else { loadScript(\"vega\") .then(() => loadScript(\"vega-lite\")) .then(() => loadScript(\"vega-embed\")) .catch(showError) .then(() => displayChart(vegaEmbed)); } })({\"config\": {\"view\": {\"continuousWidth\": 400, \"continuousHeight\": 300}}, \"data\": {\"name\": \"data-c73fdd6af4ddb3276ed1e9ff4cf51303\"}, \"mark\": \"bar\", \"encoding\": {\"x\": {\"type\": \"nominal\", \"field\": \"day\", \"title\": \"Day\"}, \"y\": {\"type\": \"quantitative\", \"field\": \"difference\", \"title\": \"Difference between stay and visit distribution\"}}, \"width\": 800, \"$schema\": \"https://vega.github.io/schema/vega-lite/v4.8.1.json\", \"datasets\": {\"data-c73fdd6af4ddb3276ed1e9ff4cf51303\": [{\"day\": 1, \"difference\": -0.027006749102342464}, {\"day\": 2, \"difference\": 0.01033346483182146}, {\"day\": 3, \"difference\": 0.007239005875545053}, {\"day\": 4, \"difference\": 0.0030717184419613505}, {\"day\": 5, \"difference\": 0.0020690442365055364}, {\"day\": 6, \"difference\": 0.0014154279941927944}, {\"day\": 7, \"difference\": 0.000980536856318593}, {\"day\": 8, \"difference\": 0.00045465935368402266}, {\"day\": 9, \"difference\": 0.000363936979680967}, {\"day\": 10, \"difference\": 0.00034919589407408046}, {\"day\": 11, \"difference\": 0.0002795311639695562}, {\"day\": 12, \"difference\": 5.893607789249616e-05}, {\"day\": 13, \"difference\": 4.548385508362004e-05}, {\"day\": 14, \"difference\": -3.249517064946722e-05}, {\"day\": 15, \"difference\": -5.910057748645132e-05}, {\"day\": 16, \"difference\": 2.694345067611164e-05}, {\"day\": 17, \"difference\": 8.427410297810398e-05}, {\"day\": 18, \"difference\": 4.7029359859830384e-05}, {\"day\": 19, \"difference\": 5.519046170845542e-05}, {\"day\": 20, \"difference\": 5.216389529593595e-05}, {\"day\": 21, \"difference\": 1.1917458555982426e-05}, {\"day\": 22, \"difference\": 8.586935333599124e-05}, {\"day\": 23, \"difference\": 7.01774487702235e-05}, {\"day\": 24, \"difference\": -2.428488850986666e-06}, {\"day\": 25, \"difference\": -5.065796086878814e-05}, {\"day\": 26, \"difference\": 9.435097501201599e-05}, {\"day\": 27, \"difference\": -5.220685738920299e-05}, {\"day\": 28, \"difference\": -5.5447103686655534e-05}, {\"day\": 29, \"difference\": -2.259975693053853e-05}, {\"day\": 30, \"difference\": 3.4862608104759127e-05}, {\"day\": 31, \"difference\": 5.796434317768037e-05}]}}, {\"mode\": \"vega-lite\"});","title":"Visit merging"},{"location":"functionalities/patients-course/visit_merging/#merging-visits-into-stays","text":"In order to have a precise view of each patient's course of care, it can be useful to merge together visit occurrences into stays. A crude way of doing so is by using the preceding_visit_occurrence_id column in the visit_occurrence table. However, this columns isn't always filled, and a lot of visits would be missed by using only this method. The method proposed here relies on how close two visits are in order to put them in the same stay. This is the role of the merge_visits() functions: import eds_scikit spark , sc , sql = eds_scikit . improve_performances () from eds_scikit.period.stays import merge_visits Another function, get_stays_duration() , can then be used to extract a stay DataFrame with useful informations: from eds_scikit.period.stays import get_stays_duration % config Completer . use_jedi = False % load_ext autoreload % autoreload 2","title":"Merging visits into stays"},{"location":"functionalities/patients-course/visit_merging/#loading-data","text":"import databricks.koalas as ks import pandas as pd import altair as alt from datetime import datetime , timedelta from eds_scikit.io import HiveData from eds_scikit.utils.datetime_helpers import substract_datetime data = HiveData ( spark , database_name = 'eds_lib_poc' ) vo = data . visit_occurrence This cohort is of reasonnable size, so we can work with Pandas in this case: vo_pd = vo . to_pandas ()","title":"Loading data"},{"location":"functionalities/patients-course/visit_merging/#getting-stays","text":"We can now merge visits into stays. Check the corresponding documentation for more informations about each individual parameter vo_pd = merge_visits ( vo_pd , remove_deleted_visits = True , long_stay_threshold = timedelta ( days = 365 ), long_stay_filtering = 'all' , max_timedelta = timedelta ( days = 2 ), merge_different_hospitals = False , merge_different_source_values = [ 'hospitalis\u00e9s' , 'urgence' ], ) This functions will add a 'STAY_ID' column, corresponding to the visit_occurrence_id of the first visit of the stay. We can check that indeed, most stays are composed of a single visit (notice that the Y-axis is in log scale): stats = vo_pd . groupby ( 'STAY_ID' ) . visit_occurrence_id . count () _ = stats . hist ( bins = 20 , log = True ) We can finally display the number of merged visits: stats [ stats > 1 ] . sum () 42738 or in % of the total number of visits: round ( 100 * stats [ stats > 1 ] . sum () / len ( vo_pd ), 2 ) 8.69","title":"Getting stays"},{"location":"functionalities/patients-course/visit_merging/#getting-stays-durations","text":"This second function generates an easy-to-use stay DataFrame : We will only focus on emergency and hospitalisation for this part, which is no problem since we only allowed merging those two types of stay (via merge_different_source_values=['hospitalis\u00e9s', 'urgence'] ) vo_pd = vo_pd [ vo_pd . visit_source_value . isin ([ 'hospitalis\u00e9s' , 'urgence' ])] stays = get_stays_duration ( vo_pd , algo = 'visits_date_difference' , missing_end_date_handling = 'coerce' ) stays . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } person_id t_start t_end STAY_DURATION STAY_ID -2.147468e+09 -793380275 2017-08-22 18:45:00 2017-08-25 14:24:00 67.650000 -2.147465e+09 -1632607976 2019-12-03 12:20:00 2019-12-03 13:42:00 1.366667 -2.147450e+09 56307107 2019-04-23 17:18:00 2019-04-25 16:21:00 47.050000 -2.147433e+09 -916176344 2021-04-23 10:40:00 2021-04-23 11:50:00 1.166667 -2.147431e+09 1814301591 2018-06-06 14:25:00 2018-06-06 15:24:00 0.983333 Let us compare the distribution of stay/visit durations. # Extracting visit duration (in hours) vo_pd [ 'VISIT_DURATION' ] = substract_datetime ( vo_pd [ 'visit_end_datetime' ], vo_pd [ 'visit_start_datetime' ], out = 'hours' ) # COnverting to days stays [ 'STAY_DURATION' ] = stays [ 'STAY_DURATION' ] / 24 vo_pd [ 'VISIT_DURATION' ] = vo_pd [ 'VISIT_DURATION' ] / 24 # Keeping only visits/stays less than a month long vo_pd = vo_pd [ vo_pd [ 'VISIT_DURATION' ] <= 31 ] stays = stays [ stays [ 'STAY_DURATION' ] <= 31 ] stays . STAY_DURATION . mean () 2.306892174348691 vo_pd . VISIT_DURATION . mean () 2.2188916112474866 We will aggregate the data into bins of 1 week days = list ( range ( 1 , 32 )) stays_distribution = pd . cut ( stays [ 'STAY_DURATION' ], 31 , labels = days ) . value_counts ( normalize = True ) . sort_index () visits_distribution = pd . cut ( vo_pd [ 'VISIT_DURATION' ], 31 , labels = days ) . value_counts ( normalize = True ) . sort_index () data = pd . concat ([ pd . DataFrame ( data = { 'density' : stays_distribution . values , 'day' : stays_distribution . index , 'type' : 'STAY' }), pd . DataFrame ( data = { 'density' : visits_distribution . values , 'day' : visits_distribution . index , 'type' : 'VISIT' }) ]) diff_distribution = ( stays_distribution - visits_distribution ) . to_frame () . reset_index () diff_distribution . columns = [ 'day' , 'difference' ] alt . Chart ( data ) . mark_bar ( opacity = 0.5 ) . encode ( x = \"day:N\" , y = alt . Y ( \"density:Q\" , stack = None , scale = alt . Scale ( type = 'log' )), color = alt . Color ( \"type:N\" , title = 'Type' ) ) . properties ( width = 800 ) (function(spec, embedOpt){ let outputDiv = document.currentScript.previousElementSibling; if (outputDiv.id !== \"altair-viz-06979841a60142e78d60594f1d1b0722\") { outputDiv = document.getElementById(\"altair-viz-06979841a60142e78d60594f1d1b0722\"); } const paths = { \"vega\": \"https://cdn.jsdelivr.net/npm//vega@5?noext\", \"vega-lib\": \"https://cdn.jsdelivr.net/npm//vega-lib?noext\", \"vega-lite\": \"https://cdn.jsdelivr.net/npm//vega-lite@4.8.1?noext\", \"vega-embed\": \"https://cdn.jsdelivr.net/npm//vega-embed@6?noext\", }; function loadScript(lib) { return new Promise(function(resolve, reject) { var s = document.createElement('script'); s.src = paths[lib]; s.async = true; s.onload = () => resolve(paths[lib]); s.onerror = () => reject(`Error loading script: ${paths[lib]}`); document.getElementsByTagName(\"head\")[0].appendChild(s); }); } function showError(err) { outputDiv.innerHTML = `<div class=\"error\" style=\"color:red;\">${err}</div>`; throw err; } function displayChart(vegaEmbed) { vegaEmbed(outputDiv, spec, embedOpt) .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`)); } if(typeof define === \"function\" && define.amd) { requirejs.config({paths}); require([\"vega-embed\"], displayChart, err => showError(`Error loading script: ${err.message}`)); } else if (typeof vegaEmbed === \"function\") { displayChart(vegaEmbed); } else { loadScript(\"vega\") .then(() => loadScript(\"vega-lite\")) .then(() => loadScript(\"vega-embed\")) .catch(showError) .then(() => displayChart(vegaEmbed)); } })({\"config\": {\"view\": {\"continuousWidth\": 400, \"continuousHeight\": 300}}, \"data\": {\"name\": \"data-b0c53aa5acb801ec3d3cb4990609769b\"}, \"mark\": {\"type\": \"bar\", \"opacity\": 0.5}, \"encoding\": {\"color\": {\"type\": \"nominal\", \"field\": \"type\", \"title\": \"Type\"}, \"x\": {\"type\": \"nominal\", \"field\": \"day\"}, \"y\": {\"type\": \"quantitative\", \"field\": \"density\", \"scale\": {\"type\": \"log\"}, \"stack\": null}}, \"width\": 800, \"$schema\": \"https://vega.github.io/schema/vega-lite/v4.8.1.json\", \"datasets\": {\"data-b0c53aa5acb801ec3d3cb4990609769b\": [{\"density\": 0.6257372293084775, \"day\": 1, \"type\": \"STAY\"}, {\"density\": 0.08317573975454683, \"day\": 2, \"type\": \"STAY\"}, {\"density\": 0.060734788244734876, \"day\": 3, \"type\": \"STAY\"}, {\"density\": 0.05161026032401614, \"day\": 4, \"type\": \"STAY\"}, {\"density\": 0.04074612375912593, \"day\": 5, \"type\": \"STAY\"}, {\"density\": 0.024679996184084264, \"day\": 6, \"type\": \"STAY\"}, {\"density\": 0.01920303477533796, \"day\": 7, \"type\": \"STAY\"}, {\"density\": 0.016015622807952816, \"day\": 8, \"type\": \"STAY\"}, {\"density\": 0.010914641331979058, \"day\": 9, \"type\": \"STAY\"}, {\"density\": 0.008630703531405548, \"day\": 10, \"type\": \"STAY\"}, {\"density\": 0.007272686460794271, \"day\": 11, \"type\": \"STAY\"}, {\"density\": 0.005869776263881797, \"day\": 12, \"type\": \"STAY\"}, {\"density\": 0.005145874602274959, \"day\": 13, \"type\": \"STAY\"}, {\"density\": 0.005173932806213209, \"day\": 14, \"type\": \"STAY\"}, {\"density\": 0.004595933805085269, \"day\": 15, \"type\": \"STAY\"}, {\"density\": 0.003849585580327832, \"day\": 16, \"type\": \"STAY\"}, {\"density\": 0.0031593537634468944, \"day\": 17, \"type\": \"STAY\"}, {\"density\": 0.002676752655709003, \"day\": 18, \"type\": \"STAY\"}, {\"density\": 0.0025364616360177552, \"day\": 19, \"type\": \"STAY\"}, {\"density\": 0.0022895494413611595, \"day\": 20, \"type\": \"STAY\"}, {\"density\": 0.002328830926874709, \"day\": 21, \"type\": \"STAY\"}, {\"density\": 0.002227821392697011, \"day\": 22, \"type\": \"STAY\"}, {\"density\": 0.0016554340323567207, \"day\": 23, \"type\": \"STAY\"}, {\"density\": 0.0014926964495148736, \"day\": 24, \"type\": \"STAY\"}, {\"density\": 0.0011952794877694288, \"day\": 25, \"type\": \"STAY\"}, {\"density\": 0.0012289493324953284, \"day\": 26, \"type\": \"STAY\"}, {\"density\": 0.0011672212838311794, \"day\": 27, \"type\": \"STAY\"}, {\"density\": 0.0014926964495148736, \"day\": 28, \"type\": \"STAY\"}, {\"density\": 0.0012233376917076785, \"day\": 29, \"type\": \"STAY\"}, {\"density\": 0.001015706982564632, \"day\": 30, \"type\": \"STAY\"}, {\"density\": 0.0009539789339004832, \"day\": 31, \"type\": \"STAY\"}, {\"density\": 0.65274397841082, \"day\": 1, \"type\": \"VISIT\"}, {\"density\": 0.07284227492272537, \"day\": 2, \"type\": \"VISIT\"}, {\"density\": 0.05349578236918982, \"day\": 3, \"type\": \"VISIT\"}, {\"density\": 0.04853854188205479, \"day\": 4, \"type\": \"VISIT\"}, {\"density\": 0.038677079522620395, \"day\": 5, \"type\": \"VISIT\"}, {\"density\": 0.02326456818989147, \"day\": 6, \"type\": \"VISIT\"}, {\"density\": 0.018222497919019368, \"day\": 7, \"type\": \"VISIT\"}, {\"density\": 0.015560963454268793, \"day\": 8, \"type\": \"VISIT\"}, {\"density\": 0.010550704352298091, \"day\": 9, \"type\": \"VISIT\"}, {\"density\": 0.008281507637331468, \"day\": 10, \"type\": \"VISIT\"}, {\"density\": 0.006993155296824715, \"day\": 11, \"type\": \"VISIT\"}, {\"density\": 0.005810840185989301, \"day\": 12, \"type\": \"VISIT\"}, {\"density\": 0.005100390747191339, \"day\": 13, \"type\": \"VISIT\"}, {\"density\": 0.005206427976862676, \"day\": 14, \"type\": \"VISIT\"}, {\"density\": 0.0046550343825717206, \"day\": 15, \"type\": \"VISIT\"}, {\"density\": 0.0038226421296517206, \"day\": 16, \"type\": \"VISIT\"}, {\"density\": 0.0030750796604687904, \"day\": 17, \"type\": \"VISIT\"}, {\"density\": 0.0026297232958491725, \"day\": 18, \"type\": \"VISIT\"}, {\"density\": 0.0024812711743093, \"day\": 19, \"type\": \"VISIT\"}, {\"density\": 0.0022373855460652236, \"day\": 20, \"type\": \"VISIT\"}, {\"density\": 0.0023169134683187266, \"day\": 21, \"type\": \"VISIT\"}, {\"density\": 0.0021419520393610196, \"day\": 22, \"type\": \"VISIT\"}, {\"density\": 0.0015852565835864972, \"day\": 23, \"type\": \"VISIT\"}, {\"density\": 0.0014951249383658603, \"day\": 24, \"type\": \"VISIT\"}, {\"density\": 0.001245937448638217, \"day\": 25, \"type\": \"VISIT\"}, {\"density\": 0.0011345983574833124, \"day\": 26, \"type\": \"VISIT\"}, {\"density\": 0.0012194281412203824, \"day\": 27, \"type\": \"VISIT\"}, {\"density\": 0.0015481435532015292, \"day\": 28, \"type\": \"VISIT\"}, {\"density\": 0.001245937448638217, \"day\": 29, \"type\": \"VISIT\"}, {\"density\": 0.0009808443744598729, \"day\": 30, \"type\": \"VISIT\"}, {\"density\": 0.0008960145907228028, \"day\": 31, \"type\": \"VISIT\"}]}}, {\"mode\": \"vega-lite\"}); Plotting the difference between those two distributions shows that - Some short visits ( < 1 week ) seems to be merged into longer stays - This cause stays of duration ~ N weeks to be more frequent than visits of duration ~ N weeks alt . Chart ( diff_distribution ) . mark_bar () . encode ( x = alt . X ( \"day:N\" , title = \"Day\" ), y = alt . Y ( \"difference:Q\" , title = 'Difference between stay and visit distribution' ) ) . properties ( width = 800 ) (function(spec, embedOpt){ let outputDiv = document.currentScript.previousElementSibling; if (outputDiv.id !== \"altair-viz-bfc21174f79b41678f31cbc2a2bdb70d\") { outputDiv = document.getElementById(\"altair-viz-bfc21174f79b41678f31cbc2a2bdb70d\"); } const paths = { \"vega\": \"https://cdn.jsdelivr.net/npm//vega@5?noext\", \"vega-lib\": \"https://cdn.jsdelivr.net/npm//vega-lib?noext\", \"vega-lite\": \"https://cdn.jsdelivr.net/npm//vega-lite@4.8.1?noext\", \"vega-embed\": \"https://cdn.jsdelivr.net/npm//vega-embed@6?noext\", }; function loadScript(lib) { return new Promise(function(resolve, reject) { var s = document.createElement('script'); s.src = paths[lib]; s.async = true; s.onload = () => resolve(paths[lib]); s.onerror = () => reject(`Error loading script: ${paths[lib]}`); document.getElementsByTagName(\"head\")[0].appendChild(s); }); } function showError(err) { outputDiv.innerHTML = `<div class=\"error\" style=\"color:red;\">${err}</div>`; throw err; } function displayChart(vegaEmbed) { vegaEmbed(outputDiv, spec, embedOpt) .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`)); } if(typeof define === \"function\" && define.amd) { requirejs.config({paths}); require([\"vega-embed\"], displayChart, err => showError(`Error loading script: ${err.message}`)); } else if (typeof vegaEmbed === \"function\") { displayChart(vegaEmbed); } else { loadScript(\"vega\") .then(() => loadScript(\"vega-lite\")) .then(() => loadScript(\"vega-embed\")) .catch(showError) .then(() => displayChart(vegaEmbed)); } })({\"config\": {\"view\": {\"continuousWidth\": 400, \"continuousHeight\": 300}}, \"data\": {\"name\": \"data-c73fdd6af4ddb3276ed1e9ff4cf51303\"}, \"mark\": \"bar\", \"encoding\": {\"x\": {\"type\": \"nominal\", \"field\": \"day\", \"title\": \"Day\"}, \"y\": {\"type\": \"quantitative\", \"field\": \"difference\", \"title\": \"Difference between stay and visit distribution\"}}, \"width\": 800, \"$schema\": \"https://vega.github.io/schema/vega-lite/v4.8.1.json\", \"datasets\": {\"data-c73fdd6af4ddb3276ed1e9ff4cf51303\": [{\"day\": 1, \"difference\": -0.027006749102342464}, {\"day\": 2, \"difference\": 0.01033346483182146}, {\"day\": 3, \"difference\": 0.007239005875545053}, {\"day\": 4, \"difference\": 0.0030717184419613505}, {\"day\": 5, \"difference\": 0.0020690442365055364}, {\"day\": 6, \"difference\": 0.0014154279941927944}, {\"day\": 7, \"difference\": 0.000980536856318593}, {\"day\": 8, \"difference\": 0.00045465935368402266}, {\"day\": 9, \"difference\": 0.000363936979680967}, {\"day\": 10, \"difference\": 0.00034919589407408046}, {\"day\": 11, \"difference\": 0.0002795311639695562}, {\"day\": 12, \"difference\": 5.893607789249616e-05}, {\"day\": 13, \"difference\": 4.548385508362004e-05}, {\"day\": 14, \"difference\": -3.249517064946722e-05}, {\"day\": 15, \"difference\": -5.910057748645132e-05}, {\"day\": 16, \"difference\": 2.694345067611164e-05}, {\"day\": 17, \"difference\": 8.427410297810398e-05}, {\"day\": 18, \"difference\": 4.7029359859830384e-05}, {\"day\": 19, \"difference\": 5.519046170845542e-05}, {\"day\": 20, \"difference\": 5.216389529593595e-05}, {\"day\": 21, \"difference\": 1.1917458555982426e-05}, {\"day\": 22, \"difference\": 8.586935333599124e-05}, {\"day\": 23, \"difference\": 7.01774487702235e-05}, {\"day\": 24, \"difference\": -2.428488850986666e-06}, {\"day\": 25, \"difference\": -5.065796086878814e-05}, {\"day\": 26, \"difference\": 9.435097501201599e-05}, {\"day\": 27, \"difference\": -5.220685738920299e-05}, {\"day\": 28, \"difference\": -5.5447103686655534e-05}, {\"day\": 29, \"difference\": -2.259975693053853e-05}, {\"day\": 30, \"difference\": 3.4862608104759127e-05}, {\"day\": 31, \"difference\": 5.796434317768037e-05}]}}, {\"mode\": \"vega-lite\"});","title":"Getting stays durations"},{"location":"functionalities/patients-course/visit_merging/","text":"Merging visits into stays Presentation of the problem In order to have a precise view of each patient's course of care, it can be useful to merge together visit occurrences into stays. A crude way of doing so is by using the preceding_visit_occurrence_id column in the visit_occurrence table. However, this column isn't always filled, and a lot of visits would be missed by using only this method. The method proposed here relies on how close two visits are in order to put them in the same stay. This is the role of the merge_visits() functions. The figure below shows how the merging of visits into stays would occurs The merge_visits() function from eds_scikit.io import HiveData data = HiveData ( DBNAME ) from eds_scikit.period.stays import merge_visits visit_occurrence = merge_visits ( visit_occurrence ) Warning The snippet above should run as is , however the merge_visits() function provides a lot of parameters that you should check in order to use it properly. Those parameters are described below or in the corresponding code reference Merge \"close\" visit occurrences to consider them as a single stay by adding a STAY_ID and CONTIGUOUS_STAY_ID columns to the DataFrame. The value of these columns will be the visit_occurrence_id of the first (meaning the oldest) visit of the stay. From a temporal point of view, we consider that two visits belong to the same stay if either: They intersect The time difference between the end of the most recent and the start of the oldest is lower than max_timedelta (for STAY_ID ) or 0 (for CONTIGUOUS_STAY_ID ) Additionally, other parameters are available to further adjust the merging rules. See below. PARAMETER DESCRIPTION vo The visit_occurrence DataFrame, with at least the following columns: - visit_occurrence_id - person_id - visit_start_datetime_calc (from preprocessing) - visit_end_datetime (from preprocessing) Depending on the input parameters, additional columns may be required: - care_site_id (if merge_different_hospitals == True ) - visit_source_value (if merge_different_source_values != False ) - row_status_source_value (if remove_deleted_visits= True ) TYPE: DataFrame remove_deleted_visits Wether to remove deleted visits from the merging procedure. Deleted visits are extracted via the row_status_source_value column TYPE: bool DEFAULT: True long_stay_filtering Filtering method for long and/or non-closed visits. First of all, visits with no starting date won't be merged with any other visit, and visits with no ending date will have a temporary \"theoretical\" ending date set by datetime.now() . That being said, some visits are sometimes years long because they weren't closed at time. If other visits occurred during this timespan, they could be all merged into the same stay. To avoid this issue, filtering can be done depending on the long_stay_filtering value: all : All long stays (closed and open) are removed from the merging procedure open : Only long open stays are removed from the merging procedure None : No filtering is done on long visits Long stays are determined by the long_stay_threshold value. TYPE: Optional [ str ] DEFAULT: 'all' long_stay_threshold Minimum visit duration value to consider a visit as candidate for \"long visits filtering\" TYPE: timedelta DEFAULT: timedelta(days=365) open_stay_end_datetime Datetime to use in order to fill the visit_end_datetime of open visits. This is necessary in order to compute stay duration and to filter long stays. If not provided datetime.now() will be used. You might provide the extraction date of your data here. TYPE: Optional [ datetime ] DEFAULT: None max_timedelta Maximum time difference between the end of a visit and the start of another to consider them as belonging to the same stay. This duration is internally converted in seconds before comparing. Thus, if you want e.g. to merge visits happening in two consecutive days, you should use timedelta(days=2) and NOT timedelta(days=1) in order to take into account extreme cases such as an first visit ending on Monday at 00h01 AM and another one starting at 23h59 PM on Tuesday TYPE: timedelta DEFAULT: timedelta(days=2) merge_different_hospitals Wether to allow visits occurring in different hospitals to be merged into a same stay TYPE: bool DEFAULT: False merge_different_source_values Wether to allow visits with different visit_source_value to be merged into a same stay. Values can be: True : the visit_source_value isn't taken into account for the merging False : only visits with the same visit_source_value can be merged into a same stay List[str] : only visits which visit_source_value is in the provided list can be merged together. Warning : You should avoid merging visits where visit_source_value == \"hospitalisation incompl\u00e8te\" , because those stays are often never closed. TYPE: Union [ bool , List [ str ]] DEFAULT: ['hospitalis\u00e9s', 'urgence'] RETURNS DESCRIPTION vo Visit occurrence DataFrame with additional STAY_ID column TYPE: DataFrame Examples: >>> import pandas as pd >>> from datetime import datetime , timedelta >>> data = { 1 : ['A', 999, datetime(2021,1,1), datetime(2021,1,5), 'hospitalis\u00e9s'], 2 : ['B', 999, datetime(2021,1,4), datetime(2021,1,8), 'hospitalis\u00e9s'], 3 : ['C', 999, datetime(2021,1,12), datetime(2021,1,18), 'hospitalis\u00e9s'], 4 : ['D', 999, datetime(2021,1,13), datetime(2021,1,14), 'urgence'], 5 : ['E', 999, datetime(2021,1,19), datetime(2021,1,21), 'hospitalis\u00e9s'], 6 : ['F', 999, datetime(2021,1,25), datetime(2021,1,27), 'hospitalis\u00e9s'], 7 : ['G', 999, datetime(2017,1,1), None, \"hospitalis\u00e9s\"] } >>> vo = pd . DataFrame . from_dict ( data, orient=\"index\", columns=[ \"visit_occurrence_id\", \"person_id\", \"visit_start_datetime\", \"visit_end_datetime\", \"visit_source_value\", ], ) >>> vo visit_occurrence_id person_id visit_start_datetime visit_end_datetime visit_source_value 1 A 999 2021-01-01 2021-01-05 hospitalis\u00e9s 2 B 999 2021-01-04 2021-01-08 hospitalis\u00e9s 3 C 999 2021-01-12 2021-01-18 hospitalis\u00e9s 4 D 999 2021-01-13 2021-01-14 urgence 5 E 999 2021-01-19 2021-01-21 hospitalis\u00e9s 6 F 999 2021-01-25 2021-01-27 hospitalis\u00e9s 7 G 999 2017-01-01 NaT hospitalis\u00e9s >>> vo = merge_visits ( vo, remove_deleted_visits=True, long_stay_threshold=timedelta(days=365), long_stay_filtering=\"all\", max_timedelta=timedelta(hours=24), merge_different_hospitals=False, merge_different_source_values=[\"hospitalis\u00e9s\", \"urgence\"], ) >>> vo visit_occurrence_id person_id visit_start_datetime visit_end_datetime visit_source_value STAY_ID CONTIGUOUS_STAY_ID 1 A 999 2021-01-01 2021-01-05 hospitalis\u00e9s A A 2 B 999 2021-01-04 2021-01-08 hospitalis\u00e9s A A 3 C 999 2021-01-12 2021-01-18 hospitalis\u00e9s C C 4 D 999 2021-01-13 2021-01-14 urgence C C 5 E 999 2021-01-19 2021-01-21 hospitalis\u00e9s C E 6 F 999 2021-01-25 2021-01-27 hospitalis\u00e9s F F 7 G 999 2017-01-01 NaT hospitalis\u00e9s G G Computing stay duration Presentation of the problem Once that visits are grouped into stays, you might want to compute stays duration. The get_stays_duration() function from eds_scikit.period.stays import get_stays_duration This function should be used once you called the merge_visits() functions. It adds a STAY_DURATION column. vo = get_stays_duration ( vo , algo = \"visits_date_difference\" , missing_end_date_handling = \"fill\" , ) There are actually two ways to compute those stays durations. Pick the \"algo\" value that suits your needs. Availables algorithms (values for \"algo\" ) 'visits_date_difference' 'sum_of_visits_duration' The stay duration corresponds to the difference between the end datetime of the stay's last visit and the start datetime of the stay's first visit . The stay duration corresponds to the sum of the duration of all visits of the stay (and by handling overlapping) Please check the documentation for additional parameters.","title":"Visit merging"},{"location":"functionalities/patients-course/visit_merging/#merging-visits-into-stays","text":"","title":"Merging visits into stays"},{"location":"functionalities/patients-course/visit_merging/#presentation-of-the-problem","text":"In order to have a precise view of each patient's course of care, it can be useful to merge together visit occurrences into stays. A crude way of doing so is by using the preceding_visit_occurrence_id column in the visit_occurrence table. However, this column isn't always filled, and a lot of visits would be missed by using only this method. The method proposed here relies on how close two visits are in order to put them in the same stay. This is the role of the merge_visits() functions. The figure below shows how the merging of visits into stays would occurs","title":"Presentation of the problem"},{"location":"functionalities/patients-course/visit_merging/#the-merge_visits-function","text":"from eds_scikit.io import HiveData data = HiveData ( DBNAME ) from eds_scikit.period.stays import merge_visits visit_occurrence = merge_visits ( visit_occurrence ) Warning The snippet above should run as is , however the merge_visits() function provides a lot of parameters that you should check in order to use it properly. Those parameters are described below or in the corresponding code reference Merge \"close\" visit occurrences to consider them as a single stay by adding a STAY_ID and CONTIGUOUS_STAY_ID columns to the DataFrame. The value of these columns will be the visit_occurrence_id of the first (meaning the oldest) visit of the stay. From a temporal point of view, we consider that two visits belong to the same stay if either: They intersect The time difference between the end of the most recent and the start of the oldest is lower than max_timedelta (for STAY_ID ) or 0 (for CONTIGUOUS_STAY_ID ) Additionally, other parameters are available to further adjust the merging rules. See below. PARAMETER DESCRIPTION vo The visit_occurrence DataFrame, with at least the following columns: - visit_occurrence_id - person_id - visit_start_datetime_calc (from preprocessing) - visit_end_datetime (from preprocessing) Depending on the input parameters, additional columns may be required: - care_site_id (if merge_different_hospitals == True ) - visit_source_value (if merge_different_source_values != False ) - row_status_source_value (if remove_deleted_visits= True ) TYPE: DataFrame remove_deleted_visits Wether to remove deleted visits from the merging procedure. Deleted visits are extracted via the row_status_source_value column TYPE: bool DEFAULT: True long_stay_filtering Filtering method for long and/or non-closed visits. First of all, visits with no starting date won't be merged with any other visit, and visits with no ending date will have a temporary \"theoretical\" ending date set by datetime.now() . That being said, some visits are sometimes years long because they weren't closed at time. If other visits occurred during this timespan, they could be all merged into the same stay. To avoid this issue, filtering can be done depending on the long_stay_filtering value: all : All long stays (closed and open) are removed from the merging procedure open : Only long open stays are removed from the merging procedure None : No filtering is done on long visits Long stays are determined by the long_stay_threshold value. TYPE: Optional [ str ] DEFAULT: 'all' long_stay_threshold Minimum visit duration value to consider a visit as candidate for \"long visits filtering\" TYPE: timedelta DEFAULT: timedelta(days=365) open_stay_end_datetime Datetime to use in order to fill the visit_end_datetime of open visits. This is necessary in order to compute stay duration and to filter long stays. If not provided datetime.now() will be used. You might provide the extraction date of your data here. TYPE: Optional [ datetime ] DEFAULT: None max_timedelta Maximum time difference between the end of a visit and the start of another to consider them as belonging to the same stay. This duration is internally converted in seconds before comparing. Thus, if you want e.g. to merge visits happening in two consecutive days, you should use timedelta(days=2) and NOT timedelta(days=1) in order to take into account extreme cases such as an first visit ending on Monday at 00h01 AM and another one starting at 23h59 PM on Tuesday TYPE: timedelta DEFAULT: timedelta(days=2) merge_different_hospitals Wether to allow visits occurring in different hospitals to be merged into a same stay TYPE: bool DEFAULT: False merge_different_source_values Wether to allow visits with different visit_source_value to be merged into a same stay. Values can be: True : the visit_source_value isn't taken into account for the merging False : only visits with the same visit_source_value can be merged into a same stay List[str] : only visits which visit_source_value is in the provided list can be merged together. Warning : You should avoid merging visits where visit_source_value == \"hospitalisation incompl\u00e8te\" , because those stays are often never closed. TYPE: Union [ bool , List [ str ]] DEFAULT: ['hospitalis\u00e9s', 'urgence'] RETURNS DESCRIPTION vo Visit occurrence DataFrame with additional STAY_ID column TYPE: DataFrame Examples: >>> import pandas as pd >>> from datetime import datetime , timedelta >>> data = { 1 : ['A', 999, datetime(2021,1,1), datetime(2021,1,5), 'hospitalis\u00e9s'], 2 : ['B', 999, datetime(2021,1,4), datetime(2021,1,8), 'hospitalis\u00e9s'], 3 : ['C', 999, datetime(2021,1,12), datetime(2021,1,18), 'hospitalis\u00e9s'], 4 : ['D', 999, datetime(2021,1,13), datetime(2021,1,14), 'urgence'], 5 : ['E', 999, datetime(2021,1,19), datetime(2021,1,21), 'hospitalis\u00e9s'], 6 : ['F', 999, datetime(2021,1,25), datetime(2021,1,27), 'hospitalis\u00e9s'], 7 : ['G', 999, datetime(2017,1,1), None, \"hospitalis\u00e9s\"] } >>> vo = pd . DataFrame . from_dict ( data, orient=\"index\", columns=[ \"visit_occurrence_id\", \"person_id\", \"visit_start_datetime\", \"visit_end_datetime\", \"visit_source_value\", ], ) >>> vo visit_occurrence_id person_id visit_start_datetime visit_end_datetime visit_source_value 1 A 999 2021-01-01 2021-01-05 hospitalis\u00e9s 2 B 999 2021-01-04 2021-01-08 hospitalis\u00e9s 3 C 999 2021-01-12 2021-01-18 hospitalis\u00e9s 4 D 999 2021-01-13 2021-01-14 urgence 5 E 999 2021-01-19 2021-01-21 hospitalis\u00e9s 6 F 999 2021-01-25 2021-01-27 hospitalis\u00e9s 7 G 999 2017-01-01 NaT hospitalis\u00e9s >>> vo = merge_visits ( vo, remove_deleted_visits=True, long_stay_threshold=timedelta(days=365), long_stay_filtering=\"all\", max_timedelta=timedelta(hours=24), merge_different_hospitals=False, merge_different_source_values=[\"hospitalis\u00e9s\", \"urgence\"], ) >>> vo visit_occurrence_id person_id visit_start_datetime visit_end_datetime visit_source_value STAY_ID CONTIGUOUS_STAY_ID 1 A 999 2021-01-01 2021-01-05 hospitalis\u00e9s A A 2 B 999 2021-01-04 2021-01-08 hospitalis\u00e9s A A 3 C 999 2021-01-12 2021-01-18 hospitalis\u00e9s C C 4 D 999 2021-01-13 2021-01-14 urgence C C 5 E 999 2021-01-19 2021-01-21 hospitalis\u00e9s C E 6 F 999 2021-01-25 2021-01-27 hospitalis\u00e9s F F 7 G 999 2017-01-01 NaT hospitalis\u00e9s G G","title":"The merge_visits() function"},{"location":"functionalities/patients-course/visit_merging/#computing-stay-duration","text":"","title":"Computing stay duration"},{"location":"functionalities/patients-course/visit_merging/#presentation-of-the-problem_1","text":"Once that visits are grouped into stays, you might want to compute stays duration.","title":"Presentation of the problem"},{"location":"functionalities/patients-course/visit_merging/#the-get_stays_duration-function","text":"from eds_scikit.period.stays import get_stays_duration This function should be used once you called the merge_visits() functions. It adds a STAY_DURATION column. vo = get_stays_duration ( vo , algo = \"visits_date_difference\" , missing_end_date_handling = \"fill\" , ) There are actually two ways to compute those stays durations. Pick the \"algo\" value that suits your needs. Availables algorithms (values for \"algo\" ) 'visits_date_difference' 'sum_of_visits_duration' The stay duration corresponds to the difference between the end datetime of the stay's last visit and the start datetime of the stay's first visit . The stay duration corresponds to the sum of the duration of all visits of the stay (and by handling overlapping) Please check the documentation for additional parameters.","title":"The get_stays_duration() function"},{"location":"functionalities/phenotyping/base/","text":"How to use and developp phenotyping algorithms in eds-scikit The Phenotype class Phenotyping is done via the Phenotype class. Using this class, we can add features that will be stored in the features attribute. Features are DataFrames containing at least a person_id and a phenotype column. Additionaly: If phenotyping at the visit level, features contains a visit_occurrence_id column If using sub-phenotypes (e.g. types of diabetes, or various cancer localiizations), features contains a subphenotype column. We distinguish 2 main ways of adding features to a Phenotype instance: By querying the database to extract raw features By aggregating one or multiple existing features Available phenotypes eds-scikit is shipped with various phenotyping algorithms. For instance, the CancerFromICD10 class can be used to extract visits or patients with a cancer-related ICD10 code. All those phenotyping algorithms share the same API. We will demonstrate it using the CancerFromICD10 class from eds_scikit.io import HiveData data = HiveData ( DBNAME ) from eds_scikit.phenotype import CancerFromICD10 cancer = CancerFromICD10 ( data ) To run the phenotyping algorithm, simply run: data = cancer . to_data () This will put the resulting phenotype DataFrame in data.computed[\"CancerFromICD10\"] Most available phenotypes share the same parameters: PARAMETER DESCRIPTION data A BaseData object TYPE: BaseData cancer_types Optional list of cancer types to use for phenotyping TYPE: Optional[List[str]] DEFAULT: None level On which level to do the aggregation, either \"patient\" or \"visit\" TYPE: str DEFAULT: 'patient' subphenotype Whether the threshold should apply to the phenotype (\"phenotype\" column) of the subphenotype (\"subphenotype\" column) TYPE: bool DEFAULT: True threshold Minimal number of events (which definition depends on the level value) TYPE: int DEFAULT: 1 Please look into each algorithm's documentation for further specific details. Implement your own phenotyping algorithm TO help you implement your own phenotyping algorithm, the Phenotype class exposes method to Easily featch features based on ICD10 and CCAM codes Easily aggregate feature(s) using simple threshold rules The following paragraph will show how to implement a dummy phenotyping algorithm for moderate to terminal Chronic Kidney Disease (CKD). In short, it will: - Extract patients with ICD10 code for CKD - Extract patients with CCAM code for dialysis or kidney transplant - Aggregate those two feature by keeping patients with both features We will start by creating an instance of the Phenotype class: from eds_scikit.phenotype import Phenotype ckd = Phenotype ( data , name = \"DummyCKD\" ) Next we define the ICD10 and CCAM codes Codes formatting Under the hood, Phenotype will use the conditions_from_icd10 and procedures_from_ccam functions. Check their documentation for details on how to format the provided codes icd10_codes = { \"CKD\" : { \"regex\" : [ \"N18[345]\" ]}, } ccam_codes = { \"dialysis\" : { \"regex\" : [ \"JVJB001\" ]}, \"transplant\" : { \"exact\" : [ \"JAEA003\" ]}, } Finally, we can start designing the phenotyping algorithm: Get ICD10 features ckd = ckd.add_code_feature( output_feature=\"icd10\", source=\"icd10\", codes=icd10_codes, ) Get CCAM features ckd = ckd.add_code_feature( output_feature=\"ccam\", source=\"ccam\", codes=ccam_codes, ) Aggregate those 2 features ckd = ckd.agg_two_features( input_feature_1=\"icd10\", input_feature_2=\"ccam\", output_feature=\"CKD\", how=\"AND\", level=\"patient\", subphenotype=False, thresholds=(1, 1), ) The final phenotype DataFrame can now be added to the data object: data = ckd . to_data () It will be available under data.computed.CKD Available methods on Phenotype : Base class for phenotyping PARAMETER DESCRIPTION data A BaseData object TYPE: BaseData name Name of the phenotype. If left to None, the name of the class will be used instead TYPE: Optional [ str ] DEFAULT: None add_code_feature add_code_feature ( output_feature : str , codes : dict , source : str = 'icd10' , additional_filtering : Optional [ dict ] = None ) Adds a feature from either ICD10 or CCAM codes PARAMETER DESCRIPTION output_feature Name of the feature TYPE: str codes Dictionary of codes to provide to the from_codes function TYPE: dict source Either 'icd10' or 'ccam', by default 'icd10' TYPE: str DEFAULT: 'icd10' additional_filtering Dictionary passed to the from_codes functions for filtering TYPE: Optional [ dict ] DEFAULT: None RETURNS DESCRIPTION Phenotype The current Phenotype object with an additional feature stored in self.features[output_feature] agg_single_feature agg_single_feature ( input_feature : str , output_feature : Optional [ str ] = None , level : str = 'patient' , subphenotype : bool = True , threshold : int = 1 ) -> Phenotype Simple aggregation rule on a feature: If level=\"patient\", keeps patients with at least threshold visits showing the (sub)phenotype If level=\"visit\", keeps visits with at least threshold events (could be ICD10 codes, NLP features, biology, etc) showing the (sub)phenotype PARAMETER DESCRIPTION input_feature Name of the input feature TYPE: str output_feature Name of the input feature. If None, will be set to input_feature + \"_agg\" TYPE: Optional [ str ] DEFAULT: None level On which level to do the aggregation, either \"patient\" or \"visit\" TYPE: str DEFAULT: 'patient' subphenotype Whether the threshold should apply to the phenotype (\"phenotype\" column) of the subphenotype (\"subphenotype\" column) TYPE: bool DEFAULT: True threshold Minimal number of events (which definition depends on the level value) TYPE: int , optional DEFAULT: 1 RETURNS DESCRIPTION Phenotype The current Phenotype object with an additional feature stored in self.features[output_feature] agg_two_features agg_two_features ( input_feature_1 : str , input_feature_2 : str , output_feature : str = None , how : str = 'AND' , level : str = 'patient' , subphenotype : bool = True , thresholds : Tuple [ int , int ] = ( 1 , 1 )) -> Phenotype If level='patient', keeps a specific patient if At least thresholds[0] visits are found in feature_1 AND/OR At least thresholds[1] visits are found in feature_2 If level='visit', keeps a specific visit if At least thresholds[0] events are found in feature_1 AND/OR At least thresholds[1] events are found in feature_2 PARAMETER DESCRIPTION input_feature_1 Name of the first input feature TYPE: str input_feature_2 Name of the second input feature TYPE: str output_feature Name of the input feature. If None, will be set to input_feature + \"_agg\" TYPE: str DEFAULT: None how Whether to perform a boolean \"AND\" or \"OR\" aggregation TYPE: str , optional DEFAULT: 'AND' level On which level to do the aggregation, either \"patient\" or \"visit\" TYPE: str DEFAULT: 'patient' subphenotype Whether the threshold should apply to the phenotype (\"phenotype\" column) of the subphenotype (\"subphenotype\" column) TYPE: bool DEFAULT: True thresholds Repsective threshold for the first and second feature TYPE: Tuple [ int , int ], optional DEFAULT: (1, 1) RETURNS DESCRIPTION Phenotype The current Phenotype object with an additional feature stored in self.features[output_feature] compute compute ( ** kwargs ) Fetch all necessary features and perform aggregation to_data to_data ( key : Optional [ str ] = None ) -> BaseData Appends the feature found in self.features[key] to the data object. If no key is provided, uses the last added feature PARAMETER DESCRIPTION key Key of the self.feature dictionary TYPE: Optional [ str ] DEFAULT: None RETURNS DESCRIPTION BaseData The data object with phenotype added to data.computed Citation Most available phenotypes implement an algorithm described in an academic paper. When using this algorithm, you can get the BibTex citation of the corrresponding paper by calling the cite method. For instance: cancer . cite () @article { kempf2022impact , title = {Impact of two waves of Sars-Cov2 outbreak on the number, clinical presentation, care trajectories and survival of patients newly referred for a colorectal cancer: A French multicentric cohort study from a large group of University hospitals} , author = {Kempf, Emmanuelle and Priou, Sonia and Lam{\\'e}, Guillaume and Daniel, Christel and Bellamine, Ali and Sommacale, Daniele and Belkacemi, Yazid and Bey, Romain and Galula, Gilles and Taright, Namik and others} , journal = {International Journal of Cancer} , volume = {150} , number = {10} , pages = {1609--1618} , year = {2022} , publisher = {Wiley Online Library} }","title":"The `Phenotype` class"},{"location":"functionalities/phenotyping/base/#how-to-use-and-developp-phenotyping-algorithms-in-eds-scikit","text":"","title":"How to use and developp phenotyping algorithms in eds-scikit"},{"location":"functionalities/phenotyping/base/#the-phenotype-class","text":"Phenotyping is done via the Phenotype class. Using this class, we can add features that will be stored in the features attribute. Features are DataFrames containing at least a person_id and a phenotype column. Additionaly: If phenotyping at the visit level, features contains a visit_occurrence_id column If using sub-phenotypes (e.g. types of diabetes, or various cancer localiizations), features contains a subphenotype column. We distinguish 2 main ways of adding features to a Phenotype instance: By querying the database to extract raw features By aggregating one or multiple existing features","title":"The Phenotype class"},{"location":"functionalities/phenotyping/base/#available-phenotypes","text":"eds-scikit is shipped with various phenotyping algorithms. For instance, the CancerFromICD10 class can be used to extract visits or patients with a cancer-related ICD10 code. All those phenotyping algorithms share the same API. We will demonstrate it using the CancerFromICD10 class from eds_scikit.io import HiveData data = HiveData ( DBNAME ) from eds_scikit.phenotype import CancerFromICD10 cancer = CancerFromICD10 ( data ) To run the phenotyping algorithm, simply run: data = cancer . to_data () This will put the resulting phenotype DataFrame in data.computed[\"CancerFromICD10\"] Most available phenotypes share the same parameters: PARAMETER DESCRIPTION data A BaseData object TYPE: BaseData cancer_types Optional list of cancer types to use for phenotyping TYPE: Optional[List[str]] DEFAULT: None level On which level to do the aggregation, either \"patient\" or \"visit\" TYPE: str DEFAULT: 'patient' subphenotype Whether the threshold should apply to the phenotype (\"phenotype\" column) of the subphenotype (\"subphenotype\" column) TYPE: bool DEFAULT: True threshold Minimal number of events (which definition depends on the level value) TYPE: int DEFAULT: 1 Please look into each algorithm's documentation for further specific details.","title":"Available phenotypes"},{"location":"functionalities/phenotyping/base/#implement-your-own-phenotyping-algorithm","text":"TO help you implement your own phenotyping algorithm, the Phenotype class exposes method to Easily featch features based on ICD10 and CCAM codes Easily aggregate feature(s) using simple threshold rules The following paragraph will show how to implement a dummy phenotyping algorithm for moderate to terminal Chronic Kidney Disease (CKD). In short, it will: - Extract patients with ICD10 code for CKD - Extract patients with CCAM code for dialysis or kidney transplant - Aggregate those two feature by keeping patients with both features We will start by creating an instance of the Phenotype class: from eds_scikit.phenotype import Phenotype ckd = Phenotype ( data , name = \"DummyCKD\" ) Next we define the ICD10 and CCAM codes Codes formatting Under the hood, Phenotype will use the conditions_from_icd10 and procedures_from_ccam functions. Check their documentation for details on how to format the provided codes icd10_codes = { \"CKD\" : { \"regex\" : [ \"N18[345]\" ]}, } ccam_codes = { \"dialysis\" : { \"regex\" : [ \"JVJB001\" ]}, \"transplant\" : { \"exact\" : [ \"JAEA003\" ]}, } Finally, we can start designing the phenotyping algorithm:","title":"Implement your own phenotyping algorithm"},{"location":"functionalities/phenotyping/base/#get-icd10-features","text":"ckd = ckd.add_code_feature( output_feature=\"icd10\", source=\"icd10\", codes=icd10_codes, )","title":"Get ICD10 features"},{"location":"functionalities/phenotyping/base/#get-ccam-features","text":"ckd = ckd.add_code_feature( output_feature=\"ccam\", source=\"ccam\", codes=ccam_codes, )","title":"Get CCAM features"},{"location":"functionalities/phenotyping/base/#aggregate-those-2-features","text":"ckd = ckd.agg_two_features( input_feature_1=\"icd10\", input_feature_2=\"ccam\", output_feature=\"CKD\", how=\"AND\", level=\"patient\", subphenotype=False, thresholds=(1, 1), ) The final phenotype DataFrame can now be added to the data object: data = ckd . to_data () It will be available under data.computed.CKD","title":"Aggregate those 2 features"},{"location":"functionalities/phenotyping/base/#available-methods-on-phenotype","text":"Base class for phenotyping PARAMETER DESCRIPTION data A BaseData object TYPE: BaseData name Name of the phenotype. If left to None, the name of the class will be used instead TYPE: Optional [ str ] DEFAULT: None","title":"Available methods on Phenotype:"},{"location":"functionalities/phenotyping/base/#eds_scikit.phenotype.base.Phenotype.add_code_feature","text":"add_code_feature ( output_feature : str , codes : dict , source : str = 'icd10' , additional_filtering : Optional [ dict ] = None ) Adds a feature from either ICD10 or CCAM codes PARAMETER DESCRIPTION output_feature Name of the feature TYPE: str codes Dictionary of codes to provide to the from_codes function TYPE: dict source Either 'icd10' or 'ccam', by default 'icd10' TYPE: str DEFAULT: 'icd10' additional_filtering Dictionary passed to the from_codes functions for filtering TYPE: Optional [ dict ] DEFAULT: None RETURNS DESCRIPTION Phenotype The current Phenotype object with an additional feature stored in self.features[output_feature]","title":"add_code_feature()"},{"location":"functionalities/phenotyping/base/#eds_scikit.phenotype.base.Phenotype.agg_single_feature","text":"agg_single_feature ( input_feature : str , output_feature : Optional [ str ] = None , level : str = 'patient' , subphenotype : bool = True , threshold : int = 1 ) -> Phenotype Simple aggregation rule on a feature: If level=\"patient\", keeps patients with at least threshold visits showing the (sub)phenotype If level=\"visit\", keeps visits with at least threshold events (could be ICD10 codes, NLP features, biology, etc) showing the (sub)phenotype PARAMETER DESCRIPTION input_feature Name of the input feature TYPE: str output_feature Name of the input feature. If None, will be set to input_feature + \"_agg\" TYPE: Optional [ str ] DEFAULT: None level On which level to do the aggregation, either \"patient\" or \"visit\" TYPE: str DEFAULT: 'patient' subphenotype Whether the threshold should apply to the phenotype (\"phenotype\" column) of the subphenotype (\"subphenotype\" column) TYPE: bool DEFAULT: True threshold Minimal number of events (which definition depends on the level value) TYPE: int , optional DEFAULT: 1 RETURNS DESCRIPTION Phenotype The current Phenotype object with an additional feature stored in self.features[output_feature]","title":"agg_single_feature()"},{"location":"functionalities/phenotyping/base/#eds_scikit.phenotype.base.Phenotype.agg_two_features","text":"agg_two_features ( input_feature_1 : str , input_feature_2 : str , output_feature : str = None , how : str = 'AND' , level : str = 'patient' , subphenotype : bool = True , thresholds : Tuple [ int , int ] = ( 1 , 1 )) -> Phenotype If level='patient', keeps a specific patient if At least thresholds[0] visits are found in feature_1 AND/OR At least thresholds[1] visits are found in feature_2 If level='visit', keeps a specific visit if At least thresholds[0] events are found in feature_1 AND/OR At least thresholds[1] events are found in feature_2 PARAMETER DESCRIPTION input_feature_1 Name of the first input feature TYPE: str input_feature_2 Name of the second input feature TYPE: str output_feature Name of the input feature. If None, will be set to input_feature + \"_agg\" TYPE: str DEFAULT: None how Whether to perform a boolean \"AND\" or \"OR\" aggregation TYPE: str , optional DEFAULT: 'AND' level On which level to do the aggregation, either \"patient\" or \"visit\" TYPE: str DEFAULT: 'patient' subphenotype Whether the threshold should apply to the phenotype (\"phenotype\" column) of the subphenotype (\"subphenotype\" column) TYPE: bool DEFAULT: True thresholds Repsective threshold for the first and second feature TYPE: Tuple [ int , int ], optional DEFAULT: (1, 1) RETURNS DESCRIPTION Phenotype The current Phenotype object with an additional feature stored in self.features[output_feature]","title":"agg_two_features()"},{"location":"functionalities/phenotyping/base/#eds_scikit.phenotype.base.Phenotype.compute","text":"compute ( ** kwargs ) Fetch all necessary features and perform aggregation","title":"compute()"},{"location":"functionalities/phenotyping/base/#eds_scikit.phenotype.base.Phenotype.to_data","text":"to_data ( key : Optional [ str ] = None ) -> BaseData Appends the feature found in self.features[key] to the data object. If no key is provided, uses the last added feature PARAMETER DESCRIPTION key Key of the self.feature dictionary TYPE: Optional [ str ] DEFAULT: None RETURNS DESCRIPTION BaseData The data object with phenotype added to data.computed","title":"to_data()"},{"location":"functionalities/phenotyping/base/#citation","text":"Most available phenotypes implement an algorithm described in an academic paper. When using this algorithm, you can get the BibTex citation of the corrresponding paper by calling the cite method. For instance: cancer . cite () @article { kempf2022impact , title = {Impact of two waves of Sars-Cov2 outbreak on the number, clinical presentation, care trajectories and survival of patients newly referred for a colorectal cancer: A French multicentric cohort study from a large group of University hospitals} , author = {Kempf, Emmanuelle and Priou, Sonia and Lam{\\'e}, Guillaume and Daniel, Christel and Bellamine, Ali and Sommacale, Daniele and Belkacemi, Yazid and Bey, Romain and Galula, Gilles and Taright, Namik and others} , journal = {International Journal of Cancer} , volume = {150} , number = {10} , pages = {1609--1618} , year = {2022} , publisher = {Wiley Online Library} }","title":"Citation"},{"location":"functionalities/phenotyping/diabetes/","text":"Diabetes Presentation For the moment, we provide a diabetes phenotyping function based solely on ICD-10 codes. The diabetes_from_icd10() function from eds_scikit.io import HiveData data = HiveData ( DBNAME ) from eds_scikit.event import diabetes_from_icd10 visit_occurrence = diabetes_from_icd10 ( data . condition_occurrence , data . visit_occurrence , ) The snippet above will run as is and add two columns to the condition_occurrence DataFrame: A \"concept\" column, containing the \"DIABETES_FROM_ICD10\" value A \"value\" column, containing the type of diabetes extracted Please check the code reference for a complete explanation of the function.","title":"Diabetes"},{"location":"functionalities/phenotyping/diabetes/#diabetes","text":"","title":"Diabetes"},{"location":"functionalities/phenotyping/diabetes/#presentation","text":"For the moment, we provide a diabetes phenotyping function based solely on ICD-10 codes.","title":"Presentation"},{"location":"functionalities/phenotyping/diabetes/#the-diabetes_from_icd10-function","text":"from eds_scikit.io import HiveData data = HiveData ( DBNAME ) from eds_scikit.event import diabetes_from_icd10 visit_occurrence = diabetes_from_icd10 ( data . condition_occurrence , data . visit_occurrence , ) The snippet above will run as is and add two columns to the condition_occurrence DataFrame: A \"concept\" column, containing the \"DIABETES_FROM_ICD10\" value A \"value\" column, containing the type of diabetes extracted Please check the code reference for a complete explanation of the function.","title":"The diabetes_from_icd10() function"},{"location":"functionalities/phenotyping/suicide_attempts/","text":"Suicide attempt Presentation We provide the tag_suicide_attempt() function to extract suicide attempt from ICD-10 codes. The tag_suicide_attempt() function from eds_scikit.io import HiveData data = HiveData ( DBNAME ) from eds_scikit.event import tag_suicide_attempt visit_occurrence = tag_suicide_attempt ( data . visit_occurrence , data . condition_occurrence , algo = \"X60-X84\" , ) Availables algorithms (values for \"algo\" ) 'X60-X84' 'Haguenoer2008' Returns the visits that have at least one ICD code that belongs to the range X60 to X84. Returns the visits that follow the definiton of \"Haguenoer, Ken, Agn\u00e8s Caille, Marc Fillatre, Anne Isabelle Lecuyer, et Emmanuel Rusch. \u00ab Tentatives de Suicide \u00bb, 2008, 4.\". This rule requires at least one Main Diagnostic (DP) belonging to S00 to T98, and at least one Associated Diagnostic (DAS) that belongs to the range X60 to X84. You can check the documentation of the function for additional parameters: Function to return visits that fulfill different definitions of suicide attempt by ICD10. PARAMETER DESCRIPTION visit_occurrence TYPE: DataFrame condition_occurrence TYPE: DataFrame date_min Minimal starting date (on visit_start_datetime ) TYPE: Optional [ datetime ] DEFAULT: None date_max Maximal starting date (on visit_start_datetime ) TYPE: Optional [ datetime ] DEFAULT: None algo Method to use. Available values are: \"X60-X84\" : Will return a the visits that have at least one ICD code that belongs to the range X60 to X84. \"Haguenoer2008\" : Will return a the visits that follow the definiton of \" Haguenoer, Ken, Agn\u00e8s Caille, Marc Fillatre, Anne Isabelle Lecuyer, et Emmanuel Rusch. \u00ab Tentatives de Suicide \u00bb, 2008, 4. \". This rule requires at least one Main Diagnostic (DP) belonging to S00 to T98, and at least one Associated Diagnostic (DAS) that belongs to the range X60 to X84. TYPE: str DEFAULT: 'X60-X84' RETURNS DESCRIPTION visit_occurrence Tagged with an additional column SUICIDE_ATTEMPT TYPE: DataFrame Tip These rules were implemented in the CSE project n\u00b0210013","title":"Suicide attempt"},{"location":"functionalities/phenotyping/suicide_attempts/#suicide-attempt","text":"","title":"Suicide attempt"},{"location":"functionalities/phenotyping/suicide_attempts/#presentation","text":"We provide the tag_suicide_attempt() function to extract suicide attempt from ICD-10 codes.","title":"Presentation"},{"location":"functionalities/phenotyping/suicide_attempts/#the-tag_suicide_attempt-function","text":"from eds_scikit.io import HiveData data = HiveData ( DBNAME ) from eds_scikit.event import tag_suicide_attempt visit_occurrence = tag_suicide_attempt ( data . visit_occurrence , data . condition_occurrence , algo = \"X60-X84\" , ) Availables algorithms (values for \"algo\" ) 'X60-X84' 'Haguenoer2008' Returns the visits that have at least one ICD code that belongs to the range X60 to X84. Returns the visits that follow the definiton of \"Haguenoer, Ken, Agn\u00e8s Caille, Marc Fillatre, Anne Isabelle Lecuyer, et Emmanuel Rusch. \u00ab Tentatives de Suicide \u00bb, 2008, 4.\". This rule requires at least one Main Diagnostic (DP) belonging to S00 to T98, and at least one Associated Diagnostic (DAS) that belongs to the range X60 to X84. You can check the documentation of the function for additional parameters: Function to return visits that fulfill different definitions of suicide attempt by ICD10. PARAMETER DESCRIPTION visit_occurrence TYPE: DataFrame condition_occurrence TYPE: DataFrame date_min Minimal starting date (on visit_start_datetime ) TYPE: Optional [ datetime ] DEFAULT: None date_max Maximal starting date (on visit_start_datetime ) TYPE: Optional [ datetime ] DEFAULT: None algo Method to use. Available values are: \"X60-X84\" : Will return a the visits that have at least one ICD code that belongs to the range X60 to X84. \"Haguenoer2008\" : Will return a the visits that follow the definiton of \" Haguenoer, Ken, Agn\u00e8s Caille, Marc Fillatre, Anne Isabelle Lecuyer, et Emmanuel Rusch. \u00ab Tentatives de Suicide \u00bb, 2008, 4. \". This rule requires at least one Main Diagnostic (DP) belonging to S00 to T98, and at least one Associated Diagnostic (DAS) that belongs to the range X60 to X84. TYPE: str DEFAULT: 'X60-X84' RETURNS DESCRIPTION visit_occurrence Tagged with an additional column SUICIDE_ATTEMPT TYPE: DataFrame Tip These rules were implemented in the CSE project n\u00b0210013","title":"The tag_suicide_attempt() function"},{"location":"functionalities/phenotyping/working-with-codes/","text":"Using ICD-10 and CCAM eds-scikit provides two functions to ease the extraction of occurrrences of ICD-10 codes : eds_scikit.event.icd10.conditions_from_icd10 CCAM codes : eds_scikit.event.ccam.procedures_from_ccam These two functions are by design similar. In fact, they call under the hood the same base function . Let us see a minimal working example that would allow us to select patients with Deep Vein Thrombosis based on the presence of specific ICD-10 codes. from eds_scikit.io import HiveData data = HiveData ( DBNAME ) codes = dict ( DVT = dict ( exact = [ \"I81\" , \"O223\" , \"O082\" , \"O871\" ], regex = [ \"I82[02389]\" , \"I80[12]\" ] # (1) ) ) from eds_scikit.event.icd10 import conditions_from_icd10 DVTs = conditions_from_icd10 ( condition_occurrence = data . condition_occurrence , visit_occurrence = data . visit_occurrence , codes = codes , date_from_visit = True , additional_filtering = dict ( condition_status_source_value = { \"DP\" , \"DAS\" }, # (1) ), ) Here you can provide either exact , regex or prefix codes With this syntax we will keep only DP ( Diagnostic Principal ) or DAS ( Diagnostic Associ\u00e9 ) diagnoses Of course, you are encouraged to check the documentation of those functions as they provide additional parameters that might be useful depending on your needs.","title":"Using ICD-10 and CCAM"},{"location":"functionalities/phenotyping/working-with-codes/#using-icd-10-and-ccam","text":"eds-scikit provides two functions to ease the extraction of occurrrences of ICD-10 codes : eds_scikit.event.icd10.conditions_from_icd10 CCAM codes : eds_scikit.event.ccam.procedures_from_ccam These two functions are by design similar. In fact, they call under the hood the same base function . Let us see a minimal working example that would allow us to select patients with Deep Vein Thrombosis based on the presence of specific ICD-10 codes. from eds_scikit.io import HiveData data = HiveData ( DBNAME ) codes = dict ( DVT = dict ( exact = [ \"I81\" , \"O223\" , \"O082\" , \"O871\" ], regex = [ \"I82[02389]\" , \"I80[12]\" ] # (1) ) ) from eds_scikit.event.icd10 import conditions_from_icd10 DVTs = conditions_from_icd10 ( condition_occurrence = data . condition_occurrence , visit_occurrence = data . visit_occurrence , codes = codes , date_from_visit = True , additional_filtering = dict ( condition_status_source_value = { \"DP\" , \"DAS\" }, # (1) ), ) Here you can provide either exact , regex or prefix codes With this syntax we will keep only DP ( Diagnostic Principal ) or DAS ( Diagnostic Associ\u00e9 ) diagnoses Of course, you are encouraged to check the documentation of those functions as they provide additional parameters that might be useful depending on your needs.","title":"Using ICD-10 and CCAM"},{"location":"functionalities/phenotyping/phenotypes/cancer/","text":"Cancer Presentation We provide the CancerFromICD10 class to extract visits or patients with cancer related ICD10 code Available cancer types Anus Biliary_duct Bladder Bowel Breast CNS CUP Cervix Colon Endometrium Eye Gastric Head_neck Hodgkin_lymphoma Kidney Leukemia Liver Lung Melanoma Mesothelioma Myeloma Nonhodgkin_lymphoma Oesophagus Osteosarcoma Other_digestive Other_endocrinial Other_gynecology Other_hematologic_malignancies Other_pneumology Other_skin Other_urothelial Ovary PNS Pancreas Prostate Rectum Soft_tissue Testis Thyroid How it works The algorithm works by looking for either DP ou DR ICD10 codes associated with cancer. The codes terminology comes from this article 1 and is available under CancerFromICD10.ICD10_CODES Usage By default, all cancer types mentionned above are extracted from eds_scikit.io import HiveData data = HiveData ( DBNAME ) from eds_scikit.phenotype import CancerFromICD10 cancer = CancerFromICD10 ( data ) data = cancer . to_data () To choose a subset of cancer types, use the cancer_types argument: cancer = CancerFromICD10 ( data , cancer_types = [ \"Eye\" , \"Liver\" , \"Leukemia\" , ], ) The final phenotype DataFrame is then available at data.computed[\"CancerFromICD10\"] Optional parameters PARAMETER DESCRIPTION data A BaseData object TYPE: BaseData cancer_types Optional list of cancer types to use for phenotyping TYPE: Optional[List[str]] DEFAULT: None level On which level to do the aggregation, either \"patient\" or \"visit\" TYPE: str DEFAULT: 'patient' subphenotype Whether the threshold should apply to the phenotype (\"phenotype\" column) of the subphenotype (\"subphenotype\" column) TYPE: bool DEFAULT: True threshold Minimal number of events (which definition depends on the level value) TYPE: int DEFAULT: 1 Citation You can get the BibTex of the corresponding article 1 by calling cancer . cite () @article { kempf2022impact , title = {Impact of two waves of Sars-Cov2 outbreak on the number, clinical presentation, care trajectories and survival of patients newly referred for a colorectal cancer: A French multicentric cohort study from a large group of University hospitals} , author = {Kempf, Emmanuelle and Priou, Sonia and Lam{\\'e}, Guillaume and Daniel, Christel and Bellamine, Ali and Sommacale, Daniele and Belkacemi, Yazid and Bey, Romain and Galula, Gilles and Taright, Namik and others} , journal = {International Journal of Cancer} , volume = {150} , number = {10} , pages = {1609--1618} , year = {2022} , publisher = {Wiley Online Library} } Reference Check the code reference here for a more detailled look. Emmanuelle Kempf, Sonia Priou, Guillaume Lam\u00e9, Christel Daniel, Ali Bellamine, Daniele Sommacale, Yazid Belkacemi, Romain Bey, Gilles Galula, Namik Taright, and others. Impact of two waves of sars-cov2 outbreak on the number, clinical presentation, care trajectories and survival of patients newly referred for a colorectal cancer: a french multicentric cohort study from a large group of university hospitals. International Journal of Cancer , 150(10):1609\u20131618, 2022. \u21a9 \u21a9","title":"Cancer"},{"location":"functionalities/phenotyping/phenotypes/cancer/#cancer","text":"","title":"Cancer"},{"location":"functionalities/phenotyping/phenotypes/cancer/#presentation","text":"We provide the CancerFromICD10 class to extract visits or patients with cancer related ICD10 code Available cancer types Anus Biliary_duct Bladder Bowel Breast CNS CUP Cervix Colon Endometrium Eye Gastric Head_neck Hodgkin_lymphoma Kidney Leukemia Liver Lung Melanoma Mesothelioma Myeloma Nonhodgkin_lymphoma Oesophagus Osteosarcoma Other_digestive Other_endocrinial Other_gynecology Other_hematologic_malignancies Other_pneumology Other_skin Other_urothelial Ovary PNS Pancreas Prostate Rectum Soft_tissue Testis Thyroid How it works The algorithm works by looking for either DP ou DR ICD10 codes associated with cancer. The codes terminology comes from this article 1 and is available under CancerFromICD10.ICD10_CODES","title":"Presentation"},{"location":"functionalities/phenotyping/phenotypes/cancer/#usage","text":"By default, all cancer types mentionned above are extracted from eds_scikit.io import HiveData data = HiveData ( DBNAME ) from eds_scikit.phenotype import CancerFromICD10 cancer = CancerFromICD10 ( data ) data = cancer . to_data () To choose a subset of cancer types, use the cancer_types argument: cancer = CancerFromICD10 ( data , cancer_types = [ \"Eye\" , \"Liver\" , \"Leukemia\" , ], ) The final phenotype DataFrame is then available at data.computed[\"CancerFromICD10\"]","title":"Usage"},{"location":"functionalities/phenotyping/phenotypes/cancer/#optional-parameters","text":"PARAMETER DESCRIPTION data A BaseData object TYPE: BaseData cancer_types Optional list of cancer types to use for phenotyping TYPE: Optional[List[str]] DEFAULT: None level On which level to do the aggregation, either \"patient\" or \"visit\" TYPE: str DEFAULT: 'patient' subphenotype Whether the threshold should apply to the phenotype (\"phenotype\" column) of the subphenotype (\"subphenotype\" column) TYPE: bool DEFAULT: True threshold Minimal number of events (which definition depends on the level value) TYPE: int DEFAULT: 1","title":"Optional parameters"},{"location":"functionalities/phenotyping/phenotypes/cancer/#citation","text":"You can get the BibTex of the corresponding article 1 by calling cancer . cite () @article { kempf2022impact , title = {Impact of two waves of Sars-Cov2 outbreak on the number, clinical presentation, care trajectories and survival of patients newly referred for a colorectal cancer: A French multicentric cohort study from a large group of University hospitals} , author = {Kempf, Emmanuelle and Priou, Sonia and Lam{\\'e}, Guillaume and Daniel, Christel and Bellamine, Ali and Sommacale, Daniele and Belkacemi, Yazid and Bey, Romain and Galula, Gilles and Taright, Namik and others} , journal = {International Journal of Cancer} , volume = {150} , number = {10} , pages = {1609--1618} , year = {2022} , publisher = {Wiley Online Library} }","title":"Citation"},{"location":"functionalities/phenotyping/phenotypes/cancer/#reference","text":"Check the code reference here for a more detailled look. Emmanuelle Kempf, Sonia Priou, Guillaume Lam\u00e9, Christel Daniel, Ali Bellamine, Daniele Sommacale, Yazid Belkacemi, Romain Bey, Gilles Galula, Namik Taright, and others. Impact of two waves of sars-cov2 outbreak on the number, clinical presentation, care trajectories and survival of patients newly referred for a colorectal cancer: a french multicentric cohort study from a large group of university hospitals. International Journal of Cancer , 150(10):1609\u20131618, 2022. \u21a9 \u21a9","title":"Reference"},{"location":"functionalities/phenotyping/phenotypes/diabetes/","text":"Diabetes Presentation We provide the DiabetesFromICD10 class to extract visits or patients with ICD10 codes related to diabetes Available diabetes types DIABETES_IN_PREGNANCY DIABETES_MALNUTRITION DIABETES_TYPE_I DIABETES_TYPE_II OTHER_DIABETES_MELLITUS How it works The algorithm works by looking for either DP, DR or DAS ICD10 codes associated with cancer. Those codes are available under DiabetesFromICD10.ICD10_CODES Usage By default, all diabetes types mentionned above are extracted from eds_scikit.io import HiveData data = HiveData ( DBNAME ) from eds_scikit.phenotype import DiabetesFromICD10 diabetes = DiabetesFromICD10 ( data ) data = diabetes . to_data () To choose a subset of disorders, use the diabetes_types argument: diabetes = DiabetesFromICD10 ( data , diabetes_types = [ \"DIABETES_TYPE_I\" , \"DIABETES_IN_PREGNANCY\" , ], ) The final phenotype DataFrame is then available at data.computed[\"DiabetesFromICD10\"] Optional parameters PARAMETER DESCRIPTION data A BaseData object TYPE: BaseData diabetes_types Optional list of diabetes types to use for phenotyping TYPE: Optional[List[str]] DEFAULT: None level On which level to do the aggregation, either \"patient\" or \"visit\" TYPE: str DEFAULT: 'visit' subphenotype Whether the threshold should apply to the phenotype (\"phenotype\" column) of the subphenotype (\"subphenotype\" column) TYPE: bool DEFAULT: True threshold Minimal number of events (which definition depends on the level value) TYPE: int DEFAULT: 1 Reference Check the code reference here for a more detailled look.","title":"Diabetes"},{"location":"functionalities/phenotyping/phenotypes/diabetes/#diabetes","text":"","title":"Diabetes"},{"location":"functionalities/phenotyping/phenotypes/diabetes/#presentation","text":"We provide the DiabetesFromICD10 class to extract visits or patients with ICD10 codes related to diabetes Available diabetes types DIABETES_IN_PREGNANCY DIABETES_MALNUTRITION DIABETES_TYPE_I DIABETES_TYPE_II OTHER_DIABETES_MELLITUS How it works The algorithm works by looking for either DP, DR or DAS ICD10 codes associated with cancer. Those codes are available under DiabetesFromICD10.ICD10_CODES","title":"Presentation"},{"location":"functionalities/phenotyping/phenotypes/diabetes/#usage","text":"By default, all diabetes types mentionned above are extracted from eds_scikit.io import HiveData data = HiveData ( DBNAME ) from eds_scikit.phenotype import DiabetesFromICD10 diabetes = DiabetesFromICD10 ( data ) data = diabetes . to_data () To choose a subset of disorders, use the diabetes_types argument: diabetes = DiabetesFromICD10 ( data , diabetes_types = [ \"DIABETES_TYPE_I\" , \"DIABETES_IN_PREGNANCY\" , ], ) The final phenotype DataFrame is then available at data.computed[\"DiabetesFromICD10\"]","title":"Usage"},{"location":"functionalities/phenotyping/phenotypes/diabetes/#optional-parameters","text":"PARAMETER DESCRIPTION data A BaseData object TYPE: BaseData diabetes_types Optional list of diabetes types to use for phenotyping TYPE: Optional[List[str]] DEFAULT: None level On which level to do the aggregation, either \"patient\" or \"visit\" TYPE: str DEFAULT: 'visit' subphenotype Whether the threshold should apply to the phenotype (\"phenotype\" column) of the subphenotype (\"subphenotype\" column) TYPE: bool DEFAULT: True threshold Minimal number of events (which definition depends on the level value) TYPE: int DEFAULT: 1","title":"Optional parameters"},{"location":"functionalities/phenotyping/phenotypes/diabetes/#reference","text":"Check the code reference here for a more detailled look.","title":"Reference"},{"location":"functionalities/phenotyping/phenotypes/psychiatric_disorder/","text":"Psychiatric disorder Presentation We provide the PsychiatricDisorderFromICD10 class to extract visits or patients with ICD10 codes related to psychiatric disorders Available disorders Anxiety Disorders Bipolar and Related Disorders Depressive Disorders Disruptive, Impulse Control and Conduct Disorders Dissociative Disorders Feeding and Eating Disorders Mental Health Symptom Miscellaneous Obsessive-Compulsive and Related Disorders Personality Disorders Schizophrenia Spectrum and Other Psychotic Disorders Sleep-Wake Disorders Somatic Symptom and Related Disorders Substance-Related and Addictive Disorders Suicide or Self-Injury Trauma and Stressor-Related Disorders How it works The algorithm works by looking for either DP, DR or DAS ICD10 codes associated with psychiatric disorder. The codes terminology comes from this article 1 and is available under PsychiatricDisorderFromICD10.ICD10_CODES Usage By default, all cancer types mentionned above are extracted from eds_scikit.io import HiveData data = HiveData ( DBNAME ) from eds_scikit.phenotype import PsychiatricDisorderFromICD10 psy = PsychiatricDisorderFromICD10 ( data ) data = psy . to_data () To choose a subset of disorders, use the disorder_types argument: psy = PsychiatricDisorderFromICD10 ( data , disorder_types = [ \"Anxiety Disorders\" , \"Trauma and Stressor-Related Disorders\" , ], ) The final phenotype DataFrame is then available at data.computed[\"PsychiatricDisorderFromICD10\"] Optional parameters PARAMETER DESCRIPTION data A BaseData object TYPE: BaseData disorder_types Optional list of disorder types to use for phenotyping TYPE: Optional[List[str]] DEFAULT: None level On which level to do the aggregation, either \"patient\" or \"visit\" TYPE: str DEFAULT: 'patient' subphenotype Whether the threshold should apply to the phenotype (\"phenotype\" column) of the subphenotype (\"subphenotype\" column) TYPE: bool DEFAULT: True threshold Minimal number of events (which definition depends on the level value) TYPE: int DEFAULT: 1 Citation You can get the BibTex of the corresponding article 1 by calling cancer . cite () @article { 2022_covid_4CE , author = {Guti\u00e9rrez-Sacrist\u00e1n, Alba and Serret-Larmande, Arnaud and Hutch, Meghan R. and S\u00e1ez, Carlos and Aronow, Bruce J. and Bhatnagar, Surbhi and Bonzel, Clara-Lea and Cai, Tianxi and Devkota, Batsal and Hanauer, David A. and Loh, Ne Hooi Will and Luo, Yuan and Moal, Bertrand and Ahooyi, Taha Mohseni and Njoroge, Wanjik\u0169 F. M. and Omenn, Gilbert S. and Sanchez-Pinto, L. Nelson and South, Andrew M. and Sperotto, Francesca and Tan, Amelia L. M. and Taylor, Deanne M. and Verdy, Guillaume and Visweswaran, Shyam and Xia, Zongqi and Zahner, Janet and Avillach, Paul and Bourgeois, Florence T. and Consortium for Clinical Characterization of COVID-19 by EHR (4CE)} , title = \"{Hospitalizations Associated With Mental Health Conditions Among Adolescents in the US and France During the COVID-19 Pandemic}\" , journal = {JAMA Network Open} , volume = {5} , number = {12} , pages = {e2246548-e2246548} , year = {2022} , month = {12} , abstract = \"{The COVID-19 pandemic has been associated with an increase in mental health diagnoses among adolescents, though the extent of the increase, particularly for severe cases requiring hospitalization, has not been well characterized. Large-scale federated informatics approaches provide the ability to efficiently and securely query health care data sets to assess and monitor hospitalization patterns for mental health conditions among adolescents.To estimate changes in the proportion of hospitalizations associated with mental health conditions among adolescents following onset of the COVID-19 pandemic.This retrospective, multisite cohort study of adolescents 11 to 17 years of age who were hospitalized with at least 1 mental health condition diagnosis between February 1, 2019, and April 30, 2021, used patient-level data from electronic health records of 8 children\u2019s hospitals in the US and France.Change in the monthly proportion of mental health condition\u2013associated hospitalizations between the prepandemic (February 1, 2019, to March 31, 2020) and pandemic (April 1, 2020, to April 30, 2021) periods using interrupted time series analysis.There were 9696 adolescents hospitalized with a mental health condition during the prepandemic period (5966 [61.5\\\\%] female) and 11\u202f101 during the pandemic period (7603 [68.5\\\\%] female). The mean (SD) age in the prepandemic cohort was 14.6 (1.9) years and in the pandemic cohort, 14.7 (1.8) years. The most prevalent diagnoses during the pandemic were anxiety (6066 [57.4\\\\%]), depression (5065 [48.0\\\\%]), and suicidality or self-injury (4673 [44.2\\\\%]). There was an increase in the proportions of monthly hospitalizations during the pandemic for anxiety (0.55\\\\%; 95\\\\% CI, 0.26\\\\%-0.84\\\\%), depression (0.50\\\\%; 95\\\\% CI, 0.19\\\\%-0.79\\\\%), and suicidality or self-injury (0.38\\\\%; 95\\\\% CI, 0.08\\\\%-0.68\\\\%). There was an estimated 0.60\\\\% increase (95\\\\% CI, 0.31\\\\%-0.89\\\\%) overall in the monthly proportion of mental health\u2013associated hospitalizations following onset of the pandemic compared with the prepandemic period.In this cohort study, onset of the COVID-19 pandemic was associated with increased hospitalizations with mental health diagnoses among adolescents. These findings support the need for greater resources within children\u2019s hospitals to care for adolescents with mental health conditions during the pandemic and beyond.}\" , issn = {2574-3805} , doi = {10.1001/jamanetworkopen.2022.46548} , url = {https://doi.org/10.1001/jamanetworkopen.2022.46548} , eprint = {https://jamanetwork.com/journals/jamanetworkopen/articlepdf/2799437/gutirrezsacristn\\_2022\\_oi\\_221314\\_1670339179.72376.pdf} , } Reference Check the code reference here for a more detailled look. Alba Guti\u00e9rrez-Sacrist\u00e1n, Arnaud Serret-Larmande, Meghan R. Hutch, Carlos S\u00e1ez, Bruce J. Aronow, Surbhi Bhatnagar, Clara-Lea Bonzel, Tianxi Cai, Batsal Devkota, David A. Hanauer, Ne Hooi Will Loh, Yuan Luo, Bertrand Moal, Taha Mohseni Ahooyi, Wanjik\u0169 F. M. Njoroge, Gilbert S. Omenn, L. Nelson Sanchez-Pinto, Andrew M. South, Francesca Sperotto, Amelia L. M. Tan, Deanne M. Taylor, Guillaume Verdy, Shyam Visweswaran, Zongqi Xia, Janet Zahner, Paul Avillach, Florence T. Bourgeois, and Consortium for Clinical Characterization of COVID-19 by EHR (4CE). Hospitalizations Associated With Mental Health Conditions Among Adolescents in the US and France During the COVID-19 Pandemic. JAMA Network Open , 5(12):e2246548\u2013e2246548, 12 2022. URL: https://doi.org/10.1001/jamanetworkopen.2022.46548 , arXiv:https://jamanetwork.com/journals/jamanetworkopen/articlepdf/2799437/gutirrezsacristn\\_2022\\_oi\\_221314\\_1670339179.72376.pdf , doi:10.1001/jamanetworkopen.2022.46548 . \u21a9 \u21a9","title":"Psychiatric disorder"},{"location":"functionalities/phenotyping/phenotypes/psychiatric_disorder/#psychiatric-disorder","text":"","title":"Psychiatric disorder"},{"location":"functionalities/phenotyping/phenotypes/psychiatric_disorder/#presentation","text":"We provide the PsychiatricDisorderFromICD10 class to extract visits or patients with ICD10 codes related to psychiatric disorders Available disorders Anxiety Disorders Bipolar and Related Disorders Depressive Disorders Disruptive, Impulse Control and Conduct Disorders Dissociative Disorders Feeding and Eating Disorders Mental Health Symptom Miscellaneous Obsessive-Compulsive and Related Disorders Personality Disorders Schizophrenia Spectrum and Other Psychotic Disorders Sleep-Wake Disorders Somatic Symptom and Related Disorders Substance-Related and Addictive Disorders Suicide or Self-Injury Trauma and Stressor-Related Disorders How it works The algorithm works by looking for either DP, DR or DAS ICD10 codes associated with psychiatric disorder. The codes terminology comes from this article 1 and is available under PsychiatricDisorderFromICD10.ICD10_CODES","title":"Presentation"},{"location":"functionalities/phenotyping/phenotypes/psychiatric_disorder/#usage","text":"By default, all cancer types mentionned above are extracted from eds_scikit.io import HiveData data = HiveData ( DBNAME ) from eds_scikit.phenotype import PsychiatricDisorderFromICD10 psy = PsychiatricDisorderFromICD10 ( data ) data = psy . to_data () To choose a subset of disorders, use the disorder_types argument: psy = PsychiatricDisorderFromICD10 ( data , disorder_types = [ \"Anxiety Disorders\" , \"Trauma and Stressor-Related Disorders\" , ], ) The final phenotype DataFrame is then available at data.computed[\"PsychiatricDisorderFromICD10\"]","title":"Usage"},{"location":"functionalities/phenotyping/phenotypes/psychiatric_disorder/#optional-parameters","text":"PARAMETER DESCRIPTION data A BaseData object TYPE: BaseData disorder_types Optional list of disorder types to use for phenotyping TYPE: Optional[List[str]] DEFAULT: None level On which level to do the aggregation, either \"patient\" or \"visit\" TYPE: str DEFAULT: 'patient' subphenotype Whether the threshold should apply to the phenotype (\"phenotype\" column) of the subphenotype (\"subphenotype\" column) TYPE: bool DEFAULT: True threshold Minimal number of events (which definition depends on the level value) TYPE: int DEFAULT: 1","title":"Optional parameters"},{"location":"functionalities/phenotyping/phenotypes/psychiatric_disorder/#citation","text":"You can get the BibTex of the corresponding article 1 by calling cancer . cite () @article { 2022_covid_4CE , author = {Guti\u00e9rrez-Sacrist\u00e1n, Alba and Serret-Larmande, Arnaud and Hutch, Meghan R. and S\u00e1ez, Carlos and Aronow, Bruce J. and Bhatnagar, Surbhi and Bonzel, Clara-Lea and Cai, Tianxi and Devkota, Batsal and Hanauer, David A. and Loh, Ne Hooi Will and Luo, Yuan and Moal, Bertrand and Ahooyi, Taha Mohseni and Njoroge, Wanjik\u0169 F. M. and Omenn, Gilbert S. and Sanchez-Pinto, L. Nelson and South, Andrew M. and Sperotto, Francesca and Tan, Amelia L. M. and Taylor, Deanne M. and Verdy, Guillaume and Visweswaran, Shyam and Xia, Zongqi and Zahner, Janet and Avillach, Paul and Bourgeois, Florence T. and Consortium for Clinical Characterization of COVID-19 by EHR (4CE)} , title = \"{Hospitalizations Associated With Mental Health Conditions Among Adolescents in the US and France During the COVID-19 Pandemic}\" , journal = {JAMA Network Open} , volume = {5} , number = {12} , pages = {e2246548-e2246548} , year = {2022} , month = {12} , abstract = \"{The COVID-19 pandemic has been associated with an increase in mental health diagnoses among adolescents, though the extent of the increase, particularly for severe cases requiring hospitalization, has not been well characterized. Large-scale federated informatics approaches provide the ability to efficiently and securely query health care data sets to assess and monitor hospitalization patterns for mental health conditions among adolescents.To estimate changes in the proportion of hospitalizations associated with mental health conditions among adolescents following onset of the COVID-19 pandemic.This retrospective, multisite cohort study of adolescents 11 to 17 years of age who were hospitalized with at least 1 mental health condition diagnosis between February 1, 2019, and April 30, 2021, used patient-level data from electronic health records of 8 children\u2019s hospitals in the US and France.Change in the monthly proportion of mental health condition\u2013associated hospitalizations between the prepandemic (February 1, 2019, to March 31, 2020) and pandemic (April 1, 2020, to April 30, 2021) periods using interrupted time series analysis.There were 9696 adolescents hospitalized with a mental health condition during the prepandemic period (5966 [61.5\\\\%] female) and 11\u202f101 during the pandemic period (7603 [68.5\\\\%] female). The mean (SD) age in the prepandemic cohort was 14.6 (1.9) years and in the pandemic cohort, 14.7 (1.8) years. The most prevalent diagnoses during the pandemic were anxiety (6066 [57.4\\\\%]), depression (5065 [48.0\\\\%]), and suicidality or self-injury (4673 [44.2\\\\%]). There was an increase in the proportions of monthly hospitalizations during the pandemic for anxiety (0.55\\\\%; 95\\\\% CI, 0.26\\\\%-0.84\\\\%), depression (0.50\\\\%; 95\\\\% CI, 0.19\\\\%-0.79\\\\%), and suicidality or self-injury (0.38\\\\%; 95\\\\% CI, 0.08\\\\%-0.68\\\\%). There was an estimated 0.60\\\\% increase (95\\\\% CI, 0.31\\\\%-0.89\\\\%) overall in the monthly proportion of mental health\u2013associated hospitalizations following onset of the pandemic compared with the prepandemic period.In this cohort study, onset of the COVID-19 pandemic was associated with increased hospitalizations with mental health diagnoses among adolescents. These findings support the need for greater resources within children\u2019s hospitals to care for adolescents with mental health conditions during the pandemic and beyond.}\" , issn = {2574-3805} , doi = {10.1001/jamanetworkopen.2022.46548} , url = {https://doi.org/10.1001/jamanetworkopen.2022.46548} , eprint = {https://jamanetwork.com/journals/jamanetworkopen/articlepdf/2799437/gutirrezsacristn\\_2022\\_oi\\_221314\\_1670339179.72376.pdf} , }","title":"Citation"},{"location":"functionalities/phenotyping/phenotypes/psychiatric_disorder/#reference","text":"Check the code reference here for a more detailled look. Alba Guti\u00e9rrez-Sacrist\u00e1n, Arnaud Serret-Larmande, Meghan R. Hutch, Carlos S\u00e1ez, Bruce J. Aronow, Surbhi Bhatnagar, Clara-Lea Bonzel, Tianxi Cai, Batsal Devkota, David A. Hanauer, Ne Hooi Will Loh, Yuan Luo, Bertrand Moal, Taha Mohseni Ahooyi, Wanjik\u0169 F. M. Njoroge, Gilbert S. Omenn, L. Nelson Sanchez-Pinto, Andrew M. South, Francesca Sperotto, Amelia L. M. Tan, Deanne M. Taylor, Guillaume Verdy, Shyam Visweswaran, Zongqi Xia, Janet Zahner, Paul Avillach, Florence T. Bourgeois, and Consortium for Clinical Characterization of COVID-19 by EHR (4CE). Hospitalizations Associated With Mental Health Conditions Among Adolescents in the US and France During the COVID-19 Pandemic. JAMA Network Open , 5(12):e2246548\u2013e2246548, 12 2022. URL: https://doi.org/10.1001/jamanetworkopen.2022.46548 , arXiv:https://jamanetwork.com/journals/jamanetworkopen/articlepdf/2799437/gutirrezsacristn\\_2022\\_oi\\_221314\\_1670339179.72376.pdf , doi:10.1001/jamanetworkopen.2022.46548 . \u21a9 \u21a9","title":"Reference"},{"location":"functionalities/phenotyping/phenotypes/suicide_attempt/","text":"Suicide Attempt Presentation We provide the SuicideAttemptFromICD10 class to extract visits linked to suicide attempt from ICD-10 codes. Usage As mentionned below, two algorithms ( \"Haguenoer2008\" (default) and \"X60-X84\" ) are available from eds_scikit.io import HiveData data = HiveData ( DBNAME ) from eds_scikit.phenotype import SuicideAttemptFromICD10 sa = SuicideAttemptFromICD10 ( data ) data = sa . to_data () The final phenotype DataFrame is then available at data.computed[\"SuicideAttemptFromICD10_Haguenoer2008\"] or data.computed[\"SuicideAttemptFromICD10_X60_X84\"] depending on the used algorithm Availables algorithms (values for \"algo\" ) The ICD10 codes are available under SuicideAttemptFromICD10.ICD10_CODES 'X60-X84' 'Haguenoer2008' Returns the visits that have at least one ICD code that belongs to the range X60 to X84. Returns the visits that follow the definiton of Haguenoer2008 1 . This rule requires at least one Main Diagnostic (DP) belonging to S00 to T98, and at least one Associated Diagnostic (DAS) that belongs to the range X60 to X84. Citation When using algo=\"Haguenoer2008\" , you can get the BibTex of the corresponding article 1 by calling sa . cite () @misc { haguenoer_tentatives_2008 , title = {\u00c9pid\u00e9miologie des tentatives de suicide en r\u00e9gion Centre} , language = {fr} , author = {Haguenoer, Ken and Caille, Agn\u00e8s and Fillatre, Marc and Lecuyer, Anne Isabelle and Rusch, Emmanuel} , year = {2008} , pages = {4} , howpublished = {\\url{https://www.esante-centre.fr/portail_pro/minisite_25/media-files/56393/plaquette-2006-2009.pdf}} } Reference Check the code reference here for a more detailled look. Ken Haguenoer, Agn\u00e8s Caille, Marc Fillatre, Anne Isabelle Lecuyer, and Emmanuel Rusch. \u00c9pid\u00e9miologie des tentatives de suicide en r\u00e9gion centre. \\url https://www.esante-centre.fr/portail_pro/minisite_25/media-files/56393/plaquette-2006-2009.pdf, 2008. \u21a9 \u21a9","title":"Suicide Attempt"},{"location":"functionalities/phenotyping/phenotypes/suicide_attempt/#suicide-attempt","text":"","title":"Suicide Attempt"},{"location":"functionalities/phenotyping/phenotypes/suicide_attempt/#presentation","text":"We provide the SuicideAttemptFromICD10 class to extract visits linked to suicide attempt from ICD-10 codes.","title":"Presentation"},{"location":"functionalities/phenotyping/phenotypes/suicide_attempt/#usage","text":"As mentionned below, two algorithms ( \"Haguenoer2008\" (default) and \"X60-X84\" ) are available from eds_scikit.io import HiveData data = HiveData ( DBNAME ) from eds_scikit.phenotype import SuicideAttemptFromICD10 sa = SuicideAttemptFromICD10 ( data ) data = sa . to_data () The final phenotype DataFrame is then available at data.computed[\"SuicideAttemptFromICD10_Haguenoer2008\"] or data.computed[\"SuicideAttemptFromICD10_X60_X84\"] depending on the used algorithm Availables algorithms (values for \"algo\" ) The ICD10 codes are available under SuicideAttemptFromICD10.ICD10_CODES 'X60-X84' 'Haguenoer2008' Returns the visits that have at least one ICD code that belongs to the range X60 to X84. Returns the visits that follow the definiton of Haguenoer2008 1 . This rule requires at least one Main Diagnostic (DP) belonging to S00 to T98, and at least one Associated Diagnostic (DAS) that belongs to the range X60 to X84.","title":"Usage"},{"location":"functionalities/phenotyping/phenotypes/suicide_attempt/#citation","text":"When using algo=\"Haguenoer2008\" , you can get the BibTex of the corresponding article 1 by calling sa . cite () @misc { haguenoer_tentatives_2008 , title = {\u00c9pid\u00e9miologie des tentatives de suicide en r\u00e9gion Centre} , language = {fr} , author = {Haguenoer, Ken and Caille, Agn\u00e8s and Fillatre, Marc and Lecuyer, Anne Isabelle and Rusch, Emmanuel} , year = {2008} , pages = {4} , howpublished = {\\url{https://www.esante-centre.fr/portail_pro/minisite_25/media-files/56393/plaquette-2006-2009.pdf}} }","title":"Citation"},{"location":"functionalities/phenotyping/phenotypes/suicide_attempt/#reference","text":"Check the code reference here for a more detailled look. Ken Haguenoer, Agn\u00e8s Caille, Marc Fillatre, Anne Isabelle Lecuyer, and Emmanuel Rusch. \u00c9pid\u00e9miologie des tentatives de suicide en r\u00e9gion centre. \\url https://www.esante-centre.fr/portail_pro/minisite_25/media-files/56393/plaquette-2006-2009.pdf, 2008. \u21a9 \u21a9","title":"Reference"},{"location":"functionalities/plotting/age_pyramid/","text":"Visualizing age pyramid The age pyramid is helpful to quickly visualize the age and gender distributions in a cohort. Load a synthetic dataset plot_age_pyramid uses the \"person\" table: from eds_scikit.datasets.synthetic.person import load_person df_person = load_person () df_person . head () | | person_id | gender_source_value | birth_datetime | |---|-----------|---------------------|----------------| | 0 | 0 | m | 2010-01-01 | | 1 | 1 | m | 1938-01-01 | | 2 | 2 | f | 1994-01-01 | | 3 | 3 | m | 1994-01-01 | | 4 | 4 | m | 2004-01-01 | Visualize age pyramid Basic usage By default, plot_age_pyramid will compute age as the difference between today and the date of birth: from eds_scikit.plot.age_pyramid import plot_age_pyramid plot_age_pyramid ( df_person ) Advanced parameters Further configuration can be provided, including : datetime_ref : Choose the reference to compute the age from. It can be either a single datetime (string or datetime type), an array of datetime (one reference for each patient) or a string representing a column of the input dataframe return_array : If set to True, return a dataframe instead of a chart. import pandas as pd from datetime import datetime from eds_scikit.plot.age_pyramid import plot_age_pyramid dates_of_first_visit = pd . Series ([ datetime ( 2020 , 1 , 1 )] * df_person . shape [ 0 ]) plot_age_pyramid ( df_person , datetime_ref = dates_of_first_visit ) Please check the documentation for further details on the function's parameters.","title":"Age pyramid"},{"location":"functionalities/plotting/age_pyramid/#visualizing-age-pyramid","text":"The age pyramid is helpful to quickly visualize the age and gender distributions in a cohort.","title":"Visualizing age pyramid"},{"location":"functionalities/plotting/age_pyramid/#load-a-synthetic-dataset","text":"plot_age_pyramid uses the \"person\" table: from eds_scikit.datasets.synthetic.person import load_person df_person = load_person () df_person . head () | | person_id | gender_source_value | birth_datetime | |---|-----------|---------------------|----------------| | 0 | 0 | m | 2010-01-01 | | 1 | 1 | m | 1938-01-01 | | 2 | 2 | f | 1994-01-01 | | 3 | 3 | m | 1994-01-01 | | 4 | 4 | m | 2004-01-01 |","title":"Load a synthetic dataset"},{"location":"functionalities/plotting/age_pyramid/#visualize-age-pyramid","text":"","title":"Visualize age pyramid"},{"location":"functionalities/plotting/age_pyramid/#basic-usage","text":"By default, plot_age_pyramid will compute age as the difference between today and the date of birth: from eds_scikit.plot.age_pyramid import plot_age_pyramid plot_age_pyramid ( df_person )","title":"Basic usage"},{"location":"functionalities/plotting/age_pyramid/#advanced-parameters","text":"Further configuration can be provided, including : datetime_ref : Choose the reference to compute the age from. It can be either a single datetime (string or datetime type), an array of datetime (one reference for each patient) or a string representing a column of the input dataframe return_array : If set to True, return a dataframe instead of a chart. import pandas as pd from datetime import datetime from eds_scikit.plot.age_pyramid import plot_age_pyramid dates_of_first_visit = pd . Series ([ datetime ( 2020 , 1 , 1 )] * df_person . shape [ 0 ]) plot_age_pyramid ( df_person , datetime_ref = dates_of_first_visit ) Please check the documentation for further details on the function's parameters.","title":"Advanced parameters"},{"location":"functionalities/plotting/event_sequences/","text":"Visualizing event sequences When studying sequences of events (e.g care trajectories, drug sequences, ...), it might be useful to visualize individual sequences. To that end, we provide the plot_event_sequences function to plot individual sequences given an events dataframe. Load a synthetic dataset An events dataset has been created to illustrate the visualization function. It can be load as follows : from eds_scikit.datasets.synthetic.event_sequences import load_event_sequences df_events = load_event_sequences () The df_events dataset contains occurrences of 12 events, derived from 7 events' families (\"A\", \"B\", \"C\", \"D\", \"E\", \"F\", \"G). Events can be both one-time and continuous. An index_date is also provided and refers to the inclusion date of each patient in the cohort. The first raws of the dataframe are as follows : df_events . head () | | person_id | event_family | event | event_start_datetime | event_end_datetime | index_date | |---|-----------|--------------|-------|----------------------|--------------------|------------| | 0 | 1 | A | a1 | 2020-01-01 | 2020-01-02 | 2020-01-01 | | 1 | 1 | A | a2 | 2020-01-03 | 2020-01-04 | 2020-01-01 | | 2 | 1 | B | b1 | 2020-01-03 | 2020-01-06 | 2020-01-01 | | 3 | 1 | C | c1 | 2020-01-05 | NaT | 2020-01-01 | | 4 | 1 | C | c2 | 2020-01-06 | 2020-01-08 | 2020-01-01 | Visualize individual sequences Basic usage Individual sequences of events can be plotted using the plot_event_sequences function : from eds_scikit.plot.event_sequences import plot_event_sequences chart = plot_event_sequences ( df_events ) chart Advanced parameters Further configuration can be provided, including : - dim_mapping : dictionary to set colors and labels for each event type. - family_col : column name of events' families. - list_person_ids : List of specific person_id - same_x_axis_scale : boolean to set all individual charts to the same scale Here we provide an exemple of dim_mapping , and we plot sequences aggregated following the event_family classification. dim_mapping = { \"a1\" : { \"color\" : ( 255 , 200 , 150 ), \"label\" : \"eventA1\" }, \"a2\" : { \"color\" : ( 255 , 150 , 150 ), \"label\" : \"eventA2\" }, \"a3\" : { \"color\" : ( 255 , 100 , 150 ), \"label\" : \"eventA3\" }, \"b1\" : { \"color\" : ( 100 , 200 , 150 ), \"label\" : \"eventB1\" }, \"c1\" : { \"color\" : ( 50 , 255 , 255 ), \"label\" : \"eventC1\" }, \"c2\" : { \"color\" : ( 50 , 200 , 255 ), \"label\" : \"eventC2\" }, \"c3\" : { \"color\" : ( 50 , 100 , 255 ), \"label\" : \"eventC3\" }, \"d1\" : { \"color\" : ( 180 , 200 , 100 ), \"label\" : \"eventD1\" }, \"d2\" : { \"color\" : ( 180 , 150 , 100 ), \"label\" : \"eventD2\" }, \"e1\" : { \"color\" : ( 130 , 60 , 10 ), \"label\" : \"eventE1\" }, \"f1\" : { \"color\" : ( 255 , 0 , 0 ), \"label\" : \"eventF1\" }, \"g1\" : { \"color\" : ( 100 , 0 , 200 ), \"label\" : \"eventG1\" }, } plot_event_sequences ( df_events , family_col = \"event_family\" , dim_mapping = dim_mapping , same_x_axis_scale = True , title = \"Event sequences\" , ) Please check the documentation for further details on the function's parameters.","title":"Event sequence"},{"location":"functionalities/plotting/event_sequences/#visualizing-event-sequences","text":"When studying sequences of events (e.g care trajectories, drug sequences, ...), it might be useful to visualize individual sequences. To that end, we provide the plot_event_sequences function to plot individual sequences given an events dataframe.","title":"Visualizing event sequences"},{"location":"functionalities/plotting/event_sequences/#load-a-synthetic-dataset","text":"An events dataset has been created to illustrate the visualization function. It can be load as follows : from eds_scikit.datasets.synthetic.event_sequences import load_event_sequences df_events = load_event_sequences () The df_events dataset contains occurrences of 12 events, derived from 7 events' families (\"A\", \"B\", \"C\", \"D\", \"E\", \"F\", \"G). Events can be both one-time and continuous. An index_date is also provided and refers to the inclusion date of each patient in the cohort. The first raws of the dataframe are as follows : df_events . head () | | person_id | event_family | event | event_start_datetime | event_end_datetime | index_date | |---|-----------|--------------|-------|----------------------|--------------------|------------| | 0 | 1 | A | a1 | 2020-01-01 | 2020-01-02 | 2020-01-01 | | 1 | 1 | A | a2 | 2020-01-03 | 2020-01-04 | 2020-01-01 | | 2 | 1 | B | b1 | 2020-01-03 | 2020-01-06 | 2020-01-01 | | 3 | 1 | C | c1 | 2020-01-05 | NaT | 2020-01-01 | | 4 | 1 | C | c2 | 2020-01-06 | 2020-01-08 | 2020-01-01 |","title":"Load a synthetic dataset"},{"location":"functionalities/plotting/event_sequences/#visualize-individual-sequences","text":"","title":"Visualize individual sequences"},{"location":"functionalities/plotting/event_sequences/#basic-usage","text":"Individual sequences of events can be plotted using the plot_event_sequences function : from eds_scikit.plot.event_sequences import plot_event_sequences chart = plot_event_sequences ( df_events ) chart","title":"Basic usage"},{"location":"functionalities/plotting/event_sequences/#advanced-parameters","text":"Further configuration can be provided, including : - dim_mapping : dictionary to set colors and labels for each event type. - family_col : column name of events' families. - list_person_ids : List of specific person_id - same_x_axis_scale : boolean to set all individual charts to the same scale Here we provide an exemple of dim_mapping , and we plot sequences aggregated following the event_family classification. dim_mapping = { \"a1\" : { \"color\" : ( 255 , 200 , 150 ), \"label\" : \"eventA1\" }, \"a2\" : { \"color\" : ( 255 , 150 , 150 ), \"label\" : \"eventA2\" }, \"a3\" : { \"color\" : ( 255 , 100 , 150 ), \"label\" : \"eventA3\" }, \"b1\" : { \"color\" : ( 100 , 200 , 150 ), \"label\" : \"eventB1\" }, \"c1\" : { \"color\" : ( 50 , 255 , 255 ), \"label\" : \"eventC1\" }, \"c2\" : { \"color\" : ( 50 , 200 , 255 ), \"label\" : \"eventC2\" }, \"c3\" : { \"color\" : ( 50 , 100 , 255 ), \"label\" : \"eventC3\" }, \"d1\" : { \"color\" : ( 180 , 200 , 100 ), \"label\" : \"eventD1\" }, \"d2\" : { \"color\" : ( 180 , 150 , 100 ), \"label\" : \"eventD2\" }, \"e1\" : { \"color\" : ( 130 , 60 , 10 ), \"label\" : \"eventE1\" }, \"f1\" : { \"color\" : ( 255 , 0 , 0 ), \"label\" : \"eventF1\" }, \"g1\" : { \"color\" : ( 100 , 0 , 200 ), \"label\" : \"eventG1\" }, } plot_event_sequences ( df_events , family_col = \"event_family\" , dim_mapping = dim_mapping , same_x_axis_scale = True , title = \"Event sequences\" , ) Please check the documentation for further details on the function's parameters.","title":"Advanced parameters"},{"location":"functionalities/plotting/flowchart/","text":"(function() { function addWidgetsRenderer() { var requireJsScript = document.createElement('script'); requireJsScript.src = 'https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js'; var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var jupyterWidgetsScript = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} jupyterWidgetsScript.src = widgetRendererSrc; document.body.appendChild(requireJsScript); document.body.appendChild(jupyterWidgetsScript); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); You can download this notebook directly here Generation of an inclusion/exclusion flowchart Inclusion and exclusion flowcharts are one of the key figure to generate when doing medical research. We provide a class to help you in this task. To summarize, you can sequentialy add criteria to the flowchart, by providing A description of the criterion Which patients check the criterion To make the use of this class easier, each criterion can be build independently. The order will be determined by how you add the criteria to the flowchart. At this step, criteria will be combined to output a corrrect flowchart. Counting ( n=... ) is done automatically ! The input data Data can be provided in two forms: DataFrame form or Dictionary form. DataFrame form Dictionary form In this case, data is provided as a unique DataFrame . One column (by default person_id ) stores the ids that constitutes the initial cohort. A criterion, it this case, will be defined as a boolean column, where each row is either accepted or rejected. For instance: data = pd . DataFrame ( dict ( person_id = list ( range ( 10 )), over_18 = 5 * [ True ] + 5 * [ False ], diabetes = [ True , False , True , False , True , False , True , False , True , False ], infarction = [ True , True , False , False , True , True , False , False , True , True ], final_split = [ True ] + 9 * [ False ], ) ) In this case, data is provided as dictionary. Keys represent criteria names, and values contains the ids constituting the passing cohort. Those ids can be in the form of any iterable (list, set, Series, ...). The initial cohort should be provided under the initial key . For instance: data = dict ( initial = list ( range ( 10 )), over_18 = [ 0 , 1 , 2 , 3 , 4 ], diabetes = [ 0 , 2 , 4 , 6 , 8 ], infarction = [ 0 , 1 , 4 , 5 , 8 , 9 ], final_split = [ 0 ], ) A step-by-step example Let us suppose we have a small cohort of 10 patients. From this cohort, we want to select patients with three consecutive criteria: Patients should be at least 18 years old. Patients should have Type I or Type II diabetes. Patients should've had at least one infarction event. On having multiple criteria On advantage of this flowchart generation is that it will handle multiple criteria by itself, by computing intersection iteratively import pandas as pd from eds_scikit.utils.flowchart import Flowchart So let us describe our initial cohort in the DataFrame form from eds_scikit.utils.flowchart import Flowchart data = pd . DataFrame ( dict ( person_id = list ( range ( 10 )), over_18 = 5 * [ True ] + 5 * [ False ], diabetes = [ True , False , True , False , True , False , True , False , True , False ], infarction = [ True , True , False , False , True , True , False , False , True , True ], final_split = [ True ] + 9 * [ False ], ) ) We added an extra final_split column that can, for instance, occur when splitting a cohort into a training and a testing subcohorts. This will result in a split in the flowchart (see below). Here we instantiate the Flowchart with the initial cohort: F = Flowchart ( data = data , initial_description = \"Initial population\" , ) And we add each criterion with the add_criterion method: F . add_criterion ( description = \"Patients over 18 y.o.\" , excluded_description = \"\" , criterion_name = \"over_18\" , ) F . add_criterion ( description = \"With Type I or II diabetes\" , excluded_description = \"\" , criterion_name = \"diabetes\" , ) F . add_criterion ( description = \"With infarction\" , excluded_description = \"\" , criterion_name = \"infarction\" , ) This add_criterion method expects 3 parameters: description : The description to add in the corresponding flowchart's box excluded_description : The description to add in the excluded box of the flowchart criterion_name : DataFrame form : The column name of the data object that contains boolean values to discriminate between rows that checks the criterion and rows that doesn't Dictionary form : The key of the dictionary containing the ids of the passing cohort If you need to do a final split in your flowchart, you can via the dedicated method: F . add_final_split ( left_description = \"\" , right_description = \"\" , criterion_name = \"final_split\" , left_title = \"Cohort 1\" , right_title = \"Cohort 2\" , ) At this point, we are ready to generate the flowchart. Just run the following snippet: F . generate_flowchart ( alternate = True ) 2022-11-15T13:25:23.976033 image/svg+xml Matplotlib v3.5.3, https://matplotlib.org/ *{stroke-linejoin: round; stroke-linecap: butt} Finally, you can save your flowchart (in \".png\" or \".svg\" ): F . save ( \"my_flowchart.png\" ) For more details, you can check the code reference of the Flowchart object.","title":"Generating inclusion/exclusion flowchart"},{"location":"functionalities/plotting/flowchart/#generation-of-an-inclusionexclusion-flowchart","text":"Inclusion and exclusion flowcharts are one of the key figure to generate when doing medical research. We provide a class to help you in this task. To summarize, you can sequentialy add criteria to the flowchart, by providing A description of the criterion Which patients check the criterion To make the use of this class easier, each criterion can be build independently. The order will be determined by how you add the criteria to the flowchart. At this step, criteria will be combined to output a corrrect flowchart. Counting ( n=... ) is done automatically !","title":"Generation of an inclusion/exclusion flowchart"},{"location":"functionalities/plotting/flowchart/#the-input-data","text":"Data can be provided in two forms: DataFrame form or Dictionary form. DataFrame form Dictionary form In this case, data is provided as a unique DataFrame . One column (by default person_id ) stores the ids that constitutes the initial cohort. A criterion, it this case, will be defined as a boolean column, where each row is either accepted or rejected. For instance: data = pd . DataFrame ( dict ( person_id = list ( range ( 10 )), over_18 = 5 * [ True ] + 5 * [ False ], diabetes = [ True , False , True , False , True , False , True , False , True , False ], infarction = [ True , True , False , False , True , True , False , False , True , True ], final_split = [ True ] + 9 * [ False ], ) ) In this case, data is provided as dictionary. Keys represent criteria names, and values contains the ids constituting the passing cohort. Those ids can be in the form of any iterable (list, set, Series, ...). The initial cohort should be provided under the initial key . For instance: data = dict ( initial = list ( range ( 10 )), over_18 = [ 0 , 1 , 2 , 3 , 4 ], diabetes = [ 0 , 2 , 4 , 6 , 8 ], infarction = [ 0 , 1 , 4 , 5 , 8 , 9 ], final_split = [ 0 ], )","title":"The input data"},{"location":"functionalities/plotting/flowchart/#a-step-by-step-example","text":"Let us suppose we have a small cohort of 10 patients. From this cohort, we want to select patients with three consecutive criteria: Patients should be at least 18 years old. Patients should have Type I or Type II diabetes. Patients should've had at least one infarction event. On having multiple criteria On advantage of this flowchart generation is that it will handle multiple criteria by itself, by computing intersection iteratively import pandas as pd from eds_scikit.utils.flowchart import Flowchart So let us describe our initial cohort in the DataFrame form from eds_scikit.utils.flowchart import Flowchart data = pd . DataFrame ( dict ( person_id = list ( range ( 10 )), over_18 = 5 * [ True ] + 5 * [ False ], diabetes = [ True , False , True , False , True , False , True , False , True , False ], infarction = [ True , True , False , False , True , True , False , False , True , True ], final_split = [ True ] + 9 * [ False ], ) ) We added an extra final_split column that can, for instance, occur when splitting a cohort into a training and a testing subcohorts. This will result in a split in the flowchart (see below). Here we instantiate the Flowchart with the initial cohort: F = Flowchart ( data = data , initial_description = \"Initial population\" , ) And we add each criterion with the add_criterion method: F . add_criterion ( description = \"Patients over 18 y.o.\" , excluded_description = \"\" , criterion_name = \"over_18\" , ) F . add_criterion ( description = \"With Type I or II diabetes\" , excluded_description = \"\" , criterion_name = \"diabetes\" , ) F . add_criterion ( description = \"With infarction\" , excluded_description = \"\" , criterion_name = \"infarction\" , ) This add_criterion method expects 3 parameters: description : The description to add in the corresponding flowchart's box excluded_description : The description to add in the excluded box of the flowchart criterion_name : DataFrame form : The column name of the data object that contains boolean values to discriminate between rows that checks the criterion and rows that doesn't Dictionary form : The key of the dictionary containing the ids of the passing cohort If you need to do a final split in your flowchart, you can via the dedicated method: F . add_final_split ( left_description = \"\" , right_description = \"\" , criterion_name = \"final_split\" , left_title = \"Cohort 1\" , right_title = \"Cohort 2\" , ) At this point, we are ready to generate the flowchart. Just run the following snippet: F . generate_flowchart ( alternate = True ) 2022-11-15T13:25:23.976033 image/svg+xml Matplotlib v3.5.3, https://matplotlib.org/ *{stroke-linejoin: round; stroke-linecap: butt} Finally, you can save your flowchart (in \".png\" or \".svg\" ): F . save ( \"my_flowchart.png\" ) For more details, you can check the code reference of the Flowchart object.","title":"A step-by-step example"},{"location":"recipes/small-cohorts/","text":"(function() { function addWidgetsRenderer() { var requireJsScript = document.createElement('script'); requireJsScript.src = 'https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js'; var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var jupyterWidgetsScript = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} jupyterWidgetsScript.src = widgetRendererSrc; document.body.appendChild(requireJsScript); document.body.appendChild(jupyterWidgetsScript); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); You can download this notebook directly here Introduction The goal of this small notebook is to show you how to: Work on a big cohort by staying distributed Do some phenotyping to select a small subcohort Save this subcohort locally to work on it later As a dummy example, we will select patients that underwent a cardiac transplantation. The selection will be performed by using both ICD-10 and by CCAM terminologies. Data Loading import eds_scikit spark , sc , sql = eds_scikit . improve_performances () DBNAME = \"YOUR_DATABASE_NAME\" from eds_scikit.io.hive import HiveData # Data from Hive data = HiveData ( DBNAME ) Phenotyping from eds_scikit.event.ccam import procedures_from_ccam from eds_scikit.event.icd10 import conditions_from_icd10 CCAM = dict ( HEART_TRANSPLANT = dict ( prefix = \"DZEA00\" , # ) ) ICD10 = dict ( HEART_TRANSPLANT = dict ( exact = \"Z941\" , # ) ) procedure_occurrence = procedures_from_ccam ( procedure_occurrence = data . procedure_occurrence , visit_occurrence = data . visit_occurrence , codes = CCAM , date_from_visit = True , ) condition_occurrence = conditions_from_icd10 ( condition_occurrence = data . condition_occurrence , visit_occurrence = data . visit_occurrence , codes = ICD10 , date_from_visit = True , additional_filtering = dict ( condition_status_source_value = { \"DP\" , \"DAS\" }, # ) ) procedure_occurrence . groupby ([ \"concept\" , \"value\" ]) . size () concept value HEART_TRANSPLANT DZEA002 39 dtype: int64 condition_occurrence . groupby ([ \"concept\" , \"value\" ]) . size () concept value HEART_TRANSPLANT Z941 602 dtype: int64 Saving to disk cohort = set ( procedure_occurrence . person_id . to_list () + condition_occurrence . person_id . to_list () ) We can check that our cohort is indeed small and can be stored locally without any concerns: len ( cohort ) 53 And we can also compute a very crude prevalence of heart transplant in our database: f \" { 100 * len ( cohort ) / len ( set ( data . procedure_occurrence . person_id . to_list () + data . condition_occurrence . person_id . to_list ())) : .5f } %\" '0.06849 %' Finally let us save the tables we need locally. Under the hood, eds-scikit will only keep data corresponding to the provided cohort. import os folder = os . path . abspath ( \"./heart_transplant_cohort\" ) tables_to_save = [ \"person\" , \"visit_detail\" , \"visit_occurrence\" , \"procedure_occurrence\" , \"condition_occurrence\" , ] data . persist_tables_to_folder ( folder , tables = tables_to_save , person_ids = cohort , ) Number of unique patients: 53 writing /export/home/cse210038/Thomas/eds-scikit/docs/recipes/heart_transplant_cohort/person.parquet writing /export/home/cse210038/Thomas/eds-scikit/docs/recipes/heart_transplant_cohort/visit_detail.parquet writing /export/home/cse210038/Thomas/eds-scikit/docs/recipes/heart_transplant_cohort/visit_occurrence.parquet writing /export/home/cse210038/Thomas/eds-scikit/docs/recipes/heart_transplant_cohort/procedure_occurrence.parquet writing /export/home/cse210038/Thomas/eds-scikit/docs/recipes/heart_transplant_cohort/condition_occurrence.parquet Using the saved cohort Now that our cohort is saved locally, it can be accessed directly by using the PandasData class. Its akin to the HiveData class, except that the loaded tables will be stored directly as Pandas DataFrames, allowing for faster and easier analysis from eds_scikit.io.files import PandasData data = PandasData ( folder ) As a sanity check, let us display the number of patient in our saved cohort (we are expecting 30) cohort = data . person . person_id . to_list () len ( cohort ) 53 And the crude prevalence that should now be 100% ! f \" { 100 * len ( cohort ) / len ( set ( data . procedure_occurrence . person_id . to_list () + data . condition_occurrence . person_id . to_list ())) : .5f } %\" '100.00000 %'","title":"Saving small cohorts locally"},{"location":"recipes/small-cohorts/#introduction","text":"The goal of this small notebook is to show you how to: Work on a big cohort by staying distributed Do some phenotyping to select a small subcohort Save this subcohort locally to work on it later As a dummy example, we will select patients that underwent a cardiac transplantation. The selection will be performed by using both ICD-10 and by CCAM terminologies.","title":"Introduction"},{"location":"recipes/small-cohorts/#data-loading","text":"import eds_scikit spark , sc , sql = eds_scikit . improve_performances () DBNAME = \"YOUR_DATABASE_NAME\" from eds_scikit.io.hive import HiveData # Data from Hive data = HiveData ( DBNAME )","title":"Data Loading"},{"location":"recipes/small-cohorts/#phenotyping","text":"from eds_scikit.event.ccam import procedures_from_ccam from eds_scikit.event.icd10 import conditions_from_icd10 CCAM = dict ( HEART_TRANSPLANT = dict ( prefix = \"DZEA00\" , # ) ) ICD10 = dict ( HEART_TRANSPLANT = dict ( exact = \"Z941\" , # ) ) procedure_occurrence = procedures_from_ccam ( procedure_occurrence = data . procedure_occurrence , visit_occurrence = data . visit_occurrence , codes = CCAM , date_from_visit = True , ) condition_occurrence = conditions_from_icd10 ( condition_occurrence = data . condition_occurrence , visit_occurrence = data . visit_occurrence , codes = ICD10 , date_from_visit = True , additional_filtering = dict ( condition_status_source_value = { \"DP\" , \"DAS\" }, # ) ) procedure_occurrence . groupby ([ \"concept\" , \"value\" ]) . size () concept value HEART_TRANSPLANT DZEA002 39 dtype: int64 condition_occurrence . groupby ([ \"concept\" , \"value\" ]) . size () concept value HEART_TRANSPLANT Z941 602 dtype: int64","title":"Phenotyping"},{"location":"recipes/small-cohorts/#saving-to-disk","text":"cohort = set ( procedure_occurrence . person_id . to_list () + condition_occurrence . person_id . to_list () ) We can check that our cohort is indeed small and can be stored locally without any concerns: len ( cohort ) 53 And we can also compute a very crude prevalence of heart transplant in our database: f \" { 100 * len ( cohort ) / len ( set ( data . procedure_occurrence . person_id . to_list () + data . condition_occurrence . person_id . to_list ())) : .5f } %\" '0.06849 %' Finally let us save the tables we need locally. Under the hood, eds-scikit will only keep data corresponding to the provided cohort. import os folder = os . path . abspath ( \"./heart_transplant_cohort\" ) tables_to_save = [ \"person\" , \"visit_detail\" , \"visit_occurrence\" , \"procedure_occurrence\" , \"condition_occurrence\" , ] data . persist_tables_to_folder ( folder , tables = tables_to_save , person_ids = cohort , ) Number of unique patients: 53 writing /export/home/cse210038/Thomas/eds-scikit/docs/recipes/heart_transplant_cohort/person.parquet writing /export/home/cse210038/Thomas/eds-scikit/docs/recipes/heart_transplant_cohort/visit_detail.parquet writing /export/home/cse210038/Thomas/eds-scikit/docs/recipes/heart_transplant_cohort/visit_occurrence.parquet writing /export/home/cse210038/Thomas/eds-scikit/docs/recipes/heart_transplant_cohort/procedure_occurrence.parquet writing /export/home/cse210038/Thomas/eds-scikit/docs/recipes/heart_transplant_cohort/condition_occurrence.parquet","title":"Saving to disk"},{"location":"recipes/small-cohorts/#using-the-saved-cohort","text":"Now that our cohort is saved locally, it can be accessed directly by using the PandasData class. Its akin to the HiveData class, except that the loaded tables will be stored directly as Pandas DataFrames, allowing for faster and easier analysis from eds_scikit.io.files import PandasData data = PandasData ( folder ) As a sanity check, let us display the number of patient in our saved cohort (we are expecting 30) cohort = data . person . person_id . to_list () len ( cohort ) 53 And the crude prevalence that should now be 100% ! f \" { 100 * len ( cohort ) / len ( set ( data . procedure_occurrence . person_id . to_list () + data . condition_occurrence . person_id . to_list ())) : .5f } %\" '100.00000 %'","title":"Using the saved cohort"},{"location":"reference/","text":"eds_scikit Top-level package for eds_scikit.","title":"`eds_scikit`"},{"location":"reference/#eds_scikit","text":"Top-level package for eds_scikit.","title":"eds_scikit"},{"location":"reference/SUMMARY/","text":"eds_scikit biology cleaning cohort main utils check_data config prepare_measurement prepare_relationship process_concepts process_measurement process_units viz aggregate plot stats_summary wrapper datasets generation_scripts care_site_hierarchy synthetic base_dataset biology ccam consultation_dates event_sequences hierarchy icd10 person stay_duration suicide_attempt tagging visit_merging emergency emergency_care_site emergency_visit event ccam consultations diabetes from_code icd10 suicide_attempt icu icu_care_site icu_visit io base data_quality files hive i2b2_mapping improve_performance postgres settings table_viz_default_config period stays tagging_functions phenotype base cancer cancer diabetes diabetes psychiatric_disorder psychiatric_disorder suicide_attempt suicide_attempt plot age_pyramid default_table_viz event_sequences table_viz resources reg utils structures attributes description utils bunch checks custom_implem custom_implem cut datetime_helpers flowchart flowchart framework hierarchy logging test_utils typing","title":"SUMMARY"},{"location":"reference/biology/","text":"eds_scikit.biology","title":"`eds_scikit.biology`"},{"location":"reference/biology/#eds_scikitbiology","text":"","title":"eds_scikit.biology"},{"location":"reference/biology/cleaning/","text":"eds_scikit.biology.cleaning","title":"`eds_scikit.biology.cleaning`"},{"location":"reference/biology/cleaning/#eds_scikitbiologycleaning","text":"","title":"eds_scikit.biology.cleaning"},{"location":"reference/biology/cleaning/cohort/","text":"eds_scikit.biology.cleaning.cohort select_cohort select_cohort ( measurement : DataFrame , studied_pop : Union [ DataFrame , List [ int ]]) -> DataFrame Select the patient_ids PARAMETER DESCRIPTION measurement Target DataFrame TYPE: DataFrame studied_pop List of patient_ids to select TYPE: Union [ DataFrame , List [ int ]] RETURNS DESCRIPTION DataFrame Filtered DataFrame with selected patients Source code in eds_scikit/biology/cleaning/cohort.py 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 def select_cohort ( measurement : DataFrame , studied_pop : Union [ DataFrame , List [ int ]], ) -> DataFrame : \"\"\"Select the patient_ids Parameters ---------- measurement : DataFrame Target DataFrame studied_pop : Union[DataFrame, List[int]] List of patient_ids to select Returns ------- DataFrame Filtered DataFrame with selected patients \"\"\" logger . info ( \"Selecting cohort...\" ) if isinstance ( studied_pop , DataFrame . __args__ ): filtered_measures = measurement . merge ( studied_pop , on = \"person_id\" , ) else : filtered_measures = measurement [ measurement . person_id . isin ( studied_pop )] return filtered_measures","title":"cohort"},{"location":"reference/biology/cleaning/cohort/#eds_scikitbiologycleaningcohort","text":"","title":"eds_scikit.biology.cleaning.cohort"},{"location":"reference/biology/cleaning/cohort/#eds_scikit.biology.cleaning.cohort.select_cohort","text":"select_cohort ( measurement : DataFrame , studied_pop : Union [ DataFrame , List [ int ]]) -> DataFrame Select the patient_ids PARAMETER DESCRIPTION measurement Target DataFrame TYPE: DataFrame studied_pop List of patient_ids to select TYPE: Union [ DataFrame , List [ int ]] RETURNS DESCRIPTION DataFrame Filtered DataFrame with selected patients Source code in eds_scikit/biology/cleaning/cohort.py 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 def select_cohort ( measurement : DataFrame , studied_pop : Union [ DataFrame , List [ int ]], ) -> DataFrame : \"\"\"Select the patient_ids Parameters ---------- measurement : DataFrame Target DataFrame studied_pop : Union[DataFrame, List[int]] List of patient_ids to select Returns ------- DataFrame Filtered DataFrame with selected patients \"\"\" logger . info ( \"Selecting cohort...\" ) if isinstance ( studied_pop , DataFrame . __args__ ): filtered_measures = measurement . merge ( studied_pop , on = \"person_id\" , ) else : filtered_measures = measurement [ measurement . person_id . isin ( studied_pop )] return filtered_measures","title":"select_cohort()"},{"location":"reference/biology/cleaning/main/","text":"eds_scikit.biology.cleaning.main bioclean bioclean ( data : Data , concepts_sets : List [ ConceptsSet ] = None , start_date : datetime = None , end_date : datetime = None , convert_units : bool = False , studied_cohort : Union [ DataFrame , List [ int ]] = None ) -> Data It follows the pipeline explained here : PARAMETER DESCRIPTION data Instantiated HiveData , PostgresData or PandasData TYPE: Data concepts_sets List of concepts-sets to select TYPE: List [ ConceptsSet ], optional DEFAULT: None start_date EXAMPLE : \"2019-05-01\" TYPE: datetime , optional DEFAULT: None end_date EXAMPLE : \"2022-05-01\" TYPE: datetime , optional DEFAULT: None convert_units If True, convert units based on ConceptsSets Units object. Eager execution., by default False TYPE: bool , optional DEFAULT: False studied_cohort List of patient_ids to select TYPE: Union [ DataFrame , np . iterable , set ], optional DEFAULT: None RETURNS DESCRIPTION Data Same as the input with the transformed bioclean table Source code in eds_scikit/biology/cleaning/main.py 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 def bioclean ( data : Data , concepts_sets : List [ ConceptsSet ] = None , start_date : datetime = None , end_date : datetime = None , convert_units : bool = False , studied_cohort : Union [ DataFrame , List [ int ]] = None , ) -> Data : \"\"\"It follows the pipeline explained [here][cleaning]: Parameters ---------- data : Data Instantiated [``HiveData``][eds_scikit.io.hive.HiveData], [``PostgresData``][eds_scikit.io.postgres.PostgresData] or [``PandasData``][eds_scikit.io.files.PandasData] concepts_sets : List[ConceptsSet], optional List of concepts-sets to select start_date : datetime, optional **EXAMPLE**: `\"2019-05-01\"` end_date : datetime, optional **EXAMPLE**: `\"2022-05-01\"` convert_units : bool, optional If True, convert units based on ConceptsSets Units object. Eager execution., by default False studied_cohort : Union[DataFrame, np.iterable, set], optional List of patient_ids to select Returns ------- Data Same as the input with the transformed `bioclean` table \"\"\" if concepts_sets is None : logger . info ( \"No concepts sets provided. Loading default concepts sets.\" ) concepts_sets = fetch_all_concepts_set () measurements = prepare_measurement_table ( data , start_date , end_date , concepts_sets , False , convert_units ) # Filter Measurement. if studied_cohort : measurements = select_cohort ( measurements , studied_cohort ) # Transform values data . bioclean = measurements measurements = measurements . merge ( data . visit_occurrence [[ \"care_site_id\" , \"visit_occurrence_id\" ]], on = \"visit_occurrence_id\" , ) measurements = measurements . merge ( data . care_site [[ \"care_site_id\" , \"care_site_short_name\" ]], on = \"care_site_id\" ) # Plot values value_column = \"value_as_number_normalized\" if convert_units else \"value_as_number\" unit_column = ( \"unit_source_value_normalized\" if convert_units else \"unit_source_value\" ) plot_biology_summary ( measurements , value_column , unit_column )","title":"main"},{"location":"reference/biology/cleaning/main/#eds_scikitbiologycleaningmain","text":"","title":"eds_scikit.biology.cleaning.main"},{"location":"reference/biology/cleaning/main/#eds_scikit.biology.cleaning.main.bioclean","text":"bioclean ( data : Data , concepts_sets : List [ ConceptsSet ] = None , start_date : datetime = None , end_date : datetime = None , convert_units : bool = False , studied_cohort : Union [ DataFrame , List [ int ]] = None ) -> Data It follows the pipeline explained here : PARAMETER DESCRIPTION data Instantiated HiveData , PostgresData or PandasData TYPE: Data concepts_sets List of concepts-sets to select TYPE: List [ ConceptsSet ], optional DEFAULT: None start_date EXAMPLE : \"2019-05-01\" TYPE: datetime , optional DEFAULT: None end_date EXAMPLE : \"2022-05-01\" TYPE: datetime , optional DEFAULT: None convert_units If True, convert units based on ConceptsSets Units object. Eager execution., by default False TYPE: bool , optional DEFAULT: False studied_cohort List of patient_ids to select TYPE: Union [ DataFrame , np . iterable , set ], optional DEFAULT: None RETURNS DESCRIPTION Data Same as the input with the transformed bioclean table Source code in eds_scikit/biology/cleaning/main.py 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 def bioclean ( data : Data , concepts_sets : List [ ConceptsSet ] = None , start_date : datetime = None , end_date : datetime = None , convert_units : bool = False , studied_cohort : Union [ DataFrame , List [ int ]] = None , ) -> Data : \"\"\"It follows the pipeline explained [here][cleaning]: Parameters ---------- data : Data Instantiated [``HiveData``][eds_scikit.io.hive.HiveData], [``PostgresData``][eds_scikit.io.postgres.PostgresData] or [``PandasData``][eds_scikit.io.files.PandasData] concepts_sets : List[ConceptsSet], optional List of concepts-sets to select start_date : datetime, optional **EXAMPLE**: `\"2019-05-01\"` end_date : datetime, optional **EXAMPLE**: `\"2022-05-01\"` convert_units : bool, optional If True, convert units based on ConceptsSets Units object. Eager execution., by default False studied_cohort : Union[DataFrame, np.iterable, set], optional List of patient_ids to select Returns ------- Data Same as the input with the transformed `bioclean` table \"\"\" if concepts_sets is None : logger . info ( \"No concepts sets provided. Loading default concepts sets.\" ) concepts_sets = fetch_all_concepts_set () measurements = prepare_measurement_table ( data , start_date , end_date , concepts_sets , False , convert_units ) # Filter Measurement. if studied_cohort : measurements = select_cohort ( measurements , studied_cohort ) # Transform values data . bioclean = measurements measurements = measurements . merge ( data . visit_occurrence [[ \"care_site_id\" , \"visit_occurrence_id\" ]], on = \"visit_occurrence_id\" , ) measurements = measurements . merge ( data . care_site [[ \"care_site_id\" , \"care_site_short_name\" ]], on = \"care_site_id\" ) # Plot values value_column = \"value_as_number_normalized\" if convert_units else \"value_as_number\" unit_column = ( \"unit_source_value_normalized\" if convert_units else \"unit_source_value\" ) plot_biology_summary ( measurements , value_column , unit_column )","title":"bioclean()"},{"location":"reference/biology/utils/","text":"eds_scikit.biology.utils","title":"`eds_scikit.biology.utils`"},{"location":"reference/biology/utils/#eds_scikitbiologyutils","text":"","title":"eds_scikit.biology.utils"},{"location":"reference/biology/utils/check_data/","text":"eds_scikit.biology.utils.check_data check_data_and_select_columns_measurement check_data_and_select_columns_measurement ( data : Data ) Check the required tables and columns in the Data and extract them. PARAMETER DESCRIPTION data Instantiated HiveData , PostgresData or PandasData TYPE: Data Source code in eds_scikit/biology/utils/check_data.py 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 def check_data_and_select_columns_measurement ( data : Data ): \"\"\"Check the required tables and columns in the Data and extract them. Parameters ---------- data : Data Instantiated [``HiveData``][eds_scikit.io.hive.HiveData], [``PostgresData``][eds_scikit.io.postgres.PostgresData] or [``PandasData``][eds_scikit.io.files.PandasData] \"\"\" check_tables ( data , required_tables = [ \"measurement\" , \"concept\" , \"concept_relationship\" , ], ) _measurement_required_columns = [ \"measurement_id\" , \"person_id\" , \"visit_occurrence_id\" , \"measurement_date\" , \"measurement_datetime\" , \"value_source_value\" , \"value_as_number\" , \"unit_source_value\" , \"row_status_source_value\" , \"measurement_source_concept_id\" , \"range_high\" , \"range_low\" , ] _concept_required_columns = [ \"concept_id\" , \"concept_name\" , \"concept_code\" , \"vocabulary_id\" , ] _relationship_required_columns = [ \"concept_id_1\" , \"concept_id_2\" , \"relationship_id\" ] check_columns ( data . measurement , required_columns = _measurement_required_columns , ) check_columns ( data . concept , required_columns = _concept_required_columns ) check_columns ( data . concept_relationship , required_columns = _relationship_required_columns , ) measurement = data . measurement concept = data . concept [ _concept_required_columns ] concept_relationship = data . concept_relationship [ _relationship_required_columns ] return measurement , concept , concept_relationship check_data_and_select_columns_relationship check_data_and_select_columns_relationship ( data : Data ) Check the required tables and columns in the Data and extract them. PARAMETER DESCRIPTION data Instantiated HiveData , PostgresData or PandasData TYPE: Data Source code in eds_scikit/biology/utils/check_data.py 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 def check_data_and_select_columns_relationship ( data : Data ): \"\"\"Check the required tables and columns in the Data and extract them. Parameters ---------- data : Data Instantiated [``HiveData``][eds_scikit.io.hive.HiveData], [``PostgresData``][eds_scikit.io.postgres.PostgresData] or [``PandasData``][eds_scikit.io.files.PandasData] \"\"\" check_tables ( data , required_tables = [ \"concept\" , \"concept_relationship\" , ], ) _concept_required_columns = [ \"concept_id\" , \"concept_name\" , \"concept_code\" , \"vocabulary_id\" , ] _relationship_required_columns = [ \"concept_id_1\" , \"concept_id_2\" , \"relationship_id\" , ] check_columns ( data . concept , required_columns = _concept_required_columns ) check_columns ( data . concept_relationship , required_columns = _relationship_required_columns , ) concept = data . concept [ _concept_required_columns ] concept_relationship = data . concept_relationship [ _relationship_required_columns ] return concept , concept_relationship","title":"check_data"},{"location":"reference/biology/utils/check_data/#eds_scikitbiologyutilscheck_data","text":"","title":"eds_scikit.biology.utils.check_data"},{"location":"reference/biology/utils/check_data/#eds_scikit.biology.utils.check_data.check_data_and_select_columns_measurement","text":"check_data_and_select_columns_measurement ( data : Data ) Check the required tables and columns in the Data and extract them. PARAMETER DESCRIPTION data Instantiated HiveData , PostgresData or PandasData TYPE: Data Source code in eds_scikit/biology/utils/check_data.py 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 def check_data_and_select_columns_measurement ( data : Data ): \"\"\"Check the required tables and columns in the Data and extract them. Parameters ---------- data : Data Instantiated [``HiveData``][eds_scikit.io.hive.HiveData], [``PostgresData``][eds_scikit.io.postgres.PostgresData] or [``PandasData``][eds_scikit.io.files.PandasData] \"\"\" check_tables ( data , required_tables = [ \"measurement\" , \"concept\" , \"concept_relationship\" , ], ) _measurement_required_columns = [ \"measurement_id\" , \"person_id\" , \"visit_occurrence_id\" , \"measurement_date\" , \"measurement_datetime\" , \"value_source_value\" , \"value_as_number\" , \"unit_source_value\" , \"row_status_source_value\" , \"measurement_source_concept_id\" , \"range_high\" , \"range_low\" , ] _concept_required_columns = [ \"concept_id\" , \"concept_name\" , \"concept_code\" , \"vocabulary_id\" , ] _relationship_required_columns = [ \"concept_id_1\" , \"concept_id_2\" , \"relationship_id\" ] check_columns ( data . measurement , required_columns = _measurement_required_columns , ) check_columns ( data . concept , required_columns = _concept_required_columns ) check_columns ( data . concept_relationship , required_columns = _relationship_required_columns , ) measurement = data . measurement concept = data . concept [ _concept_required_columns ] concept_relationship = data . concept_relationship [ _relationship_required_columns ] return measurement , concept , concept_relationship","title":"check_data_and_select_columns_measurement()"},{"location":"reference/biology/utils/check_data/#eds_scikit.biology.utils.check_data.check_data_and_select_columns_relationship","text":"check_data_and_select_columns_relationship ( data : Data ) Check the required tables and columns in the Data and extract them. PARAMETER DESCRIPTION data Instantiated HiveData , PostgresData or PandasData TYPE: Data Source code in eds_scikit/biology/utils/check_data.py 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 def check_data_and_select_columns_relationship ( data : Data ): \"\"\"Check the required tables and columns in the Data and extract them. Parameters ---------- data : Data Instantiated [``HiveData``][eds_scikit.io.hive.HiveData], [``PostgresData``][eds_scikit.io.postgres.PostgresData] or [``PandasData``][eds_scikit.io.files.PandasData] \"\"\" check_tables ( data , required_tables = [ \"concept\" , \"concept_relationship\" , ], ) _concept_required_columns = [ \"concept_id\" , \"concept_name\" , \"concept_code\" , \"vocabulary_id\" , ] _relationship_required_columns = [ \"concept_id_1\" , \"concept_id_2\" , \"relationship_id\" , ] check_columns ( data . concept , required_columns = _concept_required_columns ) check_columns ( data . concept_relationship , required_columns = _relationship_required_columns , ) concept = data . concept [ _concept_required_columns ] concept_relationship = data . concept_relationship [ _relationship_required_columns ] return concept , concept_relationship","title":"check_data_and_select_columns_relationship()"},{"location":"reference/biology/utils/config/","text":"eds_scikit.biology.utils.config create_config_from_stats create_config_from_stats ( concepts_sets : List [ ConceptsSet ], config_name : str , stats_folder : str = 'Biology_summary' ) Generate the configuration file from a statistical summary. It is needed here PARAMETER DESCRIPTION concepts_sets List of concepts-sets to select TYPE: List [ ConceptsSet ] config_name Name of the folder where the configuration will be saved. TYPE: str stats_folder Name of the statistical summary folder TYPE: str DEFAULT: 'Biology_summary' Source code in eds_scikit/biology/utils/config.py 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 def create_config_from_stats ( concepts_sets : List [ ConceptsSet ], config_name : str , stats_folder : str = \"Biology_summary\" , ): \"\"\"Generate the configuration file from a statistical summary. It is needed [here][eds_scikit.biology.cleaning.transform.transform_measurement] Parameters ---------- concepts_sets : List[ConceptsSet] List of concepts-sets to select config_name : str Name of the folder where the configuration will be saved. stats_folder : str Name of the statistical summary folder \"\"\" my_custom_config = pd . DataFrame () for concepts_set in concepts_sets : try : stats = pd . read_pickle ( \" {} / {} /measurement_stats.pkl\" . format ( stats_folder , concepts_set . name ) ) stats [ \"transformed_unit\" ] = ( stats . groupby ( \"unit_source_value\" )[ \"count\" ] . sum ( \"count\" ) . sort_values ( ascending = False ) . index [ 0 ] ) stats [ \"concepts_set\" ] = concepts_set . name stats [ \"Action\" ] = None stats [ \"Coefficient\" ] = None my_custom_config = pd . concat ([ my_custom_config , stats ]) except OSError : logger . error ( \" {} has no statistical summary saved in {} \" , concepts_set . name , stats_folder , ) pass if \"care_site_short_name\" in my_custom_config . columns : # Keep only the row computed from every care site my_custom_config = my_custom_config [ my_custom_config . care_site_short_name == \"ALL\" ] os . makedirs ( CONFIGS_PATH , exist_ok = True ) my_custom_config . to_csv ( \" {} / {} .csv\" . format ( CONFIGS_PATH , config_name ), index = False ) register_configs () list_all_configs list_all_configs () -> List [ str ] Helper to get the names of all saved biology configurations RETURNS DESCRIPTION List [ str ] The configurations names Source code in eds_scikit/biology/utils/config.py 81 82 83 84 85 86 87 88 89 90 91 92 93 94 def list_all_configs () -> List [ str ]: \"\"\" Helper to get the names of all saved biology configurations Returns ------- List[str] The configurations names \"\"\" registered = list ( registry . data . get_all () . keys ()) configs = [ r . split ( \".\" )[ - 1 ] for r in registered if r . startswith ( \"get_biology_config\" ) ] return configs","title":"config"},{"location":"reference/biology/utils/config/#eds_scikitbiologyutilsconfig","text":"","title":"eds_scikit.biology.utils.config"},{"location":"reference/biology/utils/config/#eds_scikit.biology.utils.config.create_config_from_stats","text":"create_config_from_stats ( concepts_sets : List [ ConceptsSet ], config_name : str , stats_folder : str = 'Biology_summary' ) Generate the configuration file from a statistical summary. It is needed here PARAMETER DESCRIPTION concepts_sets List of concepts-sets to select TYPE: List [ ConceptsSet ] config_name Name of the folder where the configuration will be saved. TYPE: str stats_folder Name of the statistical summary folder TYPE: str DEFAULT: 'Biology_summary' Source code in eds_scikit/biology/utils/config.py 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 def create_config_from_stats ( concepts_sets : List [ ConceptsSet ], config_name : str , stats_folder : str = \"Biology_summary\" , ): \"\"\"Generate the configuration file from a statistical summary. It is needed [here][eds_scikit.biology.cleaning.transform.transform_measurement] Parameters ---------- concepts_sets : List[ConceptsSet] List of concepts-sets to select config_name : str Name of the folder where the configuration will be saved. stats_folder : str Name of the statistical summary folder \"\"\" my_custom_config = pd . DataFrame () for concepts_set in concepts_sets : try : stats = pd . read_pickle ( \" {} / {} /measurement_stats.pkl\" . format ( stats_folder , concepts_set . name ) ) stats [ \"transformed_unit\" ] = ( stats . groupby ( \"unit_source_value\" )[ \"count\" ] . sum ( \"count\" ) . sort_values ( ascending = False ) . index [ 0 ] ) stats [ \"concepts_set\" ] = concepts_set . name stats [ \"Action\" ] = None stats [ \"Coefficient\" ] = None my_custom_config = pd . concat ([ my_custom_config , stats ]) except OSError : logger . error ( \" {} has no statistical summary saved in {} \" , concepts_set . name , stats_folder , ) pass if \"care_site_short_name\" in my_custom_config . columns : # Keep only the row computed from every care site my_custom_config = my_custom_config [ my_custom_config . care_site_short_name == \"ALL\" ] os . makedirs ( CONFIGS_PATH , exist_ok = True ) my_custom_config . to_csv ( \" {} / {} .csv\" . format ( CONFIGS_PATH , config_name ), index = False ) register_configs ()","title":"create_config_from_stats()"},{"location":"reference/biology/utils/config/#eds_scikit.biology.utils.config.list_all_configs","text":"list_all_configs () -> List [ str ] Helper to get the names of all saved biology configurations RETURNS DESCRIPTION List [ str ] The configurations names Source code in eds_scikit/biology/utils/config.py 81 82 83 84 85 86 87 88 89 90 91 92 93 94 def list_all_configs () -> List [ str ]: \"\"\" Helper to get the names of all saved biology configurations Returns ------- List[str] The configurations names \"\"\" registered = list ( registry . data . get_all () . keys ()) configs = [ r . split ( \".\" )[ - 1 ] for r in registered if r . startswith ( \"get_biology_config\" ) ] return configs","title":"list_all_configs()"},{"location":"reference/biology/utils/prepare_measurement/","text":"eds_scikit.biology.utils.prepare_measurement prepare_measurement_table prepare_measurement_table ( data : Data , start_date : datetime = None , end_date : datetime = None , concept_sets : List [ ConceptsSet ] = None , get_all_terminologies = True , convert_units = False , compute_table = False ) -> DataFrame Returns filtered measurement table based on validity, date and concept_sets. The output format is identical to data.measurement but adding following columns : - range_high_anomaly, range_low_anomaly - {terminology}_code based on concept_sets terminologies - concept_sets - normalized_units and normalized_values if convert_units==True PARAMETER DESCRIPTION data Instantiated HiveData , PostgresData or PandasData TYPE: Data start_date EXAMPLE : \"2019-05-01\" TYPE: datetime , optional DEFAULT: None end_date EXAMPLE : \"2022-05-01\" TYPE: datetime , optional DEFAULT: None concept_sets List of concepts-sets to select TYPE: List [ ConceptsSet ], optional DEFAULT: None get_all_terminologies If True, all terminologies from settings terminologies will be added, by default True TYPE: bool , optional DEFAULT: True convert_units If True, convert units based on ConceptsSets Units object. Eager execution., by default False TYPE: bool , optional DEFAULT: False compute_table If True, compute table then cache it. Useful to prevent spark issues, especially when running in notebooks. TYPE: bool , optional DEFAULT: False RETURNS DESCRIPTION DataFrame Preprocessed measurement dataframe Source code in eds_scikit/biology/utils/prepare_measurement.py 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 def prepare_measurement_table ( data : Data , start_date : datetime = None , end_date : datetime = None , concept_sets : List [ ConceptsSet ] = None , get_all_terminologies = True , convert_units = False , compute_table = False , ) -> DataFrame : \"\"\"Returns filtered measurement table based on validity, date and concept_sets. The output format is identical to data.measurement but adding following columns : - range_high_anomaly, range_low_anomaly - {terminology}_code based on concept_sets terminologies - concept_sets - normalized_units and normalized_values if convert_units==True Parameters ---------- data : Data Instantiated [``HiveData``][eds_scikit.io.hive.HiveData], [``PostgresData``][eds_scikit.io.postgres.PostgresData] or [``PandasData``][eds_scikit.io.files.PandasData] start_date : datetime, optional **EXAMPLE**: `\"2019-05-01\"` end_date : datetime, optional **EXAMPLE**: `\"2022-05-01\"` concept_sets : List[ConceptsSet], optional List of concepts-sets to select get_all_terminologies : bool, optional If True, all terminologies from settings terminologies will be added, by default True convert_units : bool, optional If True, convert units based on ConceptsSets Units object. Eager execution., by default False compute_table : bool, optional If True, compute table then cache it. Useful to prevent spark issues, especially when running in notebooks. Returns ------- DataFrame Preprocessed measurement dataframe \"\"\" measurement , _ , _ = check_data_and_select_columns_measurement ( data ) # measurement preprocessing measurement = filter_measurement_valid ( measurement ) measurement = filter_measurement_by_date ( measurement , start_date , end_date ) measurement = normalize_unit ( measurement ) measurement = tag_measurement_anomaly ( measurement ) # measurement codes mapping biology_relationship_table = prepare_biology_relationship_table ( data , concept_sets , get_all_terminologies ) measurement = measurement . merge ( biology_relationship_table , left_on = \"measurement_source_concept_id\" , right_on = f \" { mapping [ 0 ][ 0 ] } _concept_id\" , ) if convert_units : logger . info ( \"Lazy preparation not available if convert_units=True. Table will be computed then cached.\" ) measurement = convert_measurement_units ( measurement , concept_sets ) measurement = cache ( measurement ) if compute_table or convert_units : measurement . shape if is_koalas ( measurement ): logger . info ( \"Done. Once computed, measurement will be cached.\" ) return measurement","title":"prepare_measurement"},{"location":"reference/biology/utils/prepare_measurement/#eds_scikitbiologyutilsprepare_measurement","text":"","title":"eds_scikit.biology.utils.prepare_measurement"},{"location":"reference/biology/utils/prepare_measurement/#eds_scikit.biology.utils.prepare_measurement.prepare_measurement_table","text":"prepare_measurement_table ( data : Data , start_date : datetime = None , end_date : datetime = None , concept_sets : List [ ConceptsSet ] = None , get_all_terminologies = True , convert_units = False , compute_table = False ) -> DataFrame Returns filtered measurement table based on validity, date and concept_sets. The output format is identical to data.measurement but adding following columns : - range_high_anomaly, range_low_anomaly - {terminology}_code based on concept_sets terminologies - concept_sets - normalized_units and normalized_values if convert_units==True PARAMETER DESCRIPTION data Instantiated HiveData , PostgresData or PandasData TYPE: Data start_date EXAMPLE : \"2019-05-01\" TYPE: datetime , optional DEFAULT: None end_date EXAMPLE : \"2022-05-01\" TYPE: datetime , optional DEFAULT: None concept_sets List of concepts-sets to select TYPE: List [ ConceptsSet ], optional DEFAULT: None get_all_terminologies If True, all terminologies from settings terminologies will be added, by default True TYPE: bool , optional DEFAULT: True convert_units If True, convert units based on ConceptsSets Units object. Eager execution., by default False TYPE: bool , optional DEFAULT: False compute_table If True, compute table then cache it. Useful to prevent spark issues, especially when running in notebooks. TYPE: bool , optional DEFAULT: False RETURNS DESCRIPTION DataFrame Preprocessed measurement dataframe Source code in eds_scikit/biology/utils/prepare_measurement.py 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 def prepare_measurement_table ( data : Data , start_date : datetime = None , end_date : datetime = None , concept_sets : List [ ConceptsSet ] = None , get_all_terminologies = True , convert_units = False , compute_table = False , ) -> DataFrame : \"\"\"Returns filtered measurement table based on validity, date and concept_sets. The output format is identical to data.measurement but adding following columns : - range_high_anomaly, range_low_anomaly - {terminology}_code based on concept_sets terminologies - concept_sets - normalized_units and normalized_values if convert_units==True Parameters ---------- data : Data Instantiated [``HiveData``][eds_scikit.io.hive.HiveData], [``PostgresData``][eds_scikit.io.postgres.PostgresData] or [``PandasData``][eds_scikit.io.files.PandasData] start_date : datetime, optional **EXAMPLE**: `\"2019-05-01\"` end_date : datetime, optional **EXAMPLE**: `\"2022-05-01\"` concept_sets : List[ConceptsSet], optional List of concepts-sets to select get_all_terminologies : bool, optional If True, all terminologies from settings terminologies will be added, by default True convert_units : bool, optional If True, convert units based on ConceptsSets Units object. Eager execution., by default False compute_table : bool, optional If True, compute table then cache it. Useful to prevent spark issues, especially when running in notebooks. Returns ------- DataFrame Preprocessed measurement dataframe \"\"\" measurement , _ , _ = check_data_and_select_columns_measurement ( data ) # measurement preprocessing measurement = filter_measurement_valid ( measurement ) measurement = filter_measurement_by_date ( measurement , start_date , end_date ) measurement = normalize_unit ( measurement ) measurement = tag_measurement_anomaly ( measurement ) # measurement codes mapping biology_relationship_table = prepare_biology_relationship_table ( data , concept_sets , get_all_terminologies ) measurement = measurement . merge ( biology_relationship_table , left_on = \"measurement_source_concept_id\" , right_on = f \" { mapping [ 0 ][ 0 ] } _concept_id\" , ) if convert_units : logger . info ( \"Lazy preparation not available if convert_units=True. Table will be computed then cached.\" ) measurement = convert_measurement_units ( measurement , concept_sets ) measurement = cache ( measurement ) if compute_table or convert_units : measurement . shape if is_koalas ( measurement ): logger . info ( \"Done. Once computed, measurement will be cached.\" ) return measurement","title":"prepare_measurement_table()"},{"location":"reference/biology/utils/prepare_relationship/","text":"eds_scikit.biology.utils.prepare_relationship prepare_relationship_table prepare_relationship_table ( data : Data , source_terminologies : Dict [ str , str ], mapping : List [ Tuple [ str , str , str ]]) -> ks . DataFrame Create easy-to-use relationship table based on given terminologies and mapping between them. PARAMETER DESCRIPTION data Instantiated HiveData , PostgresData or LocalData TYPE: Data source_terminologies Dictionary of concepts terminologies with their associated regex. TYPE: Dict [ str , str ] **EXAMPLE mapping Ordered mapping of terminologies based on concept_relationship table TYPE: List [ Tuple [ str , str , str ]] **EXAMPLE Output source_concept_id source_concept_name source_concept_code standard_concept_id standard_concept_name standard_concept_code 3 xxxxxxxxxxxx CX1 4 xxxxxxxxxxxx A1 9 xxxxxxxxxxxx ZY2 5 xxxxxxxxxxxx A2 9 xxxxxxxxxxxx B3F 47 xxxxxxxxxxxx D3 7 xxxxxxxxxxxx T32 4 xxxxxxxxxxxx F82 5 xxxxxxxxxxxx S23 1 xxxxxxxxxxxx A432 Source code in eds_scikit/biology/utils/prepare_relationship.py 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 def prepare_relationship_table ( data : Data , source_terminologies : Dict [ str , str ], mapping : List [ Tuple [ str , str , str ]], ) -> ks . DataFrame : # ks or pandas \"\"\" Create easy-to-use relationship table based on given terminologies and mapping between them. Parameters ---------- data : Data Instantiated [``HiveData``][edsteva.io.hive.HiveData], [``PostgresData``][edsteva.io.postgres.PostgresData] or [``LocalData``][edsteva.io.files.LocalData] source_terminologies : Dict[str, str] Dictionary of concepts terminologies with their associated regex. **EXAMPLE**: `{'source_concept' : r'src_.{0, 10}_lab', 'standard_concept' : r'std_concept'}` mapping : List[Tuple[str, str, str]] Ordered mapping of terminologies based on concept_relationship table **EXAMPLE**: `[(\"source_concept\", \"standard_concept\", \"Maps to\")]` Output ------- | source_concept_id | source_concept_name | source_concept_code | standard_concept_id | standard_concept_name | standard_concept_code | |--------------------:|:---------------------:|:---------------------:|:-------------------------:|:-------------------------:|:---------------------------:| | 3 | xxxxxxxxxxxx | CX1 | 4 | xxxxxxxxxxxx | A1 | | 9 | xxxxxxxxxxxx | ZY2 | 5 | xxxxxxxxxxxx | A2 | | 9 | xxxxxxxxxxxx | B3F | 47 | xxxxxxxxxxxx | D3 | | 7 | xxxxxxxxxxxx | T32 | 4 | xxxxxxxxxxxx | F82 | | 5 | xxxxxxxxxxxx | S23 | 1 | xxxxxxxxxxxx | A432 | \"\"\" concept , concept_relationship = check_data_and_select_columns_relationship ( data ) concept_by_terminology = {} for terminology , regex in source_terminologies . items (): concept_by_terminology [ terminology ] = ( concept [ concept . vocabulary_id . str . contains ( regex )] . rename ( columns = { \"concept_id\" : \" {} _concept_id\" . format ( terminology ), \"concept_name\" : \" {} _concept_name\" . format ( terminology ), \"concept_code\" : \" {} _concept_code\" . format ( terminology ), } ) . drop ( columns = \"vocabulary_id\" ) ) root_terminology = mapping [ 0 ][ 0 ] relationship_table = concept_by_terminology [ root_terminology ] # Look over all predefined structured mapping for source , target , relationship_id in mapping : relationship = concept_relationship . rename ( columns = { \"concept_id_1\" : \" {} _concept_id\" . format ( source ), \"concept_id_2\" : \" {} _concept_id\" . format ( target ), } )[ concept_relationship . relationship_id == relationship_id ] . drop ( columns = \"relationship_id\" ) relationship = relationship . merge ( concept_by_terminology [ target ], on = \" {} _concept_id\" . format ( target ) ) relationship_table = relationship_table . merge ( relationship , on = \" {} _concept_id\" . format ( source ), how = \"left\" ) relationship_table = relationship_table . fillna ( \"Unknown\" ) return relationship_table filter_concept_sets_relationship_table filter_concept_sets_relationship_table ( relationship_table , concept_sets ) Filter relationship table using concept_sets concept codes. PARAMETER DESCRIPTION relationship_table Biology relationship table TYPE: DataFrame concept_sets List of concepts-sets to select TYPE: List [ ConceptsSet ] RETURNS DESCRIPTION DataFrame Filtered biology relationship table Source code in eds_scikit/biology/utils/prepare_relationship.py 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 def filter_concept_sets_relationship_table ( relationship_table , concept_sets ): \"\"\"Filter relationship table using concept_sets concept codes. Parameters ---------- relationship_table : DataFrame Biology relationship table concept_sets : List[ConceptsSet] List of concepts-sets to select Returns ------- DataFrame Filtered biology relationship table \"\"\" framework = get_framework ( relationship_table ) concept_sets_tables = pd . DataFrame ({}) for concept_set in concept_sets : concept_set_table = concept_set . get_concept_codes_table () concept_sets_tables = pd . concat ( ( concept_set_table , concept_sets_tables ), axis = 0 ) terminologies = concept_sets_tables . terminology . unique () concept_sets_tables = to ( framework , concept_sets_tables ) filtered_terminology_table = framework . DataFrame ({}) for terminology in terminologies : if f \" { terminology } _concept_code\" in relationship_table . columns : filtered_terminology_table_ = concept_sets_tables [ concept_sets_tables . terminology == terminology ] . merge ( relationship_table , on = f \" { terminology } _concept_code\" , how = \"left\" , suffixes = ( \"_x\" , \"\" ), ) filtered_terminology_table_ = filtered_terminology_table_ [ [ column for column in filtered_terminology_table_ . columns if not ( \"_x\" in column ) ] ] filtered_terminology_table = framework . concat ( ( filtered_terminology_table_ , filtered_terminology_table ) ) . drop_duplicates () return filtered_terminology_table concept_sets_columns concept_sets_columns ( relationship_table : DataFrame , concept_sets : List [ ConceptsSet ], extra_terminologies : List = List [ str ]) -> List [ str ] Filter relationship_table keeping concepts_sets terminologies columns. PARAMETER DESCRIPTION relationship_table TYPE: DataFrame concept_sets TYPE: List [ ConceptsSet ] extra_terminologies TYPE: List , optional DEFAULT: List[str] RETURNS DESCRIPTION List [ str ] Source code in eds_scikit/biology/utils/prepare_relationship.py 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 def concept_sets_columns ( relationship_table : DataFrame , concept_sets : List [ ConceptsSet ], extra_terminologies : List = List [ str ], ) -> List [ str ]: \"\"\"Filter relationship_table keeping concepts_sets terminologies columns. Parameters ---------- relationship_table : DataFrame concept_sets : List[ConceptsSet] extra_terminologies : List, optional Returns ------- List[str] \"\"\" keep_terminologies = extra_terminologies for concept_set in concept_sets : keep_terminologies += concept_set . concept_codes . keys () keep_columns = [] for col in relationship_table . columns : if any ([ terminology in col for terminology in keep_terminologies ]): keep_columns . append ( col ) return keep_columns prepare_biology_relationship_table prepare_biology_relationship_table ( data : Data , concept_sets : List [ ConceptsSet ] = None , get_all_terminologies : bool = True ) -> DataFrame Prepare biology relationship table to map concept codes based on settings.source_terminologies and settings.mapping. PARAMETER DESCRIPTION data Instantiated HiveData , PostgresData or PandasData TYPE: Data concept_sets List of concepts-sets to select TYPE: List [ ConceptsSet ], optional DEFAULT: None get_all_terminologies If True, all terminologies from settings terminologies will be added, by default True TYPE: bool , optional DEFAULT: True Returns DataFrame biology_relationship_table to be merged with measurement Source code in eds_scikit/biology/utils/prepare_relationship.py 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 def prepare_biology_relationship_table ( data : Data , concept_sets : List [ ConceptsSet ] = None , get_all_terminologies : bool = True , ) -> DataFrame : \"\"\"Prepare biology relationship table to map concept codes based on settings.source_terminologies and settings.mapping. Parameters ---------- data : Data Instantiated [``HiveData``][eds_scikit.io.hive.HiveData], [``PostgresData``][eds_scikit.io.postgres.PostgresData] or [``PandasData``][eds_scikit.io.files.PandasData] concept_sets : List[ConceptsSet], optional List of concepts-sets to select get_all_terminologies : bool, optional If True, all terminologies from settings terminologies will be added, by default True Returns ------- DataFrame biology_relationship_table to be merged with measurement \"\"\" if concept_sets is None and not get_all_terminologies : raise Exception ( \"get_all_terminologies must be True if no concept_sets provided.\" ) biology_relationship_table = prepare_relationship_table ( data , source_terminologies , mapping ) biology_relationship_table = ( filter_concept_sets_relationship_table ( biology_relationship_table , concept_sets ) if concept_sets else biology_relationship_table ) keep_columns = ( biology_relationship_table . columns if get_all_terminologies else concept_sets_columns ( biology_relationship_table , concept_sets , [ mapping [ 0 ][ 0 ], \"concept_set\" ], ) ) biology_relationship_table = biology_relationship_table [ keep_columns ] return biology_relationship_table","title":"prepare_relationship"},{"location":"reference/biology/utils/prepare_relationship/#eds_scikitbiologyutilsprepare_relationship","text":"","title":"eds_scikit.biology.utils.prepare_relationship"},{"location":"reference/biology/utils/prepare_relationship/#eds_scikit.biology.utils.prepare_relationship.prepare_relationship_table","text":"prepare_relationship_table ( data : Data , source_terminologies : Dict [ str , str ], mapping : List [ Tuple [ str , str , str ]]) -> ks . DataFrame Create easy-to-use relationship table based on given terminologies and mapping between them. PARAMETER DESCRIPTION data Instantiated HiveData , PostgresData or LocalData TYPE: Data source_terminologies Dictionary of concepts terminologies with their associated regex. TYPE: Dict [ str , str ] **EXAMPLE mapping Ordered mapping of terminologies based on concept_relationship table TYPE: List [ Tuple [ str , str , str ]] **EXAMPLE","title":"prepare_relationship_table()"},{"location":"reference/biology/utils/prepare_relationship/#eds_scikit.biology.utils.prepare_relationship.prepare_relationship_table--output","text":"source_concept_id source_concept_name source_concept_code standard_concept_id standard_concept_name standard_concept_code 3 xxxxxxxxxxxx CX1 4 xxxxxxxxxxxx A1 9 xxxxxxxxxxxx ZY2 5 xxxxxxxxxxxx A2 9 xxxxxxxxxxxx B3F 47 xxxxxxxxxxxx D3 7 xxxxxxxxxxxx T32 4 xxxxxxxxxxxx F82 5 xxxxxxxxxxxx S23 1 xxxxxxxxxxxx A432 Source code in eds_scikit/biology/utils/prepare_relationship.py 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 def prepare_relationship_table ( data : Data , source_terminologies : Dict [ str , str ], mapping : List [ Tuple [ str , str , str ]], ) -> ks . DataFrame : # ks or pandas \"\"\" Create easy-to-use relationship table based on given terminologies and mapping between them. Parameters ---------- data : Data Instantiated [``HiveData``][edsteva.io.hive.HiveData], [``PostgresData``][edsteva.io.postgres.PostgresData] or [``LocalData``][edsteva.io.files.LocalData] source_terminologies : Dict[str, str] Dictionary of concepts terminologies with their associated regex. **EXAMPLE**: `{'source_concept' : r'src_.{0, 10}_lab', 'standard_concept' : r'std_concept'}` mapping : List[Tuple[str, str, str]] Ordered mapping of terminologies based on concept_relationship table **EXAMPLE**: `[(\"source_concept\", \"standard_concept\", \"Maps to\")]` Output ------- | source_concept_id | source_concept_name | source_concept_code | standard_concept_id | standard_concept_name | standard_concept_code | |--------------------:|:---------------------:|:---------------------:|:-------------------------:|:-------------------------:|:---------------------------:| | 3 | xxxxxxxxxxxx | CX1 | 4 | xxxxxxxxxxxx | A1 | | 9 | xxxxxxxxxxxx | ZY2 | 5 | xxxxxxxxxxxx | A2 | | 9 | xxxxxxxxxxxx | B3F | 47 | xxxxxxxxxxxx | D3 | | 7 | xxxxxxxxxxxx | T32 | 4 | xxxxxxxxxxxx | F82 | | 5 | xxxxxxxxxxxx | S23 | 1 | xxxxxxxxxxxx | A432 | \"\"\" concept , concept_relationship = check_data_and_select_columns_relationship ( data ) concept_by_terminology = {} for terminology , regex in source_terminologies . items (): concept_by_terminology [ terminology ] = ( concept [ concept . vocabulary_id . str . contains ( regex )] . rename ( columns = { \"concept_id\" : \" {} _concept_id\" . format ( terminology ), \"concept_name\" : \" {} _concept_name\" . format ( terminology ), \"concept_code\" : \" {} _concept_code\" . format ( terminology ), } ) . drop ( columns = \"vocabulary_id\" ) ) root_terminology = mapping [ 0 ][ 0 ] relationship_table = concept_by_terminology [ root_terminology ] # Look over all predefined structured mapping for source , target , relationship_id in mapping : relationship = concept_relationship . rename ( columns = { \"concept_id_1\" : \" {} _concept_id\" . format ( source ), \"concept_id_2\" : \" {} _concept_id\" . format ( target ), } )[ concept_relationship . relationship_id == relationship_id ] . drop ( columns = \"relationship_id\" ) relationship = relationship . merge ( concept_by_terminology [ target ], on = \" {} _concept_id\" . format ( target ) ) relationship_table = relationship_table . merge ( relationship , on = \" {} _concept_id\" . format ( source ), how = \"left\" ) relationship_table = relationship_table . fillna ( \"Unknown\" ) return relationship_table","title":"Output"},{"location":"reference/biology/utils/prepare_relationship/#eds_scikit.biology.utils.prepare_relationship.filter_concept_sets_relationship_table","text":"filter_concept_sets_relationship_table ( relationship_table , concept_sets ) Filter relationship table using concept_sets concept codes. PARAMETER DESCRIPTION relationship_table Biology relationship table TYPE: DataFrame concept_sets List of concepts-sets to select TYPE: List [ ConceptsSet ] RETURNS DESCRIPTION DataFrame Filtered biology relationship table Source code in eds_scikit/biology/utils/prepare_relationship.py 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 def filter_concept_sets_relationship_table ( relationship_table , concept_sets ): \"\"\"Filter relationship table using concept_sets concept codes. Parameters ---------- relationship_table : DataFrame Biology relationship table concept_sets : List[ConceptsSet] List of concepts-sets to select Returns ------- DataFrame Filtered biology relationship table \"\"\" framework = get_framework ( relationship_table ) concept_sets_tables = pd . DataFrame ({}) for concept_set in concept_sets : concept_set_table = concept_set . get_concept_codes_table () concept_sets_tables = pd . concat ( ( concept_set_table , concept_sets_tables ), axis = 0 ) terminologies = concept_sets_tables . terminology . unique () concept_sets_tables = to ( framework , concept_sets_tables ) filtered_terminology_table = framework . DataFrame ({}) for terminology in terminologies : if f \" { terminology } _concept_code\" in relationship_table . columns : filtered_terminology_table_ = concept_sets_tables [ concept_sets_tables . terminology == terminology ] . merge ( relationship_table , on = f \" { terminology } _concept_code\" , how = \"left\" , suffixes = ( \"_x\" , \"\" ), ) filtered_terminology_table_ = filtered_terminology_table_ [ [ column for column in filtered_terminology_table_ . columns if not ( \"_x\" in column ) ] ] filtered_terminology_table = framework . concat ( ( filtered_terminology_table_ , filtered_terminology_table ) ) . drop_duplicates () return filtered_terminology_table","title":"filter_concept_sets_relationship_table()"},{"location":"reference/biology/utils/prepare_relationship/#eds_scikit.biology.utils.prepare_relationship.concept_sets_columns","text":"concept_sets_columns ( relationship_table : DataFrame , concept_sets : List [ ConceptsSet ], extra_terminologies : List = List [ str ]) -> List [ str ] Filter relationship_table keeping concepts_sets terminologies columns. PARAMETER DESCRIPTION relationship_table TYPE: DataFrame concept_sets TYPE: List [ ConceptsSet ] extra_terminologies TYPE: List , optional DEFAULT: List[str] RETURNS DESCRIPTION List [ str ] Source code in eds_scikit/biology/utils/prepare_relationship.py 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 def concept_sets_columns ( relationship_table : DataFrame , concept_sets : List [ ConceptsSet ], extra_terminologies : List = List [ str ], ) -> List [ str ]: \"\"\"Filter relationship_table keeping concepts_sets terminologies columns. Parameters ---------- relationship_table : DataFrame concept_sets : List[ConceptsSet] extra_terminologies : List, optional Returns ------- List[str] \"\"\" keep_terminologies = extra_terminologies for concept_set in concept_sets : keep_terminologies += concept_set . concept_codes . keys () keep_columns = [] for col in relationship_table . columns : if any ([ terminology in col for terminology in keep_terminologies ]): keep_columns . append ( col ) return keep_columns","title":"concept_sets_columns()"},{"location":"reference/biology/utils/prepare_relationship/#eds_scikit.biology.utils.prepare_relationship.prepare_biology_relationship_table","text":"prepare_biology_relationship_table ( data : Data , concept_sets : List [ ConceptsSet ] = None , get_all_terminologies : bool = True ) -> DataFrame Prepare biology relationship table to map concept codes based on settings.source_terminologies and settings.mapping. PARAMETER DESCRIPTION data Instantiated HiveData , PostgresData or PandasData TYPE: Data concept_sets List of concepts-sets to select TYPE: List [ ConceptsSet ], optional DEFAULT: None get_all_terminologies If True, all terminologies from settings terminologies will be added, by default True TYPE: bool , optional DEFAULT: True Returns DataFrame biology_relationship_table to be merged with measurement Source code in eds_scikit/biology/utils/prepare_relationship.py 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 def prepare_biology_relationship_table ( data : Data , concept_sets : List [ ConceptsSet ] = None , get_all_terminologies : bool = True , ) -> DataFrame : \"\"\"Prepare biology relationship table to map concept codes based on settings.source_terminologies and settings.mapping. Parameters ---------- data : Data Instantiated [``HiveData``][eds_scikit.io.hive.HiveData], [``PostgresData``][eds_scikit.io.postgres.PostgresData] or [``PandasData``][eds_scikit.io.files.PandasData] concept_sets : List[ConceptsSet], optional List of concepts-sets to select get_all_terminologies : bool, optional If True, all terminologies from settings terminologies will be added, by default True Returns ------- DataFrame biology_relationship_table to be merged with measurement \"\"\" if concept_sets is None and not get_all_terminologies : raise Exception ( \"get_all_terminologies must be True if no concept_sets provided.\" ) biology_relationship_table = prepare_relationship_table ( data , source_terminologies , mapping ) biology_relationship_table = ( filter_concept_sets_relationship_table ( biology_relationship_table , concept_sets ) if concept_sets else biology_relationship_table ) keep_columns = ( biology_relationship_table . columns if get_all_terminologies else concept_sets_columns ( biology_relationship_table , concept_sets , [ mapping [ 0 ][ 0 ], \"concept_set\" ], ) ) biology_relationship_table = biology_relationship_table [ keep_columns ] return biology_relationship_table","title":"prepare_biology_relationship_table()"},{"location":"reference/biology/utils/process_concepts/","text":"eds_scikit.biology.utils.process_concepts ConceptsSet ConceptsSet ( name : str ) Class defining the concepts-sets with 2 attributes: name : the name of the concepts-set concept_codes : the list of concepts codes included in the concepts-set Source code in eds_scikit/biology/utils/process_concepts.py 25 26 27 28 29 30 31 32 33 def __init__ ( self , name : str ): self . name = name self . units = Units () fetched_codes = fetch_concept_codes_from_name ( name ) if fetched_codes : self . concept_codes = { \"GLIMS_ANABIO\" : fetch_concept_codes_from_name ( name )} else : self . concept_codes = {} fetch_all_concepts_set fetch_all_concepts_set ( concepts_sets_table_name : str = 'default_concepts_sets' ) -> List [ ConceptsSet ] Returns a list of all the concepts-sets of the chosen tables. By default, the table is here . PARAMETER DESCRIPTION concepts_sets_table_name Name of the table to extract concepts-sets from TYPE: str , optional DEFAULT: 'default_concepts_sets' RETURNS DESCRIPTION List [ ConceptsSet ] The list of all concepts-sets in the selected table Source code in eds_scikit/biology/utils/process_concepts.py 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 def fetch_all_concepts_set ( concepts_sets_table_name : str = \"default_concepts_sets\" , ) -> List [ ConceptsSet ]: \"\"\"Returns a list of all the concepts-sets of the chosen tables. By default, the table is [here][concepts-sets]. Parameters ---------- concepts_sets_table_name : str, optional Name of the table to extract concepts-sets from Returns ------- List[ConceptsSet] The list of all concepts-sets in the selected table \"\"\" concepts_sets = [] default_concepts_sets = getattr ( datasets , concepts_sets_table_name ) for concepts_set_name in default_concepts_sets . concepts_set_name : concepts_sets . append ( ConceptsSet ( concepts_set_name )) logger . info ( \"Fetch all concepts-sets from table {} \" , concepts_sets_table_name ) return concepts_sets","title":"process_concepts"},{"location":"reference/biology/utils/process_concepts/#eds_scikitbiologyutilsprocess_concepts","text":"","title":"eds_scikit.biology.utils.process_concepts"},{"location":"reference/biology/utils/process_concepts/#eds_scikit.biology.utils.process_concepts.ConceptsSet","text":"ConceptsSet ( name : str ) Class defining the concepts-sets with 2 attributes: name : the name of the concepts-set concept_codes : the list of concepts codes included in the concepts-set Source code in eds_scikit/biology/utils/process_concepts.py 25 26 27 28 29 30 31 32 33 def __init__ ( self , name : str ): self . name = name self . units = Units () fetched_codes = fetch_concept_codes_from_name ( name ) if fetched_codes : self . concept_codes = { \"GLIMS_ANABIO\" : fetch_concept_codes_from_name ( name )} else : self . concept_codes = {}","title":"ConceptsSet"},{"location":"reference/biology/utils/process_concepts/#eds_scikit.biology.utils.process_concepts.fetch_all_concepts_set","text":"fetch_all_concepts_set ( concepts_sets_table_name : str = 'default_concepts_sets' ) -> List [ ConceptsSet ] Returns a list of all the concepts-sets of the chosen tables. By default, the table is here . PARAMETER DESCRIPTION concepts_sets_table_name Name of the table to extract concepts-sets from TYPE: str , optional DEFAULT: 'default_concepts_sets' RETURNS DESCRIPTION List [ ConceptsSet ] The list of all concepts-sets in the selected table Source code in eds_scikit/biology/utils/process_concepts.py 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 def fetch_all_concepts_set ( concepts_sets_table_name : str = \"default_concepts_sets\" , ) -> List [ ConceptsSet ]: \"\"\"Returns a list of all the concepts-sets of the chosen tables. By default, the table is [here][concepts-sets]. Parameters ---------- concepts_sets_table_name : str, optional Name of the table to extract concepts-sets from Returns ------- List[ConceptsSet] The list of all concepts-sets in the selected table \"\"\" concepts_sets = [] default_concepts_sets = getattr ( datasets , concepts_sets_table_name ) for concepts_set_name in default_concepts_sets . concepts_set_name : concepts_sets . append ( ConceptsSet ( concepts_set_name )) logger . info ( \"Fetch all concepts-sets from table {} \" , concepts_sets_table_name ) return concepts_sets","title":"fetch_all_concepts_set()"},{"location":"reference/biology/utils/process_measurement/","text":"eds_scikit.biology.utils.process_measurement filter_measurement_valid filter_measurement_valid ( measurement : DataFrame ) -> DataFrame Filter valid observations based on the row_status_source_value column PARAMETER DESCRIPTION measurement DataFrame to filter TYPE: DataFrame RETURNS DESCRIPTION DataFrame DataFrame with valid observations only Source code in eds_scikit/biology/utils/process_measurement.py 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 def filter_measurement_valid ( measurement : DataFrame ) -> DataFrame : \"\"\"Filter valid observations based on the `row_status_source_value` column Parameters ---------- measurement : DataFrame DataFrame to filter Returns ------- DataFrame DataFrame with valid observations only \"\"\" check_columns ( df = measurement , required_columns = [ \"row_status_source_value\" ], df_name = \"measurment\" , ) measurement_valid = measurement [ measurement [ \"row_status_source_value\" ] == \"Valid\u00e9\" ] measurement_valid = measurement_valid . drop ( columns = [ \"row_status_source_value\" ]) return measurement_valid filter_measurement_by_date filter_measurement_by_date ( measurement : DataFrame , start_date : datetime = None , end_date : datetime = None ) -> DataFrame Filter observations that are inside the selected time window PARAMETER DESCRIPTION measurement DataFrame to filter TYPE: DataFrame start_date EXAMPLE : \"2019-05-01\" TYPE: datetime , optional DEFAULT: None end_date EXAMPLE : \"2022-05-01\" TYPE: datetime , optional DEFAULT: None RETURNS DESCRIPTION DataFrame DataFrame with observations inside the selected time window only Source code in eds_scikit/biology/utils/process_measurement.py 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 def filter_measurement_by_date ( measurement : DataFrame , start_date : datetime = None , end_date : datetime = None ) -> DataFrame : \"\"\"Filter observations that are inside the selected time window Parameters ---------- measurement : DataFrame DataFrame to filter start_date : datetime, optional **EXAMPLE**: `\"2019-05-01\"` end_date : datetime, optional **EXAMPLE**: `\"2022-05-01\"` Returns ------- DataFrame DataFrame with observations inside the selected time window only \"\"\" check_columns ( df = measurement , required_columns = [ \"measurement_date\" ], df_name = \"measurment\" ) measurement . measurement_date = measurement . measurement_date . astype ( \"datetime64[ns]\" ) measurement . dropna ( subset = [ \"measurement_date\" ], inplace = True ) if start_date : measurement = measurement [ measurement [ \"measurement_date\" ] >= start_date ] if end_date : measurement = measurement [ measurement [ \"measurement_date\" ] <= end_date ] return measurement tag_measurement_anomaly tag_measurement_anomaly ( measurement : DataFrame ) -> DataFrame PARAMETER DESCRIPTION measurement DataFrame to filter TYPE: DataFrame start_date EXAMPLE : \"2019-05-01\" TYPE: datetime , optional end_date EXAMPLE : \"2022-05-01\" TYPE: datetime , optional Source code in eds_scikit/biology/utils/process_measurement.py 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 def tag_measurement_anomaly ( measurement : DataFrame ) -> DataFrame : \"\"\" Parameters ---------- measurement : DataFrame DataFrame to filter start_date : datetime, optional **EXAMPLE**: `\"2019-05-01\"` end_date : datetime, optional **EXAMPLE**: `\"2022-05-01\"` Returns ------- \"\"\" measurement [ \"range_high_anomaly\" ] = ( ~ measurement . range_high . isna ()) & ( measurement [ \"value_as_number\" ] > measurement [ \"range_high\" ] ) measurement [ \"range_low_anomaly\" ] = ( ~ measurement . range_low . isna ()) & ( measurement [ \"value_as_number\" ] < measurement [ \"range_low\" ] ) return measurement convert_measurement_units convert_measurement_units ( measurement : DataFrame , concepts_sets : List [ ConceptsSet ]) -> DataFrame Add value_as_number_normalized, unit_source_value_normalized and factor columns to measurement dataframe based on concepts_sets and units. PARAMETER DESCRIPTION measurement TYPE: DataFrame concepts_sets TYPE: List [ ConceptsSet ] RETURNS DESCRIPTION DataFrame Measurement with added columns value_as_number_normalized, unit_source_value_normalized and factor. Source code in eds_scikit/biology/utils/process_measurement.py 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 def convert_measurement_units ( measurement : DataFrame , concepts_sets : List [ ConceptsSet ] ) -> DataFrame : \"\"\"Add value_as_number_normalized, unit_source_value_normalized and factor columns to measurement dataframe based on concepts_sets and units. Parameters ---------- measurement : DataFrame concepts_sets : List[ConceptsSet] Returns ------- DataFrame Measurement with added columns value_as_number_normalized, unit_source_value_normalized and factor. \"\"\" if is_koalas ( measurement ): measurement = cache ( measurement ) measurement . shape conversion_table = to ( \"koalas\" , get_conversion_table ( measurement , concepts_sets ) ) else : conversion_table = get_conversion_table ( measurement , concepts_sets ) measurement = measurement . merge ( conversion_table , on = [ \"concept_set\" , \"unit_source_value\" ] ) measurement [ \"value_as_number_normalized\" ] = ( measurement [ \"value_as_number\" ] * measurement [ \"factor\" ] ) return measurement get_conversion_table get_conversion_table ( measurement : DataFrame , concepts_sets : List [ ConceptsSet ]) -> DataFrame Given measurement dataframe and list of concepts_sets output conversion table to be merged with measurement. PARAMETER DESCRIPTION measurement TYPE: DataFrame concepts_sets TYPE: List [ ConceptsSet ] RETURNS DESCRIPTION DataFrame Conversion table to be merged with measurement Source code in eds_scikit/biology/utils/process_measurement.py 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 def get_conversion_table ( measurement : DataFrame , concepts_sets : List [ ConceptsSet ] ) -> DataFrame : \"\"\"Given measurement dataframe and list of concepts_sets output conversion table to be merged with measurement. Parameters ---------- measurement : DataFrame concepts_sets : List[ConceptsSet] Returns ------- DataFrame Conversion table to be merged with measurement \"\"\" conversion_table = ( measurement . groupby ( \"concept_set\" )[ \"unit_source_value\" ] . unique () . explode () . to_frame () . reset_index () ) conversion_table = to ( \"pandas\" , conversion_table ) conversion_table [ \"unit_source_value_normalized\" ] = conversion_table [ \"unit_source_value\" ] conversion_table [ \"factor\" ] = conversion_table . apply ( lambda x : 1 if x . unit_source_value_normalized else 0 , axis = 1 ) for concept_set in concepts_sets : unit_source_value_normalized = concept_set . units . target_unit conversion_table . loc [ conversion_table . concept_set == concept_set . name , \"unit_source_value_normalized\" , ] = conversion_table . apply ( lambda x : unit_source_value_normalized if concept_set . units . can_be_converted ( x . unit_source_value , unit_source_value_normalized ) else concept_set . units . get_unit_base ( x . unit_source_value ), axis = 1 , ) conversion_table . loc [ conversion_table . concept_set == concept_set . name , \"factor\" ] = conversion_table . apply ( lambda x : concept_set . units . convert_unit ( x . unit_source_value , x . unit_source_value_normalized ), axis = 1 , ) conversion_table = conversion_table . fillna ( 1 ) return conversion_table","title":"process_measurement"},{"location":"reference/biology/utils/process_measurement/#eds_scikitbiologyutilsprocess_measurement","text":"","title":"eds_scikit.biology.utils.process_measurement"},{"location":"reference/biology/utils/process_measurement/#eds_scikit.biology.utils.process_measurement.filter_measurement_valid","text":"filter_measurement_valid ( measurement : DataFrame ) -> DataFrame Filter valid observations based on the row_status_source_value column PARAMETER DESCRIPTION measurement DataFrame to filter TYPE: DataFrame RETURNS DESCRIPTION DataFrame DataFrame with valid observations only Source code in eds_scikit/biology/utils/process_measurement.py 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 def filter_measurement_valid ( measurement : DataFrame ) -> DataFrame : \"\"\"Filter valid observations based on the `row_status_source_value` column Parameters ---------- measurement : DataFrame DataFrame to filter Returns ------- DataFrame DataFrame with valid observations only \"\"\" check_columns ( df = measurement , required_columns = [ \"row_status_source_value\" ], df_name = \"measurment\" , ) measurement_valid = measurement [ measurement [ \"row_status_source_value\" ] == \"Valid\u00e9\" ] measurement_valid = measurement_valid . drop ( columns = [ \"row_status_source_value\" ]) return measurement_valid","title":"filter_measurement_valid()"},{"location":"reference/biology/utils/process_measurement/#eds_scikit.biology.utils.process_measurement.filter_measurement_by_date","text":"filter_measurement_by_date ( measurement : DataFrame , start_date : datetime = None , end_date : datetime = None ) -> DataFrame Filter observations that are inside the selected time window PARAMETER DESCRIPTION measurement DataFrame to filter TYPE: DataFrame start_date EXAMPLE : \"2019-05-01\" TYPE: datetime , optional DEFAULT: None end_date EXAMPLE : \"2022-05-01\" TYPE: datetime , optional DEFAULT: None RETURNS DESCRIPTION DataFrame DataFrame with observations inside the selected time window only Source code in eds_scikit/biology/utils/process_measurement.py 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 def filter_measurement_by_date ( measurement : DataFrame , start_date : datetime = None , end_date : datetime = None ) -> DataFrame : \"\"\"Filter observations that are inside the selected time window Parameters ---------- measurement : DataFrame DataFrame to filter start_date : datetime, optional **EXAMPLE**: `\"2019-05-01\"` end_date : datetime, optional **EXAMPLE**: `\"2022-05-01\"` Returns ------- DataFrame DataFrame with observations inside the selected time window only \"\"\" check_columns ( df = measurement , required_columns = [ \"measurement_date\" ], df_name = \"measurment\" ) measurement . measurement_date = measurement . measurement_date . astype ( \"datetime64[ns]\" ) measurement . dropna ( subset = [ \"measurement_date\" ], inplace = True ) if start_date : measurement = measurement [ measurement [ \"measurement_date\" ] >= start_date ] if end_date : measurement = measurement [ measurement [ \"measurement_date\" ] <= end_date ] return measurement","title":"filter_measurement_by_date()"},{"location":"reference/biology/utils/process_measurement/#eds_scikit.biology.utils.process_measurement.tag_measurement_anomaly","text":"tag_measurement_anomaly ( measurement : DataFrame ) -> DataFrame PARAMETER DESCRIPTION measurement DataFrame to filter TYPE: DataFrame start_date EXAMPLE : \"2019-05-01\" TYPE: datetime , optional end_date EXAMPLE : \"2022-05-01\" TYPE: datetime , optional Source code in eds_scikit/biology/utils/process_measurement.py 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 def tag_measurement_anomaly ( measurement : DataFrame ) -> DataFrame : \"\"\" Parameters ---------- measurement : DataFrame DataFrame to filter start_date : datetime, optional **EXAMPLE**: `\"2019-05-01\"` end_date : datetime, optional **EXAMPLE**: `\"2022-05-01\"` Returns ------- \"\"\" measurement [ \"range_high_anomaly\" ] = ( ~ measurement . range_high . isna ()) & ( measurement [ \"value_as_number\" ] > measurement [ \"range_high\" ] ) measurement [ \"range_low_anomaly\" ] = ( ~ measurement . range_low . isna ()) & ( measurement [ \"value_as_number\" ] < measurement [ \"range_low\" ] ) return measurement","title":"tag_measurement_anomaly()"},{"location":"reference/biology/utils/process_measurement/#eds_scikit.biology.utils.process_measurement.convert_measurement_units","text":"convert_measurement_units ( measurement : DataFrame , concepts_sets : List [ ConceptsSet ]) -> DataFrame Add value_as_number_normalized, unit_source_value_normalized and factor columns to measurement dataframe based on concepts_sets and units. PARAMETER DESCRIPTION measurement TYPE: DataFrame concepts_sets TYPE: List [ ConceptsSet ] RETURNS DESCRIPTION DataFrame Measurement with added columns value_as_number_normalized, unit_source_value_normalized and factor. Source code in eds_scikit/biology/utils/process_measurement.py 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 def convert_measurement_units ( measurement : DataFrame , concepts_sets : List [ ConceptsSet ] ) -> DataFrame : \"\"\"Add value_as_number_normalized, unit_source_value_normalized and factor columns to measurement dataframe based on concepts_sets and units. Parameters ---------- measurement : DataFrame concepts_sets : List[ConceptsSet] Returns ------- DataFrame Measurement with added columns value_as_number_normalized, unit_source_value_normalized and factor. \"\"\" if is_koalas ( measurement ): measurement = cache ( measurement ) measurement . shape conversion_table = to ( \"koalas\" , get_conversion_table ( measurement , concepts_sets ) ) else : conversion_table = get_conversion_table ( measurement , concepts_sets ) measurement = measurement . merge ( conversion_table , on = [ \"concept_set\" , \"unit_source_value\" ] ) measurement [ \"value_as_number_normalized\" ] = ( measurement [ \"value_as_number\" ] * measurement [ \"factor\" ] ) return measurement","title":"convert_measurement_units()"},{"location":"reference/biology/utils/process_measurement/#eds_scikit.biology.utils.process_measurement.get_conversion_table","text":"get_conversion_table ( measurement : DataFrame , concepts_sets : List [ ConceptsSet ]) -> DataFrame Given measurement dataframe and list of concepts_sets output conversion table to be merged with measurement. PARAMETER DESCRIPTION measurement TYPE: DataFrame concepts_sets TYPE: List [ ConceptsSet ] RETURNS DESCRIPTION DataFrame Conversion table to be merged with measurement Source code in eds_scikit/biology/utils/process_measurement.py 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 def get_conversion_table ( measurement : DataFrame , concepts_sets : List [ ConceptsSet ] ) -> DataFrame : \"\"\"Given measurement dataframe and list of concepts_sets output conversion table to be merged with measurement. Parameters ---------- measurement : DataFrame concepts_sets : List[ConceptsSet] Returns ------- DataFrame Conversion table to be merged with measurement \"\"\" conversion_table = ( measurement . groupby ( \"concept_set\" )[ \"unit_source_value\" ] . unique () . explode () . to_frame () . reset_index () ) conversion_table = to ( \"pandas\" , conversion_table ) conversion_table [ \"unit_source_value_normalized\" ] = conversion_table [ \"unit_source_value\" ] conversion_table [ \"factor\" ] = conversion_table . apply ( lambda x : 1 if x . unit_source_value_normalized else 0 , axis = 1 ) for concept_set in concepts_sets : unit_source_value_normalized = concept_set . units . target_unit conversion_table . loc [ conversion_table . concept_set == concept_set . name , \"unit_source_value_normalized\" , ] = conversion_table . apply ( lambda x : unit_source_value_normalized if concept_set . units . can_be_converted ( x . unit_source_value , unit_source_value_normalized ) else concept_set . units . get_unit_base ( x . unit_source_value ), axis = 1 , ) conversion_table . loc [ conversion_table . concept_set == concept_set . name , \"factor\" ] = conversion_table . apply ( lambda x : concept_set . units . convert_unit ( x . unit_source_value , x . unit_source_value_normalized ), axis = 1 , ) conversion_table = conversion_table . fillna ( 1 ) return conversion_table","title":"get_conversion_table()"},{"location":"reference/biology/utils/process_units/","text":"eds_scikit.biology.utils.process_units","title":"process_units"},{"location":"reference/biology/utils/process_units/#eds_scikitbiologyutilsprocess_units","text":"","title":"eds_scikit.biology.utils.process_units"},{"location":"reference/biology/viz/","text":"eds_scikit.biology.viz","title":"`eds_scikit.biology.viz`"},{"location":"reference/biology/viz/#eds_scikitbiologyviz","text":"","title":"eds_scikit.biology.viz"},{"location":"reference/biology/viz/aggregate/","text":"eds_scikit.biology.viz.aggregate aggregate_measurement aggregate_measurement ( measurement : DataFrame , stats_only : bool , overall_only : bool , value_column : str , unit_column : str , category_columns = [], debug = False ) Aggregates measurement dataframe in three descriptive and synthetic dataframe : - measurement_stats - measurement_volumetry - measurement_distribution Useful function before plotting. PARAMETER DESCRIPTION measurement description TYPE: DataFrame stats_only description TYPE: bool overall_only description TYPE: bool category_columns description , by default [] TYPE: list , optional DEFAULT: [] RETURNS DESCRIPTION _type_ description Source code in eds_scikit/biology/viz/aggregate.py 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 def aggregate_measurement ( measurement : DataFrame , stats_only : bool , overall_only : bool , value_column : str , unit_column : str , category_columns = [], debug = False , ): \"\"\"Aggregates measurement dataframe in three descriptive and synthetic dataframe : - measurement_stats - measurement_volumetry - measurement_distribution Useful function before plotting. Parameters ---------- measurement : DataFrame _description_ stats_only : bool _description_ overall_only : bool _description_ category_columns : list, optional _description_, by default [] Returns ------- _type_ _description_ \"\"\" check_columns ( df = measurement , required_columns = [ \"measurement_id\" , unit_column , \"measurement_date\" , value_column , ] + category_columns , df_name = \"measurement\" , ) measurement . shape # Truncate date measurement [ \"measurement_month\" ] = ( measurement [ \"measurement_date\" ] . astype ( \"datetime64\" ) . dt . strftime ( \"%Y-%m\" ) ) measurement = measurement . drop ( columns = [ \"measurement_date\" ]) # Filter measurement with missing values filtered_measurement , missing_value = filter_missing_values ( measurement ) # Compute measurement statistics by code measurement_stats = _describe_measurement_by_code ( filtered_measurement , overall_only , value_column , unit_column , category_columns , debug , ) if stats_only : return { \"measurement_stats\" : measurement_stats } # Count measurement by care_site and by code per each month measurement_volumetry = _count_measurement_by_category_and_code_per_month ( filtered_measurement , missing_value , value_column , unit_column , category_columns , debug , ) # Bin measurement values by care_site and by code measurement_distribution = _bin_measurement_value_by_category_and_code ( filtered_measurement , value_column , unit_column , category_columns , debug ) return { \"measurement_stats\" : measurement_stats , \"measurement_volumetry\" : measurement_volumetry , \"measurement_distribution\" : measurement_distribution , } add_mad_minmax add_mad_minmax ( measurement : DataFrame , category_cols : List [ str ], value_column : str = 'value_as_number' , unit_column : str = 'unit_source_value' ) -> DataFrame Add min_value, max_value column to measurement based on MAD criteria. PARAMETER DESCRIPTION measurement measurement dataframe TYPE: DataFrame category_cols measurement category columns to perform the groupby on when computing MAD TYPE: List [ str ] value_column measurement value column on which MAD will be computed TYPE: str DEFAULT: 'value_as_number' RETURNS DESCRIPTION DataFrame measurement dataframe with added columns min_value, max_value Source code in eds_scikit/biology/viz/aggregate.py 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 def add_mad_minmax ( measurement : DataFrame , category_cols : List [ str ], value_column : str = \"value_as_number\" , unit_column : str = \"unit_source_value\" , ) -> DataFrame : \"\"\"Add min_value, max_value column to measurement based on MAD criteria. Parameters ---------- measurement : DataFrame measurement dataframe category_cols : List[str] measurement category columns to perform the groupby on when computing MAD value_column : str measurement value column on which MAD will be computed Returns ------- DataFrame measurement dataframe with added columns min_value, max_value \"\"\" measurement_median = ( measurement [ category_cols + [ value_column ]] . groupby ( category_cols , as_index = False , dropna = False , ) . median () . rename ( columns = { value_column : \"median\" }) ) # Add median column to the measurement table measurement_median = measurement_median . merge ( measurement [ category_cols + [ value_column , ] ], on = category_cols , ) # Compute median deviation for each measurement measurement_median [ \"median_deviation\" ] = abs ( measurement_median [ \"median\" ] - measurement_median [ value_column ] ) # Compute MAD per care site and code measurement_mad = ( measurement_median [ category_cols + [ \"median\" , \"median_deviation\" , ] ] . groupby ( category_cols + [ \"median\" , ], as_index = False , dropna = False , ) . median () . rename ( columns = { \"median_deviation\" : \"MAD\" }) ) measurement_mad [ \"MAD\" ] = 1.48 * measurement_mad [ \"MAD\" ] # Add MAD column to the measurement table measurement = measurement_mad . merge ( measurement , on = category_cols , ) # Compute binned value measurement [ \"max_value\" ] = measurement [ \"median\" ] + 4 * measurement [ \"MAD\" ] measurement [ \"min_value\" ] = measurement [ \"median\" ] - 4 * measurement [ \"MAD\" ] return measurement","title":"aggregate"},{"location":"reference/biology/viz/aggregate/#eds_scikitbiologyvizaggregate","text":"","title":"eds_scikit.biology.viz.aggregate"},{"location":"reference/biology/viz/aggregate/#eds_scikit.biology.viz.aggregate.aggregate_measurement","text":"aggregate_measurement ( measurement : DataFrame , stats_only : bool , overall_only : bool , value_column : str , unit_column : str , category_columns = [], debug = False ) Aggregates measurement dataframe in three descriptive and synthetic dataframe : - measurement_stats - measurement_volumetry - measurement_distribution Useful function before plotting. PARAMETER DESCRIPTION measurement description TYPE: DataFrame stats_only description TYPE: bool overall_only description TYPE: bool category_columns description , by default [] TYPE: list , optional DEFAULT: [] RETURNS DESCRIPTION _type_ description Source code in eds_scikit/biology/viz/aggregate.py 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 def aggregate_measurement ( measurement : DataFrame , stats_only : bool , overall_only : bool , value_column : str , unit_column : str , category_columns = [], debug = False , ): \"\"\"Aggregates measurement dataframe in three descriptive and synthetic dataframe : - measurement_stats - measurement_volumetry - measurement_distribution Useful function before plotting. Parameters ---------- measurement : DataFrame _description_ stats_only : bool _description_ overall_only : bool _description_ category_columns : list, optional _description_, by default [] Returns ------- _type_ _description_ \"\"\" check_columns ( df = measurement , required_columns = [ \"measurement_id\" , unit_column , \"measurement_date\" , value_column , ] + category_columns , df_name = \"measurement\" , ) measurement . shape # Truncate date measurement [ \"measurement_month\" ] = ( measurement [ \"measurement_date\" ] . astype ( \"datetime64\" ) . dt . strftime ( \"%Y-%m\" ) ) measurement = measurement . drop ( columns = [ \"measurement_date\" ]) # Filter measurement with missing values filtered_measurement , missing_value = filter_missing_values ( measurement ) # Compute measurement statistics by code measurement_stats = _describe_measurement_by_code ( filtered_measurement , overall_only , value_column , unit_column , category_columns , debug , ) if stats_only : return { \"measurement_stats\" : measurement_stats } # Count measurement by care_site and by code per each month measurement_volumetry = _count_measurement_by_category_and_code_per_month ( filtered_measurement , missing_value , value_column , unit_column , category_columns , debug , ) # Bin measurement values by care_site and by code measurement_distribution = _bin_measurement_value_by_category_and_code ( filtered_measurement , value_column , unit_column , category_columns , debug ) return { \"measurement_stats\" : measurement_stats , \"measurement_volumetry\" : measurement_volumetry , \"measurement_distribution\" : measurement_distribution , }","title":"aggregate_measurement()"},{"location":"reference/biology/viz/aggregate/#eds_scikit.biology.viz.aggregate.add_mad_minmax","text":"add_mad_minmax ( measurement : DataFrame , category_cols : List [ str ], value_column : str = 'value_as_number' , unit_column : str = 'unit_source_value' ) -> DataFrame Add min_value, max_value column to measurement based on MAD criteria. PARAMETER DESCRIPTION measurement measurement dataframe TYPE: DataFrame category_cols measurement category columns to perform the groupby on when computing MAD TYPE: List [ str ] value_column measurement value column on which MAD will be computed TYPE: str DEFAULT: 'value_as_number' RETURNS DESCRIPTION DataFrame measurement dataframe with added columns min_value, max_value Source code in eds_scikit/biology/viz/aggregate.py 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 def add_mad_minmax ( measurement : DataFrame , category_cols : List [ str ], value_column : str = \"value_as_number\" , unit_column : str = \"unit_source_value\" , ) -> DataFrame : \"\"\"Add min_value, max_value column to measurement based on MAD criteria. Parameters ---------- measurement : DataFrame measurement dataframe category_cols : List[str] measurement category columns to perform the groupby on when computing MAD value_column : str measurement value column on which MAD will be computed Returns ------- DataFrame measurement dataframe with added columns min_value, max_value \"\"\" measurement_median = ( measurement [ category_cols + [ value_column ]] . groupby ( category_cols , as_index = False , dropna = False , ) . median () . rename ( columns = { value_column : \"median\" }) ) # Add median column to the measurement table measurement_median = measurement_median . merge ( measurement [ category_cols + [ value_column , ] ], on = category_cols , ) # Compute median deviation for each measurement measurement_median [ \"median_deviation\" ] = abs ( measurement_median [ \"median\" ] - measurement_median [ value_column ] ) # Compute MAD per care site and code measurement_mad = ( measurement_median [ category_cols + [ \"median\" , \"median_deviation\" , ] ] . groupby ( category_cols + [ \"median\" , ], as_index = False , dropna = False , ) . median () . rename ( columns = { \"median_deviation\" : \"MAD\" }) ) measurement_mad [ \"MAD\" ] = 1.48 * measurement_mad [ \"MAD\" ] # Add MAD column to the measurement table measurement = measurement_mad . merge ( measurement , on = category_cols , ) # Compute binned value measurement [ \"max_value\" ] = measurement [ \"median\" ] + 4 * measurement [ \"MAD\" ] measurement [ \"min_value\" ] = measurement [ \"median\" ] - 4 * measurement [ \"MAD\" ] return measurement","title":"add_mad_minmax()"},{"location":"reference/biology/viz/plot/","text":"eds_scikit.biology.viz.plot plot_concepts_set plot_concepts_set ( concepts_set_name : str , source_path : str = 'Biology_summary' ) -> Union [ alt . ConcatChart , pd . DataFrame ] Plot and save a summary table and 2 interactive dashboards. For more details, have a look on the visualization section PARAMETER DESCRIPTION concepts_set_name Name of the concepts-set to plot TYPE: str source_path Name of the folder with aggregated data where the plots will be saved TYPE: str , optional DEFAULT: 'Biology_summary' RETURNS DESCRIPTION List [ alt . ConcatChart , pd . DataFrame ] Altair plots describing the volumetric and the distribution properties of your biological data along with a pandas DataFrame with a statistical summary Source code in eds_scikit/biology/viz/plot.py 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 def plot_concepts_set ( concepts_set_name : str , source_path : str = \"Biology_summary\" , ) -> Union [ alt . ConcatChart , pd . DataFrame ]: \"\"\"Plot and save a summary table and 2 interactive dashboards. For more details, have a look on the [visualization section][visualization] Parameters ---------- concepts_set_name : str Name of the concepts-set to plot source_path : str, optional Name of the folder with aggregated data where the plots will be saved Returns ------- List[alt.ConcatChart, pd.DataFrame] Altair plots describing the volumetric and the distribution properties of your biological data along with a pandas DataFrame with a statistical summary \"\"\" if os . path . isdir ( \" {} / {} \" . format ( source_path , concepts_set_name )): if os . path . isfile ( \" {} / {} /measurement_stats.pkl\" . format ( source_path , concepts_set_name ) ): measurement_stats = pd . read_pickle ( \" {} / {} /measurement_stats.pkl\" . format ( source_path , concepts_set_name ) ) _save_and_display_table ( measurement_stats , source_path , concepts_set_name ) if os . path . isfile ( \" {} / {} /measurement_volumetry.pkl\" . format ( source_path , concepts_set_name ) ): measurement_volumetry = pd . read_pickle ( \" {} / {} /measurement_volumetry.pkl\" . format ( source_path , concepts_set_name ) ) interactive_volumetry = plot_interactive_volumetry ( measurement_volumetry , ) _save_and_display_chart ( interactive_volumetry , source_path , concepts_set_name , \"interactive_volumetry\" , ) if os . path . isfile ( \" {} / {} /measurement_distribution.pkl\" . format ( source_path , concepts_set_name ) ): measurement_distribution = pd . read_pickle ( \" {} / {} /measurement_distribution.pkl\" . format ( source_path , concepts_set_name ) ) interactive_distribution = plot_interactive_distribution ( measurement_distribution , ) _save_and_display_chart ( interactive_distribution , source_path , concepts_set_name , \"interactive_distribution\" , ) else : logger . error ( \"The folder {} has not been found\" , source_path , ) raise FileNotFoundError","title":"plot"},{"location":"reference/biology/viz/plot/#eds_scikitbiologyvizplot","text":"","title":"eds_scikit.biology.viz.plot"},{"location":"reference/biology/viz/plot/#eds_scikit.biology.viz.plot.plot_concepts_set","text":"plot_concepts_set ( concepts_set_name : str , source_path : str = 'Biology_summary' ) -> Union [ alt . ConcatChart , pd . DataFrame ] Plot and save a summary table and 2 interactive dashboards. For more details, have a look on the visualization section PARAMETER DESCRIPTION concepts_set_name Name of the concepts-set to plot TYPE: str source_path Name of the folder with aggregated data where the plots will be saved TYPE: str , optional DEFAULT: 'Biology_summary' RETURNS DESCRIPTION List [ alt . ConcatChart , pd . DataFrame ] Altair plots describing the volumetric and the distribution properties of your biological data along with a pandas DataFrame with a statistical summary Source code in eds_scikit/biology/viz/plot.py 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 def plot_concepts_set ( concepts_set_name : str , source_path : str = \"Biology_summary\" , ) -> Union [ alt . ConcatChart , pd . DataFrame ]: \"\"\"Plot and save a summary table and 2 interactive dashboards. For more details, have a look on the [visualization section][visualization] Parameters ---------- concepts_set_name : str Name of the concepts-set to plot source_path : str, optional Name of the folder with aggregated data where the plots will be saved Returns ------- List[alt.ConcatChart, pd.DataFrame] Altair plots describing the volumetric and the distribution properties of your biological data along with a pandas DataFrame with a statistical summary \"\"\" if os . path . isdir ( \" {} / {} \" . format ( source_path , concepts_set_name )): if os . path . isfile ( \" {} / {} /measurement_stats.pkl\" . format ( source_path , concepts_set_name ) ): measurement_stats = pd . read_pickle ( \" {} / {} /measurement_stats.pkl\" . format ( source_path , concepts_set_name ) ) _save_and_display_table ( measurement_stats , source_path , concepts_set_name ) if os . path . isfile ( \" {} / {} /measurement_volumetry.pkl\" . format ( source_path , concepts_set_name ) ): measurement_volumetry = pd . read_pickle ( \" {} / {} /measurement_volumetry.pkl\" . format ( source_path , concepts_set_name ) ) interactive_volumetry = plot_interactive_volumetry ( measurement_volumetry , ) _save_and_display_chart ( interactive_volumetry , source_path , concepts_set_name , \"interactive_volumetry\" , ) if os . path . isfile ( \" {} / {} /measurement_distribution.pkl\" . format ( source_path , concepts_set_name ) ): measurement_distribution = pd . read_pickle ( \" {} / {} /measurement_distribution.pkl\" . format ( source_path , concepts_set_name ) ) interactive_distribution = plot_interactive_distribution ( measurement_distribution , ) _save_and_display_chart ( interactive_distribution , source_path , concepts_set_name , \"interactive_distribution\" , ) else : logger . error ( \"The folder {} has not been found\" , source_path , ) raise FileNotFoundError","title":"plot_concepts_set()"},{"location":"reference/biology/viz/stats_summary/","text":"eds_scikit.biology.viz.stats_summary measurement_values_summary measurement_values_summary ( measurement : DataFrame , category_cols : List [ str ] = [ 'concept_set' , 'GLIMS_ANABIO_concept_code' ], value_column : str = 'value_as_number' , unit_column : str = 'unit_source_value' ) -> DataFrame Compute measurement values and units summary by category_cols. PARAMETER DESCRIPTION measurement measurement dataframe TYPE: DataFrame category_cols columns on which to groupby the summary, by default [\"concept_set\", \"GLIMS_ANABIO_concept_code\",] TYPE: List [ str ], optional DEFAULT: ['concept_set', 'GLIMS_ANABIO_concept_code'] value_column value column to summarize, by default \"value_as_number\" but can be value_as_number_normalized if units conversion is applied. TYPE: str , optional DEFAULT: 'value_as_number' unit_column units column to summarize, by default \"unit_source_value\" but can be unit_source_value_normalized if units conversion is applied. TYPE: str , optional DEFAULT: 'unit_source_value' RETURNS DESCRIPTION DataFrame statistic summary dataframe Source code in eds_scikit/biology/viz/stats_summary.py 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 def measurement_values_summary ( measurement : DataFrame , category_cols : List [ str ] = [ \"concept_set\" , \"GLIMS_ANABIO_concept_code\" , ], value_column : str = \"value_as_number\" , unit_column : str = \"unit_source_value\" , ) -> DataFrame : \"\"\"Compute measurement values and units summary by category_cols. Parameters ---------- measurement : DataFrame measurement dataframe category_cols : List[str], optional columns on which to groupby the summary, by default [\"concept_set\", \"GLIMS_ANABIO_concept_code\",] value_column : str, optional value column to summarize, by default \"value_as_number\" but can be value_as_number_normalized if units conversion is applied. unit_column : str, optional units column to summarize, by default \"unit_source_value\" but can be unit_source_value_normalized if units conversion is applied. Returns ------- DataFrame statistic summary dataframe \"\"\" measurement . shape no_units = ( measurement [ unit_column ] == \"non renseigne\" ) | ( measurement [ unit_column ] == \"Unkown\" ) stats_summary = ( measurement [ no_units ] . groupby ( category_cols ) . agg ( no_units = ( \"measurement_id\" , \"count\" )) . reset_index () ) # Count measurements measurement_count = ( measurement . groupby ([ * category_cols , unit_column ]) . agg ( measurement_count = ( \"measurement_id\" , \"count\" )) . reset_index () ) stats_summary = stats_summary . merge ( measurement_count , how = \"right\" , on = category_cols ) # Describe stats measurements measurement_stats = ( measurement [ ~ no_units ] . groupby ([ * category_cols , unit_column ])[[ value_column ]] . describe () ) measurement_stats . columns = [ \"_\" . join ( map ( str , col )) for col in measurement_stats . columns ] measurement_stats = measurement_stats . reset_index () stats_summary = measurement_stats . merge ( stats_summary , how = \"left\" , on = ([ * category_cols , unit_column ]) ) # Count anomalies occurrences_to_count = { \"range_high_anomaly_count\" : measurement [ ~ no_units ] . range_high_anomaly , \"range_low_anomaly_count\" : measurement [ ~ no_units ] . range_low_anomaly , } for key , to_count in occurrences_to_count . items (): additional_summary = ( measurement [ ~ no_units ][ to_count ] . groupby ([ * category_cols , unit_column ])[[ \"measurement_id\" ]] . count () . rename ( columns = { \"measurement_id\" : key }) . reset_index () ) stats_summary = stats_summary . merge ( additional_summary , how = \"left\" , on = [ * category_cols , unit_column ] ) stats_summary = stats_summary . fillna ( 0 ) stats_summary = stats_summary . set_index ( [ * category_cols , \"no_units\" , unit_column ] ) . sort_index () stats_summary = stats_summary [ [ * stats_summary . columns [:: - 1 ][: 3 ], * stats_summary . columns [: - 3 ]] ] stats_summary = to ( \"pandas\" , stats_summary ) return stats_summary","title":"stats_summary"},{"location":"reference/biology/viz/stats_summary/#eds_scikitbiologyvizstats_summary","text":"","title":"eds_scikit.biology.viz.stats_summary"},{"location":"reference/biology/viz/stats_summary/#eds_scikit.biology.viz.stats_summary.measurement_values_summary","text":"measurement_values_summary ( measurement : DataFrame , category_cols : List [ str ] = [ 'concept_set' , 'GLIMS_ANABIO_concept_code' ], value_column : str = 'value_as_number' , unit_column : str = 'unit_source_value' ) -> DataFrame Compute measurement values and units summary by category_cols. PARAMETER DESCRIPTION measurement measurement dataframe TYPE: DataFrame category_cols columns on which to groupby the summary, by default [\"concept_set\", \"GLIMS_ANABIO_concept_code\",] TYPE: List [ str ], optional DEFAULT: ['concept_set', 'GLIMS_ANABIO_concept_code'] value_column value column to summarize, by default \"value_as_number\" but can be value_as_number_normalized if units conversion is applied. TYPE: str , optional DEFAULT: 'value_as_number' unit_column units column to summarize, by default \"unit_source_value\" but can be unit_source_value_normalized if units conversion is applied. TYPE: str , optional DEFAULT: 'unit_source_value' RETURNS DESCRIPTION DataFrame statistic summary dataframe Source code in eds_scikit/biology/viz/stats_summary.py 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 def measurement_values_summary ( measurement : DataFrame , category_cols : List [ str ] = [ \"concept_set\" , \"GLIMS_ANABIO_concept_code\" , ], value_column : str = \"value_as_number\" , unit_column : str = \"unit_source_value\" , ) -> DataFrame : \"\"\"Compute measurement values and units summary by category_cols. Parameters ---------- measurement : DataFrame measurement dataframe category_cols : List[str], optional columns on which to groupby the summary, by default [\"concept_set\", \"GLIMS_ANABIO_concept_code\",] value_column : str, optional value column to summarize, by default \"value_as_number\" but can be value_as_number_normalized if units conversion is applied. unit_column : str, optional units column to summarize, by default \"unit_source_value\" but can be unit_source_value_normalized if units conversion is applied. Returns ------- DataFrame statistic summary dataframe \"\"\" measurement . shape no_units = ( measurement [ unit_column ] == \"non renseigne\" ) | ( measurement [ unit_column ] == \"Unkown\" ) stats_summary = ( measurement [ no_units ] . groupby ( category_cols ) . agg ( no_units = ( \"measurement_id\" , \"count\" )) . reset_index () ) # Count measurements measurement_count = ( measurement . groupby ([ * category_cols , unit_column ]) . agg ( measurement_count = ( \"measurement_id\" , \"count\" )) . reset_index () ) stats_summary = stats_summary . merge ( measurement_count , how = \"right\" , on = category_cols ) # Describe stats measurements measurement_stats = ( measurement [ ~ no_units ] . groupby ([ * category_cols , unit_column ])[[ value_column ]] . describe () ) measurement_stats . columns = [ \"_\" . join ( map ( str , col )) for col in measurement_stats . columns ] measurement_stats = measurement_stats . reset_index () stats_summary = measurement_stats . merge ( stats_summary , how = \"left\" , on = ([ * category_cols , unit_column ]) ) # Count anomalies occurrences_to_count = { \"range_high_anomaly_count\" : measurement [ ~ no_units ] . range_high_anomaly , \"range_low_anomaly_count\" : measurement [ ~ no_units ] . range_low_anomaly , } for key , to_count in occurrences_to_count . items (): additional_summary = ( measurement [ ~ no_units ][ to_count ] . groupby ([ * category_cols , unit_column ])[[ \"measurement_id\" ]] . count () . rename ( columns = { \"measurement_id\" : key }) . reset_index () ) stats_summary = stats_summary . merge ( additional_summary , how = \"left\" , on = [ * category_cols , unit_column ] ) stats_summary = stats_summary . fillna ( 0 ) stats_summary = stats_summary . set_index ( [ * category_cols , \"no_units\" , unit_column ] ) . sort_index () stats_summary = stats_summary [ [ * stats_summary . columns [:: - 1 ][: 3 ], * stats_summary . columns [: - 3 ]] ] stats_summary = to ( \"pandas\" , stats_summary ) return stats_summary","title":"measurement_values_summary()"},{"location":"reference/biology/viz/wrapper/","text":"eds_scikit.biology.viz.wrapper plot_biology_summary plot_biology_summary ( measurement : DataFrame , value_column : str = 'value_as_number' , unit_column : str = 'unit_source_value' , save_folder_path : str = 'Biology_summary' , stats_only : bool = False , terminologies : List [ str ] = None , debug : bool = False ) -> Union [ alt . ConcatChart , pd . DataFrame ] Aggregate measurements, create plots and saves all the concepts-sets in folder. PARAMETER DESCRIPTION data Instantiated HiveData , PostgresData or PandasData TYPE: Data save_folder_path Name of the folder where the plots will be saved TYPE: str , optional DEFAULT: 'Biology_summary' stats_only If True , it will only aggregate the data for the summary table . TYPE: bool , optional DEFAULT: False terminologies biology summary only on terminologies codes columns TYPE: List [ str ], optional DEFAULT: None value_column value column for distribution summary plot TYPE: str , optional DEFAULT: 'value_as_number' debug If True , info log will de displayed to follow aggregation steps TYPE: bool , optional DEFAULT: False RETURNS DESCRIPTION List [ alt . ConcatChart , pd . DataFrame ] Altair plots describing the volumetric and the distribution properties of your biological data along with a pandas DataFrame with a statistical summary Source code in eds_scikit/biology/viz/wrapper.py 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 def plot_biology_summary ( measurement : DataFrame , value_column : str = \"value_as_number\" , unit_column : str = \"unit_source_value\" , save_folder_path : str = \"Biology_summary\" , stats_only : bool = False , terminologies : List [ str ] = None , debug : bool = False , ) -> Union [ alt . ConcatChart , pd . DataFrame ]: \"\"\" Aggregate measurements, create plots and saves all the concepts-sets in folder. Parameters ---------- data : Data Instantiated [``HiveData``][eds_scikit.io.hive.HiveData], [``PostgresData``][eds_scikit.io.postgres.PostgresData] or [``PandasData``][eds_scikit.io.files.PandasData] save_folder_path : str, optional Name of the folder where the plots will be saved stats_only : bool, optional If ``True``, it will only aggregate the data for the [summary table][summary-table]. terminologies : List[str], optional biology summary only on terminologies codes columns value_column : str, optional value column for distribution summary plot debug : bool, optional If ``True``, info log will de displayed to follow aggregation steps Returns ------- List[alt.ConcatChart, pd.DataFrame] Altair plots describing the volumetric and the distribution properties of your biological data along with a pandas DataFrame with a statistical summary \"\"\" if not value_column : raise ValueError ( \"Must give a 'value_column' parameter. By default, use value_as_number. Or value_as_number_normalized if exists.\" ) if not unit_column : raise ValueError ( \"Must give a 'unit_column' parameter. By default, use unit_source_value. Or unit_source_value_normalized if exists.\" ) if not os . path . isdir ( save_folder_path ): os . mkdir ( save_folder_path ) logger . info ( \" {} folder has been created.\" , save_folder_path ) if terminologies : measurement = measurement . drop ( columns = [ f \" { col } _concept_code\" for col in terminologies ] ) tables_agg = aggregate_measurement ( measurement = measurement , value_column = value_column , unit_column = unit_column , stats_only = stats_only , overall_only = stats_only , category_columns = [ \"concept_set\" , \"care_site_short_name\" ], debug = debug , ) table_names = list ( tables_agg . keys ()) concept_sets_names = tables_agg [ table_names [ 0 ]] . concept_set . unique () for concept_set_name in concept_sets_names : concepts_set_path = \" {} / {} \" . format ( save_folder_path , concept_set_name ) rmtree ( concepts_set_path , ignore_errors = True ) os . mkdir ( concepts_set_path ) logger . info ( \" {} / {} folder has been created.\" , save_folder_path , concept_set_name , ) for table_name in table_names : table = tables_agg [ table_name ] . query ( \"concept_set == @concept_set_name\" ) table . to_pickle ( \" {} / {} / {} .pkl\" . format ( save_folder_path , concept_set_name , table_name ) ) logger . info ( \" {} has been processed and saved in {} / {} folder.\" , concept_set_name , save_folder_path , concept_set_name , ) plot_concepts_set ( concepts_set_name = concept_set_name , source_path = save_folder_path )","title":"wrapper"},{"location":"reference/biology/viz/wrapper/#eds_scikitbiologyvizwrapper","text":"","title":"eds_scikit.biology.viz.wrapper"},{"location":"reference/biology/viz/wrapper/#eds_scikit.biology.viz.wrapper.plot_biology_summary","text":"plot_biology_summary ( measurement : DataFrame , value_column : str = 'value_as_number' , unit_column : str = 'unit_source_value' , save_folder_path : str = 'Biology_summary' , stats_only : bool = False , terminologies : List [ str ] = None , debug : bool = False ) -> Union [ alt . ConcatChart , pd . DataFrame ] Aggregate measurements, create plots and saves all the concepts-sets in folder. PARAMETER DESCRIPTION data Instantiated HiveData , PostgresData or PandasData TYPE: Data save_folder_path Name of the folder where the plots will be saved TYPE: str , optional DEFAULT: 'Biology_summary' stats_only If True , it will only aggregate the data for the summary table . TYPE: bool , optional DEFAULT: False terminologies biology summary only on terminologies codes columns TYPE: List [ str ], optional DEFAULT: None value_column value column for distribution summary plot TYPE: str , optional DEFAULT: 'value_as_number' debug If True , info log will de displayed to follow aggregation steps TYPE: bool , optional DEFAULT: False RETURNS DESCRIPTION List [ alt . ConcatChart , pd . DataFrame ] Altair plots describing the volumetric and the distribution properties of your biological data along with a pandas DataFrame with a statistical summary Source code in eds_scikit/biology/viz/wrapper.py 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 def plot_biology_summary ( measurement : DataFrame , value_column : str = \"value_as_number\" , unit_column : str = \"unit_source_value\" , save_folder_path : str = \"Biology_summary\" , stats_only : bool = False , terminologies : List [ str ] = None , debug : bool = False , ) -> Union [ alt . ConcatChart , pd . DataFrame ]: \"\"\" Aggregate measurements, create plots and saves all the concepts-sets in folder. Parameters ---------- data : Data Instantiated [``HiveData``][eds_scikit.io.hive.HiveData], [``PostgresData``][eds_scikit.io.postgres.PostgresData] or [``PandasData``][eds_scikit.io.files.PandasData] save_folder_path : str, optional Name of the folder where the plots will be saved stats_only : bool, optional If ``True``, it will only aggregate the data for the [summary table][summary-table]. terminologies : List[str], optional biology summary only on terminologies codes columns value_column : str, optional value column for distribution summary plot debug : bool, optional If ``True``, info log will de displayed to follow aggregation steps Returns ------- List[alt.ConcatChart, pd.DataFrame] Altair plots describing the volumetric and the distribution properties of your biological data along with a pandas DataFrame with a statistical summary \"\"\" if not value_column : raise ValueError ( \"Must give a 'value_column' parameter. By default, use value_as_number. Or value_as_number_normalized if exists.\" ) if not unit_column : raise ValueError ( \"Must give a 'unit_column' parameter. By default, use unit_source_value. Or unit_source_value_normalized if exists.\" ) if not os . path . isdir ( save_folder_path ): os . mkdir ( save_folder_path ) logger . info ( \" {} folder has been created.\" , save_folder_path ) if terminologies : measurement = measurement . drop ( columns = [ f \" { col } _concept_code\" for col in terminologies ] ) tables_agg = aggregate_measurement ( measurement = measurement , value_column = value_column , unit_column = unit_column , stats_only = stats_only , overall_only = stats_only , category_columns = [ \"concept_set\" , \"care_site_short_name\" ], debug = debug , ) table_names = list ( tables_agg . keys ()) concept_sets_names = tables_agg [ table_names [ 0 ]] . concept_set . unique () for concept_set_name in concept_sets_names : concepts_set_path = \" {} / {} \" . format ( save_folder_path , concept_set_name ) rmtree ( concepts_set_path , ignore_errors = True ) os . mkdir ( concepts_set_path ) logger . info ( \" {} / {} folder has been created.\" , save_folder_path , concept_set_name , ) for table_name in table_names : table = tables_agg [ table_name ] . query ( \"concept_set == @concept_set_name\" ) table . to_pickle ( \" {} / {} / {} .pkl\" . format ( save_folder_path , concept_set_name , table_name ) ) logger . info ( \" {} has been processed and saved in {} / {} folder.\" , concept_set_name , save_folder_path , concept_set_name , ) plot_concepts_set ( concepts_set_name = concept_set_name , source_path = save_folder_path )","title":"plot_biology_summary()"},{"location":"reference/datasets/","text":"eds_scikit.datasets list_all_synthetics list_all_synthetics () -> List [ str ] Helper to list all available synthetic datasets RETURNS DESCRIPTION List [ str ] List of datasets names Source code in eds_scikit/datasets/__init__.py 59 60 61 62 63 64 65 66 67 68 def list_all_synthetics () -> List [ str ]: \"\"\" Helper to list all available synthetic datasets Returns ------- List[str] List of datasets names \"\"\" return [ func . __name__ for func in __all__ ]","title":"`eds_scikit.datasets`"},{"location":"reference/datasets/#eds_scikitdatasets","text":"","title":"eds_scikit.datasets"},{"location":"reference/datasets/#eds_scikit.datasets.list_all_synthetics","text":"list_all_synthetics () -> List [ str ] Helper to list all available synthetic datasets RETURNS DESCRIPTION List [ str ] List of datasets names Source code in eds_scikit/datasets/__init__.py 59 60 61 62 63 64 65 66 67 68 def list_all_synthetics () -> List [ str ]: \"\"\" Helper to list all available synthetic datasets Returns ------- List[str] List of datasets names \"\"\" return [ func . __name__ for func in __all__ ]","title":"list_all_synthetics()"},{"location":"reference/datasets/generation_scripts/","text":"eds_scikit.datasets.generation_scripts","title":"`eds_scikit.datasets.generation_scripts`"},{"location":"reference/datasets/generation_scripts/#eds_scikitdatasetsgeneration_scripts","text":"","title":"eds_scikit.datasets.generation_scripts"},{"location":"reference/datasets/generation_scripts/care_site_hierarchy/","text":"eds_scikit.datasets.generation_scripts.care_site_hierarchy generate_care_site_hierarchy generate_care_site_hierarchy ( care_site : framework . DataFrame , fact_relationship : framework . DataFrame , care_site_categories : List [ str ]) -> None Generate the care site hierarchy dataset. PARAMETER DESCRIPTION care_site The care_site DataFrame TYPE: framework . DataFrame fact_relationship The fact_relationship DataFrame TYPE: framework . DataFrame care_site_categories A list of care_site_type_source_value to use as categories TYPE: List [ str ] Source code in eds_scikit/datasets/generation_scripts/care_site_hierarchy.py 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 def generate_care_site_hierarchy ( care_site : framework . DataFrame , fact_relationship : framework . DataFrame , care_site_categories : List [ str ], ) -> None : # pragma: no cover \"\"\" Generate the care site hierarchy dataset. Parameters ---------- care_site : framework.DataFrame The `care_site` DataFrame fact_relationship : framework.DataFrame The `fact_relationship` DataFrame care_site_categories : List[str] A list of `care_site_type_source_value` to use as categories \"\"\" care_site = _load_care_site_categories ( care_site , care_site_categories ) relationships = _load_care_site_relationships ( fact_relationship ) care_site = _simplify_care_site_categories ( care_site , relationships ) care_site_hierarchy = hierarchy . build_hierarchy ( care_site , relationships ) care_site_hierarchy = _simplify_care_site_hierarchy ( care_site_hierarchy ) _save_care_site_hierarchy ( care_site_hierarchy , DATASET_FOLDER )","title":"care_site_hierarchy"},{"location":"reference/datasets/generation_scripts/care_site_hierarchy/#eds_scikitdatasetsgeneration_scriptscare_site_hierarchy","text":"","title":"eds_scikit.datasets.generation_scripts.care_site_hierarchy"},{"location":"reference/datasets/generation_scripts/care_site_hierarchy/#eds_scikit.datasets.generation_scripts.care_site_hierarchy.generate_care_site_hierarchy","text":"generate_care_site_hierarchy ( care_site : framework . DataFrame , fact_relationship : framework . DataFrame , care_site_categories : List [ str ]) -> None Generate the care site hierarchy dataset. PARAMETER DESCRIPTION care_site The care_site DataFrame TYPE: framework . DataFrame fact_relationship The fact_relationship DataFrame TYPE: framework . DataFrame care_site_categories A list of care_site_type_source_value to use as categories TYPE: List [ str ] Source code in eds_scikit/datasets/generation_scripts/care_site_hierarchy.py 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 def generate_care_site_hierarchy ( care_site : framework . DataFrame , fact_relationship : framework . DataFrame , care_site_categories : List [ str ], ) -> None : # pragma: no cover \"\"\" Generate the care site hierarchy dataset. Parameters ---------- care_site : framework.DataFrame The `care_site` DataFrame fact_relationship : framework.DataFrame The `fact_relationship` DataFrame care_site_categories : List[str] A list of `care_site_type_source_value` to use as categories \"\"\" care_site = _load_care_site_categories ( care_site , care_site_categories ) relationships = _load_care_site_relationships ( fact_relationship ) care_site = _simplify_care_site_categories ( care_site , relationships ) care_site_hierarchy = hierarchy . build_hierarchy ( care_site , relationships ) care_site_hierarchy = _simplify_care_site_hierarchy ( care_site_hierarchy ) _save_care_site_hierarchy ( care_site_hierarchy , DATASET_FOLDER )","title":"generate_care_site_hierarchy()"},{"location":"reference/datasets/synthetic/","text":"eds_scikit.datasets.synthetic","title":"`eds_scikit.datasets.synthetic`"},{"location":"reference/datasets/synthetic/#eds_scikitdatasetssynthetic","text":"","title":"eds_scikit.datasets.synthetic"},{"location":"reference/datasets/synthetic/base_dataset/","text":"eds_scikit.datasets.synthetic.base_dataset","title":"base_dataset"},{"location":"reference/datasets/synthetic/base_dataset/#eds_scikitdatasetssyntheticbase_dataset","text":"","title":"eds_scikit.datasets.synthetic.base_dataset"},{"location":"reference/datasets/synthetic/biology/","text":"eds_scikit.datasets.synthetic.biology load_biology_data load_biology_data ( n_entity : int = 5 , mean_measurement : int = 10000 , n_care_site : int = 5 , n_person : int = 5 , n_visit_occurrence : int = 5 , units : List [ str ] = [ 'g' , 'g/l' , 'mol' , 's' ], row_status_source_values : List [ str ] = [ 'Valid\u00e9' , 'Discontinu\u00e9' , 'Disponible' , 'Attendu' , 'Confirm\u00e9' , 'Initial' ], t_start : datetime = datetime ( 2017 , 1 , 1 ), t_end : datetime = datetime ( 2022 , 1 , 1 ), seed : int = None ) Create a minimalistic dataset for the bioclean function. RETURNS DESCRIPTION biology_dataset measurement, concept and concept_relationship. TYPE: BiologyDataset, a dataclass comprised of Source code in eds_scikit/datasets/synthetic/biology.py 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 def load_biology_data ( n_entity : int = 5 , mean_measurement : int = 10000 , n_care_site : int = 5 , n_person : int = 5 , n_visit_occurrence : int = 5 , units : List [ str ] = [ \"g\" , \"g/l\" , \"mol\" , \"s\" ], row_status_source_values : List [ str ] = [ \"Valid\u00e9\" , \"Discontinu\u00e9\" , \"Disponible\" , \"Attendu\" , \"Confirm\u00e9\" , \"Initial\" , ], t_start : datetime = datetime ( 2017 , 1 , 1 ), t_end : datetime = datetime ( 2022 , 1 , 1 ), seed : int = None , ): \"\"\" Create a minimalistic dataset for the `bioclean` function. Returns ------- biology_dataset: BiologyDataset, a dataclass comprised of measurement, concept and concept_relationship. \"\"\" if seed : np . random . seed ( seed = seed ) concept , concept_relationship , src_concept_name = _generate_concept ( n_entity = n_entity , units = units ) measurement = _generate_measurement ( t_start = t_start , t_end = t_end , mean_measurement = mean_measurement , units = units , src_concept_name = src_concept_name , n_visit_occurrence = n_visit_occurrence , n_person = n_person , row_status_source_values = row_status_source_values , ) care_site = _generate_care_site ( n_care_site = n_care_site ) visit_occurrence = _generate_visit_occurrence ( n_visit_occurrence = n_visit_occurrence , n_care_site = n_care_site ) return BiologyDataset ( measurement = measurement , concept = concept , concept_relationship = concept_relationship , visit_occurrence = visit_occurrence , care_site = care_site , available_tables = [ \"measurement\" , \"concept\" , \"concept_relationship\" , \"visit_occurrence\" , \"care_site\" , ], t_start = t_start , t_end = t_end , module = \"pandas\" , )","title":"biology"},{"location":"reference/datasets/synthetic/biology/#eds_scikitdatasetssyntheticbiology","text":"","title":"eds_scikit.datasets.synthetic.biology"},{"location":"reference/datasets/synthetic/biology/#eds_scikit.datasets.synthetic.biology.load_biology_data","text":"load_biology_data ( n_entity : int = 5 , mean_measurement : int = 10000 , n_care_site : int = 5 , n_person : int = 5 , n_visit_occurrence : int = 5 , units : List [ str ] = [ 'g' , 'g/l' , 'mol' , 's' ], row_status_source_values : List [ str ] = [ 'Valid\u00e9' , 'Discontinu\u00e9' , 'Disponible' , 'Attendu' , 'Confirm\u00e9' , 'Initial' ], t_start : datetime = datetime ( 2017 , 1 , 1 ), t_end : datetime = datetime ( 2022 , 1 , 1 ), seed : int = None ) Create a minimalistic dataset for the bioclean function. RETURNS DESCRIPTION biology_dataset measurement, concept and concept_relationship. TYPE: BiologyDataset, a dataclass comprised of Source code in eds_scikit/datasets/synthetic/biology.py 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 def load_biology_data ( n_entity : int = 5 , mean_measurement : int = 10000 , n_care_site : int = 5 , n_person : int = 5 , n_visit_occurrence : int = 5 , units : List [ str ] = [ \"g\" , \"g/l\" , \"mol\" , \"s\" ], row_status_source_values : List [ str ] = [ \"Valid\u00e9\" , \"Discontinu\u00e9\" , \"Disponible\" , \"Attendu\" , \"Confirm\u00e9\" , \"Initial\" , ], t_start : datetime = datetime ( 2017 , 1 , 1 ), t_end : datetime = datetime ( 2022 , 1 , 1 ), seed : int = None , ): \"\"\" Create a minimalistic dataset for the `bioclean` function. Returns ------- biology_dataset: BiologyDataset, a dataclass comprised of measurement, concept and concept_relationship. \"\"\" if seed : np . random . seed ( seed = seed ) concept , concept_relationship , src_concept_name = _generate_concept ( n_entity = n_entity , units = units ) measurement = _generate_measurement ( t_start = t_start , t_end = t_end , mean_measurement = mean_measurement , units = units , src_concept_name = src_concept_name , n_visit_occurrence = n_visit_occurrence , n_person = n_person , row_status_source_values = row_status_source_values , ) care_site = _generate_care_site ( n_care_site = n_care_site ) visit_occurrence = _generate_visit_occurrence ( n_visit_occurrence = n_visit_occurrence , n_care_site = n_care_site ) return BiologyDataset ( measurement = measurement , concept = concept , concept_relationship = concept_relationship , visit_occurrence = visit_occurrence , care_site = care_site , available_tables = [ \"measurement\" , \"concept\" , \"concept_relationship\" , \"visit_occurrence\" , \"care_site\" , ], t_start = t_start , t_end = t_end , module = \"pandas\" , )","title":"load_biology_data()"},{"location":"reference/datasets/synthetic/ccam/","text":"eds_scikit.datasets.synthetic.ccam load_ccam load_ccam () Create a minimalistic dataset for the procedures_from_ccam function. RETURNS DESCRIPTION ccam_dataset procedure_occurrence and visit_occurrence. TYPE: CCAMDataset, a dataclass comprised of Source code in eds_scikit/datasets/synthetic/ccam.py 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 def load_ccam (): \"\"\" Create a minimalistic dataset for the `procedures_from_ccam` function. Returns ------- ccam_dataset: CCAMDataset, a dataclass comprised of procedure_occurrence and visit_occurrence. \"\"\" person_ids = [ 1 , 1 , 2 , 3 , 4 , 5 ] procedure_source_values = [ \"DZEA001\" , \"DZEA003\" , \"GFEA004\" , \"EQQF006\" , \"DZEA001\" , \"DZEA001\" , ] procedure_datetimes = pd . to_datetime ( [ \"2010-01-01\" , \"2010-01-01\" , \"2012-01-01\" , \"2012-01-01\" , \"2012-01-01\" , \"2012-01-01\" , ] ) visit_occurrence_ids = [ 11 , 12 , 13 , 14 , 98 , 99 ] procedure_occurrence = pd . DataFrame ( { \"person_id\" : person_ids , \"procedure_source_value\" : procedure_source_values , \"procedure_datetime\" : procedure_datetimes , \"visit_occurrence_id\" : visit_occurrence_ids , } ) person_ids = [ 1 ] * 6 visit_occurrence_ids = [ 11 , 12 , 13 , 14 , 98 , 99 ] visit_start_datetimes = pd . to_datetime ( [ \"2010-01-01\" , \"2010-01-01\" , \"2012-01-01\" , \"2020-01-01\" , \"2000-01-01\" , \"2050-01-01\" , ] ) visit_end_datetimes = pd . to_datetime ( [ \"2010-01-01\" , \"2010-01-01\" , \"2012-01-01\" , \"2020-01-01\" , \"2020-01-01\" , \"1900-01-01\" , ] ) visit_occurrence = pd . DataFrame ( { \"person_id\" : person_ids , \"visit_occurrence_id\" : visit_occurrence_ids , \"visit_start_datetime\" : visit_start_datetimes , \"visit_end_datetime\" : visit_end_datetimes , } ) return CCAMDataset ( procedure_occurrence = procedure_occurrence , visit_occurrence = visit_occurrence , )","title":"ccam"},{"location":"reference/datasets/synthetic/ccam/#eds_scikitdatasetssyntheticccam","text":"","title":"eds_scikit.datasets.synthetic.ccam"},{"location":"reference/datasets/synthetic/ccam/#eds_scikit.datasets.synthetic.ccam.load_ccam","text":"load_ccam () Create a minimalistic dataset for the procedures_from_ccam function. RETURNS DESCRIPTION ccam_dataset procedure_occurrence and visit_occurrence. TYPE: CCAMDataset, a dataclass comprised of Source code in eds_scikit/datasets/synthetic/ccam.py 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 def load_ccam (): \"\"\" Create a minimalistic dataset for the `procedures_from_ccam` function. Returns ------- ccam_dataset: CCAMDataset, a dataclass comprised of procedure_occurrence and visit_occurrence. \"\"\" person_ids = [ 1 , 1 , 2 , 3 , 4 , 5 ] procedure_source_values = [ \"DZEA001\" , \"DZEA003\" , \"GFEA004\" , \"EQQF006\" , \"DZEA001\" , \"DZEA001\" , ] procedure_datetimes = pd . to_datetime ( [ \"2010-01-01\" , \"2010-01-01\" , \"2012-01-01\" , \"2012-01-01\" , \"2012-01-01\" , \"2012-01-01\" , ] ) visit_occurrence_ids = [ 11 , 12 , 13 , 14 , 98 , 99 ] procedure_occurrence = pd . DataFrame ( { \"person_id\" : person_ids , \"procedure_source_value\" : procedure_source_values , \"procedure_datetime\" : procedure_datetimes , \"visit_occurrence_id\" : visit_occurrence_ids , } ) person_ids = [ 1 ] * 6 visit_occurrence_ids = [ 11 , 12 , 13 , 14 , 98 , 99 ] visit_start_datetimes = pd . to_datetime ( [ \"2010-01-01\" , \"2010-01-01\" , \"2012-01-01\" , \"2020-01-01\" , \"2000-01-01\" , \"2050-01-01\" , ] ) visit_end_datetimes = pd . to_datetime ( [ \"2010-01-01\" , \"2010-01-01\" , \"2012-01-01\" , \"2020-01-01\" , \"2020-01-01\" , \"1900-01-01\" , ] ) visit_occurrence = pd . DataFrame ( { \"person_id\" : person_ids , \"visit_occurrence_id\" : visit_occurrence_ids , \"visit_start_datetime\" : visit_start_datetimes , \"visit_end_datetime\" : visit_end_datetimes , } ) return CCAMDataset ( procedure_occurrence = procedure_occurrence , visit_occurrence = visit_occurrence , )","title":"load_ccam()"},{"location":"reference/datasets/synthetic/consultation_dates/","text":"eds_scikit.datasets.synthetic.consultation_dates load_consultation_dates load_consultation_dates () Create a minimalistic dataset for the get_consultation_dates function. RETURNS DESCRIPTION consultation_dataset visit_occurence, note and note_nlp. TYPE: ConsultationDataset, a dataclass comprised of Source code in eds_scikit/datasets/synthetic/consultation_dates.py 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 def load_consultation_dates (): \"\"\" Create a minimalistic dataset for the `get_consultation_dates` function. Returns ------- consultation_dataset: ConsultationDataset, a dataclass comprised of visit_occurence, note and note_nlp. \"\"\" n_visits = 4 visit_occurrence_ids = list ( range ( n_visits )) visit_source_value = [ \"consultation externe\" , \"consultation externe\" , \"hospitalisation\" , \"consultation externe\" , ] visit_occurrence = pd . DataFrame ( { \"visit_occurrence_id\" : visit_occurrence_ids , \"visit_source_value\" : visit_source_value , } ) n_notes = 10 visit_occurrence_ids = [ n_visits * idx // n_notes for idx in range ( n_notes )] note_ids = list ( range ( n_notes )) note_datetimes = [ 1 , 1 , 5 , 6 , 7 , 1 , 1 , 2 , 3 , 8 ] note_datetimes = [ datetime ( 2020 , 1 , day ) for day in note_datetimes ] note_class_source_value = ( n_notes // 2 ) * [ \"CR-CONS\" ] + ( n_notes // 2 ) * [ \"CR-HOSP\" ] note = pd . DataFrame ( { \"visit_occurrence_id\" : visit_occurrence_ids , \"note_id\" : note_ids , \"note_datetime\" : note_datetimes , \"note_class_source_value\" : note_class_source_value , } ) n_note_nlp = 20 starts = [ 4 , 14 , 0 , 7 , 5 , 11 , 8 , 18 , 6 , 19 , 15 , 9 , 17 , 1 , 12 , 2 , 3 , 16 , 10 , 13 , ] note_ids = [ n_notes * idx // n_note_nlp for idx in range ( n_note_nlp )] consultation_dates = 2 * [ 1 , 1 , 5 , 6 , 7 , 1 , 2 , 3 , 9 , 12 ] consultation_dates = [ datetime ( 2020 , 1 , day ) for day in consultation_dates ] note_nlp = pd . DataFrame ( { \"note_id\" : note_ids , \"consultation_date\" : consultation_dates , \"start\" : starts , } ) return ConsultationDataset ( visit_occurrence = visit_occurrence , note = note , note_nlp = note_nlp , )","title":"consultation_dates"},{"location":"reference/datasets/synthetic/consultation_dates/#eds_scikitdatasetssyntheticconsultation_dates","text":"","title":"eds_scikit.datasets.synthetic.consultation_dates"},{"location":"reference/datasets/synthetic/consultation_dates/#eds_scikit.datasets.synthetic.consultation_dates.load_consultation_dates","text":"load_consultation_dates () Create a minimalistic dataset for the get_consultation_dates function. RETURNS DESCRIPTION consultation_dataset visit_occurence, note and note_nlp. TYPE: ConsultationDataset, a dataclass comprised of Source code in eds_scikit/datasets/synthetic/consultation_dates.py 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 def load_consultation_dates (): \"\"\" Create a minimalistic dataset for the `get_consultation_dates` function. Returns ------- consultation_dataset: ConsultationDataset, a dataclass comprised of visit_occurence, note and note_nlp. \"\"\" n_visits = 4 visit_occurrence_ids = list ( range ( n_visits )) visit_source_value = [ \"consultation externe\" , \"consultation externe\" , \"hospitalisation\" , \"consultation externe\" , ] visit_occurrence = pd . DataFrame ( { \"visit_occurrence_id\" : visit_occurrence_ids , \"visit_source_value\" : visit_source_value , } ) n_notes = 10 visit_occurrence_ids = [ n_visits * idx // n_notes for idx in range ( n_notes )] note_ids = list ( range ( n_notes )) note_datetimes = [ 1 , 1 , 5 , 6 , 7 , 1 , 1 , 2 , 3 , 8 ] note_datetimes = [ datetime ( 2020 , 1 , day ) for day in note_datetimes ] note_class_source_value = ( n_notes // 2 ) * [ \"CR-CONS\" ] + ( n_notes // 2 ) * [ \"CR-HOSP\" ] note = pd . DataFrame ( { \"visit_occurrence_id\" : visit_occurrence_ids , \"note_id\" : note_ids , \"note_datetime\" : note_datetimes , \"note_class_source_value\" : note_class_source_value , } ) n_note_nlp = 20 starts = [ 4 , 14 , 0 , 7 , 5 , 11 , 8 , 18 , 6 , 19 , 15 , 9 , 17 , 1 , 12 , 2 , 3 , 16 , 10 , 13 , ] note_ids = [ n_notes * idx // n_note_nlp for idx in range ( n_note_nlp )] consultation_dates = 2 * [ 1 , 1 , 5 , 6 , 7 , 1 , 2 , 3 , 9 , 12 ] consultation_dates = [ datetime ( 2020 , 1 , day ) for day in consultation_dates ] note_nlp = pd . DataFrame ( { \"note_id\" : note_ids , \"consultation_date\" : consultation_dates , \"start\" : starts , } ) return ConsultationDataset ( visit_occurrence = visit_occurrence , note = note , note_nlp = note_nlp , )","title":"load_consultation_dates()"},{"location":"reference/datasets/synthetic/event_sequences/","text":"eds_scikit.datasets.synthetic.event_sequences","title":"event_sequences"},{"location":"reference/datasets/synthetic/event_sequences/#eds_scikitdatasetssyntheticevent_sequences","text":"","title":"eds_scikit.datasets.synthetic.event_sequences"},{"location":"reference/datasets/synthetic/hierarchy/","text":"eds_scikit.datasets.synthetic.hierarchy","title":"hierarchy"},{"location":"reference/datasets/synthetic/hierarchy/#eds_scikitdatasetssynthetichierarchy","text":"","title":"eds_scikit.datasets.synthetic.hierarchy"},{"location":"reference/datasets/synthetic/icd10/","text":"eds_scikit.datasets.synthetic.icd10","title":"icd10"},{"location":"reference/datasets/synthetic/icd10/#eds_scikitdatasetssyntheticicd10","text":"","title":"eds_scikit.datasets.synthetic.icd10"},{"location":"reference/datasets/synthetic/person/","text":"eds_scikit.datasets.synthetic.person","title":"person"},{"location":"reference/datasets/synthetic/person/#eds_scikitdatasetssyntheticperson","text":"","title":"eds_scikit.datasets.synthetic.person"},{"location":"reference/datasets/synthetic/stay_duration/","text":"eds_scikit.datasets.synthetic.stay_duration","title":"stay_duration"},{"location":"reference/datasets/synthetic/stay_duration/#eds_scikitdatasetssyntheticstay_duration","text":"","title":"eds_scikit.datasets.synthetic.stay_duration"},{"location":"reference/datasets/synthetic/suicide_attempt/","text":"eds_scikit.datasets.synthetic.suicide_attempt","title":"suicide_attempt"},{"location":"reference/datasets/synthetic/suicide_attempt/#eds_scikitdatasetssyntheticsuicide_attempt","text":"","title":"eds_scikit.datasets.synthetic.suicide_attempt"},{"location":"reference/datasets/synthetic/tagging/","text":"eds_scikit.datasets.synthetic.tagging","title":"tagging"},{"location":"reference/datasets/synthetic/tagging/#eds_scikitdatasetssynthetictagging","text":"","title":"eds_scikit.datasets.synthetic.tagging"},{"location":"reference/datasets/synthetic/visit_merging/","text":"eds_scikit.datasets.synthetic.visit_merging load_visit_merging load_visit_merging () Create a minimalistic dataset for the visit_merging function. RETURNS DESCRIPTION visit_dataset TYPE: VisitDataset, a dataclass comprised of visit_occurence. Source code in eds_scikit/datasets/synthetic/visit_merging.py 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 def load_visit_merging (): \"\"\" Create a minimalistic dataset for the `visit_merging` function. Returns ------- visit_dataset : VisitDataset, a dataclass comprised of visit_occurence. \"\"\" visit_occurrence = pd . DataFrame ( { \"visit_occurrence_id\" : [ \"A\" , \"B\" , \"C\" , \"D\" , \"E\" , \"F\" , \"G\" ], \"person_id\" : [ \"999\" ] * 7 , \"visit_start_datetime\" : [ \"2021-01-01\" , \"2021-01-04\" , \"2021-01-12\" , \"2021-01-13\" , \"2021-01-19\" , \"2021-01-25\" , \"2017-01-01\" , ], \"visit_end_datetime\" : [ \"2021-01-05\" , \"2021-01-08\" , \"2021-01-18\" , \"2021-01-14\" , \"2021-01-21\" , \"2021-01-27\" , None , ], \"visit_source_value\" : [ \"hospitalis\u00e9s\" , \"hospitalis\u00e9s\" , \"hospitalis\u00e9s\" , \"urgence\" , \"hospitalis\u00e9s\" , \"hospitalis\u00e9s\" , \"hospitalis\u00e9s\" , ], \"row_status_source_value\" : [ \"supprim\u00e9\" , \"courant\" , \"courant\" , \"courant\" , \"courant\" , \"courant\" , \"courant\" , ], \"care_site_id\" : [ \"1\" , \"1\" , \"1\" , \"1\" , \"2\" , \"1\" , \"1\" ], } ) for col in [ \"visit_start_datetime\" , \"visit_end_datetime\" ]: visit_occurrence [ col ] = pd . to_datetime ( visit_occurrence [ col ]) return VisitDataset ( visit_occurrence = visit_occurrence )","title":"visit_merging"},{"location":"reference/datasets/synthetic/visit_merging/#eds_scikitdatasetssyntheticvisit_merging","text":"","title":"eds_scikit.datasets.synthetic.visit_merging"},{"location":"reference/datasets/synthetic/visit_merging/#eds_scikit.datasets.synthetic.visit_merging.load_visit_merging","text":"load_visit_merging () Create a minimalistic dataset for the visit_merging function. RETURNS DESCRIPTION visit_dataset TYPE: VisitDataset, a dataclass comprised of visit_occurence. Source code in eds_scikit/datasets/synthetic/visit_merging.py 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 def load_visit_merging (): \"\"\" Create a minimalistic dataset for the `visit_merging` function. Returns ------- visit_dataset : VisitDataset, a dataclass comprised of visit_occurence. \"\"\" visit_occurrence = pd . DataFrame ( { \"visit_occurrence_id\" : [ \"A\" , \"B\" , \"C\" , \"D\" , \"E\" , \"F\" , \"G\" ], \"person_id\" : [ \"999\" ] * 7 , \"visit_start_datetime\" : [ \"2021-01-01\" , \"2021-01-04\" , \"2021-01-12\" , \"2021-01-13\" , \"2021-01-19\" , \"2021-01-25\" , \"2017-01-01\" , ], \"visit_end_datetime\" : [ \"2021-01-05\" , \"2021-01-08\" , \"2021-01-18\" , \"2021-01-14\" , \"2021-01-21\" , \"2021-01-27\" , None , ], \"visit_source_value\" : [ \"hospitalis\u00e9s\" , \"hospitalis\u00e9s\" , \"hospitalis\u00e9s\" , \"urgence\" , \"hospitalis\u00e9s\" , \"hospitalis\u00e9s\" , \"hospitalis\u00e9s\" , ], \"row_status_source_value\" : [ \"supprim\u00e9\" , \"courant\" , \"courant\" , \"courant\" , \"courant\" , \"courant\" , \"courant\" , ], \"care_site_id\" : [ \"1\" , \"1\" , \"1\" , \"1\" , \"2\" , \"1\" , \"1\" ], } ) for col in [ \"visit_start_datetime\" , \"visit_end_datetime\" ]: visit_occurrence [ col ] = pd . to_datetime ( visit_occurrence [ col ]) return VisitDataset ( visit_occurrence = visit_occurrence )","title":"load_visit_merging()"},{"location":"reference/emergency/","text":"eds_scikit.emergency","title":"`eds_scikit.emergency`"},{"location":"reference/emergency/#eds_scikitemergency","text":"","title":"eds_scikit.emergency"},{"location":"reference/emergency/emergency_care_site/","text":"eds_scikit.emergency.emergency_care_site tag_emergency_care_site tag_emergency_care_site ( care_site : DataFrame , algo : str = 'from_mapping' ) -> DataFrame Tag care sites that correspond to medical emergency units . The tagging is done by adding a \"IS_EMERGENCY\" column to the provided DataFrame. Some algos can add an additional \"EMERGENCY_TYPE\" column to the provided DataFrame, providing a more detailled classification. PARAMETER DESCRIPTION care_site TYPE: DataFrame algo Possible values are: \"from_mapping\" relies on a list of care_site_source_value extracted by Judith LEBLANC, Ariel COHEN and validated by an ER doctor. The emergency care sites are here further labelled to distinguish the different types of emergency \"from_regex_on_care_site_description\" : relies on a specific list of RegEx applied on the description (= simplified care site name) of each care site. \"from_regex_on_parent_UF\" : relies on a specific list of regular expressions applied on the description (= simplified care site name) of each UF (Unit\u00e9 Fonctionnelle). The obtained tag is then propagated to every UF's children. TYPE: str DEFAULT: 'from_mapping' RETURNS DESCRIPTION care_site Dataframe with 1 to 2 added columns corresponding to the following concepts: \"IS_EMERGENCY\" \"EMERGENCY_TYPE\" (if using algo \"from_mapping\" ) TYPE: DataFrame Source code in eds_scikit/emergency/emergency_care_site.py 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 @algo_checker ( algos = ALGOS ) def tag_emergency_care_site ( care_site : DataFrame , algo : str = \"from_mapping\" , ) -> DataFrame : \"\"\"Tag care sites that correspond to **medical emergency units**. The tagging is done by adding a `\"IS_EMERGENCY\"` column to the provided DataFrame. Some algos can add an additional `\"EMERGENCY_TYPE\"` column to the provided DataFrame, providing a more detailled classification. Parameters ---------- care_site: DataFrame algo: str Possible values are: - [`\"from_mapping\"`][eds_scikit.emergency.emergency_care_site.from_mapping] relies on a list of `care_site_source_value` extracted by Judith LEBLANC, Ariel COHEN and validated by an ER doctor. The emergency care sites are here further labelled to distinguish the different types of emergency - [`\"from_regex_on_care_site_description\"`][eds_scikit.emergency.emergency_care_site.from_regex_on_care_site_description]: relies on a specific list of RegEx applied on the description (= simplified care site name) of each care site. - [`\"from_regex_on_parent_UF\"`][eds_scikit.emergency.emergency_care_site.from_regex_on_parent_UF]: relies on a specific list of regular expressions applied on the description (= simplified care site name) of each UF (Unit\u00e9 Fonctionnelle). The obtained tag is then propagated to every UF's children. Returns ------- care_site: DataFrame Dataframe with 1 to 2 added columns corresponding to the following concepts: - `\"IS_EMERGENCY\"` - `\"EMERGENCY_TYPE\"` (if using algo `\"from_mapping\"`) \"\"\" if algo == \"from_regex_on_parent_UF\" : return from_regex_on_parent_UF ( care_site ) elif algo == \"from_regex_on_care_site_description\" : return from_regex_on_care_site_description ( care_site ) elif algo . startswith ( \"from_mapping\" ): return from_mapping ( care_site , version = versionize ( algo )) from_mapping from_mapping ( care_site : DataFrame , version : Optional [ str ] = None ) -> DataFrame This algo uses a labelled list of 201 emergency care sites. Those care sites were extracted and verified by Ariel COHEN, Judith LEBLANC, and an ER doctor validated them. Those emergency care sites are further divised into different categories, as defined in the concept 'EMERGENCY_TYPE'. The different categories are: Urgences sp\u00e9cialis\u00e9es UHCD + Post-urgences Urgences p\u00e9diatriques Urgences g\u00e9n\u00e9rales adulte Consultation urgences SAMU / SMUR See the dataset here PARAMETER DESCRIPTION care_site Should at least contains the care_site_source_value column TYPE: DataFrame version Optional version string for the mapping TYPE: Optional [ str ] DEFAULT: None RETURNS DESCRIPTION care_site Dataframe with 2 added columns corresponding to the following concepts: \"IS_EMERGENCY\" \"EMERGENCY_TYPE\" TYPE: DataFrame Source code in eds_scikit/emergency/emergency_care_site.py 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 @concept_checker ( concepts = [ \"IS_EMERGENCY\" , \"EMERGENCY_TYPE\" ]) def from_mapping ( care_site : DataFrame , version : Optional [ str ] = None , ) -> DataFrame : \"\"\"This algo uses a labelled list of 201 emergency care sites. Those care sites were extracted and verified by Ariel COHEN, Judith LEBLANC, and an ER doctor validated them. Those emergency care sites are further divised into different categories, as defined in the concept 'EMERGENCY_TYPE'. The different categories are: - Urgences sp\u00e9cialis\u00e9es - UHCD + Post-urgences - Urgences p\u00e9diatriques - Urgences g\u00e9n\u00e9rales adulte - Consultation urgences - SAMU / SMUR See the dataset [here](/datasets/care-site-emergency) Parameters ---------- care_site: DataFrame Should at least contains the `care_site_source_value` column version: Optional[str] Optional version string for the mapping Returns ------- care_site: DataFrame Dataframe with 2 added columns corresponding to the following concepts: - `\"IS_EMERGENCY\"` - `\"EMERGENCY_TYPE\"` \"\"\" function_name = \"get_care_site_emergency_mapping\" if version is not None : function_name += f \". { version } \" mapping = registry . get ( \"data\" , function_name = function_name )() # Getting the right framework fw = framework . get_framework ( care_site ) mapping = framework . to ( fw , mapping ) care_site = care_site . merge ( mapping , how = \"left\" , on = \"care_site_source_value\" , ) care_site [ \"IS_EMERGENCY\" ] = care_site [ \"EMERGENCY_TYPE\" ] . notna () return care_site from_regex_on_care_site_description from_regex_on_care_site_description ( care_site : DataFrame ) -> DataFrame Use regular expressions on care_site_name to decide if it an emergency care site. This relies on this function . The regular expression used to detect emergency status is r\"\bURG|\bSAU\b|\bUHCDb\b|\bZHTCD\b\" PARAMETER DESCRIPTION care_site Should at least contains the care_site_name column TYPE: DataFrame RETURNS DESCRIPTION care_site Dataframe with 1 added column corresponding to the following concept: \"IS_EMERGENCY\" TYPE: DataFrame Source code in eds_scikit/emergency/emergency_care_site.py 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 def from_regex_on_care_site_description ( care_site : DataFrame ) -> DataFrame : \"\"\"Use regular expressions on `care_site_name` to decide if it an emergency care site. This relies on [this function][eds_scikit.structures.attributes.add_care_site_attributes]. The regular expression used to detect emergency status is `r\"\\bURG|\\bSAU\\b|\\bUHCDb\\b|\\bZHTCD\\b\"` Parameters ---------- care_site: DataFrame Should at least contains the `care_site_name` column Returns ------- care_site: DataFrame Dataframe with 1 added column corresponding to the following concept: - `\"IS_EMERGENCY\"` \"\"\" return attributes . add_care_site_attributes ( care_site , only_attributes = [ \"IS_EMERGENCY\" ] ) from_regex_on_parent_UF from_regex_on_parent_UF ( care_site : DataFrame ) -> DataFrame Use regular expressions on parent UF (Unit\u00e9 Fonctionnelle) to classify emergency care site. This relies on this function . The regular expression used to detect emergency status is r\"\bURG|\bSAU\b|\bUHCD\b|\bZHTCD\b\" PARAMETER DESCRIPTION care_site Should at least contains the care_site_name column TYPE: DataFrame RETURNS DESCRIPTION care_site Dataframe with 1 added column corresponding to the following concept: 'IS_EMERGENCY' TYPE: DataFrame Source code in eds_scikit/emergency/emergency_care_site.py 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 @concept_checker ( concepts = [ \"IS_EMERGENCY\" ]) def from_regex_on_parent_UF ( care_site : DataFrame ) -> DataFrame : \"\"\"Use regular expressions on parent UF (Unit\u00e9 Fonctionnelle) to classify emergency care site. This relies on [this function][eds_scikit.structures.attributes.get_parent_attributes]. The regular expression used to detect emergency status is `r\"\\bURG|\\bSAU\\b|\\bUHCD\\b|\\bZHTCD\\b\"` Parameters ---------- care_site: DataFrame Should at least contains the `care_site_name` column Returns ------- care_site: DataFrame Dataframe with 1 added column corresponding to the following concept: - 'IS_EMERGENCY' \"\"\" return attributes . get_parent_attributes ( care_site , only_attributes = [ \"IS_EMERGENCY\" ], parent_type = \"Unit\u00e9 Fonctionnelle (UF)\" , )","title":"emergency_care_site"},{"location":"reference/emergency/emergency_care_site/#eds_scikitemergencyemergency_care_site","text":"","title":"eds_scikit.emergency.emergency_care_site"},{"location":"reference/emergency/emergency_care_site/#eds_scikit.emergency.emergency_care_site.tag_emergency_care_site","text":"tag_emergency_care_site ( care_site : DataFrame , algo : str = 'from_mapping' ) -> DataFrame Tag care sites that correspond to medical emergency units . The tagging is done by adding a \"IS_EMERGENCY\" column to the provided DataFrame. Some algos can add an additional \"EMERGENCY_TYPE\" column to the provided DataFrame, providing a more detailled classification. PARAMETER DESCRIPTION care_site TYPE: DataFrame algo Possible values are: \"from_mapping\" relies on a list of care_site_source_value extracted by Judith LEBLANC, Ariel COHEN and validated by an ER doctor. The emergency care sites are here further labelled to distinguish the different types of emergency \"from_regex_on_care_site_description\" : relies on a specific list of RegEx applied on the description (= simplified care site name) of each care site. \"from_regex_on_parent_UF\" : relies on a specific list of regular expressions applied on the description (= simplified care site name) of each UF (Unit\u00e9 Fonctionnelle). The obtained tag is then propagated to every UF's children. TYPE: str DEFAULT: 'from_mapping' RETURNS DESCRIPTION care_site Dataframe with 1 to 2 added columns corresponding to the following concepts: \"IS_EMERGENCY\" \"EMERGENCY_TYPE\" (if using algo \"from_mapping\" ) TYPE: DataFrame Source code in eds_scikit/emergency/emergency_care_site.py 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 @algo_checker ( algos = ALGOS ) def tag_emergency_care_site ( care_site : DataFrame , algo : str = \"from_mapping\" , ) -> DataFrame : \"\"\"Tag care sites that correspond to **medical emergency units**. The tagging is done by adding a `\"IS_EMERGENCY\"` column to the provided DataFrame. Some algos can add an additional `\"EMERGENCY_TYPE\"` column to the provided DataFrame, providing a more detailled classification. Parameters ---------- care_site: DataFrame algo: str Possible values are: - [`\"from_mapping\"`][eds_scikit.emergency.emergency_care_site.from_mapping] relies on a list of `care_site_source_value` extracted by Judith LEBLANC, Ariel COHEN and validated by an ER doctor. The emergency care sites are here further labelled to distinguish the different types of emergency - [`\"from_regex_on_care_site_description\"`][eds_scikit.emergency.emergency_care_site.from_regex_on_care_site_description]: relies on a specific list of RegEx applied on the description (= simplified care site name) of each care site. - [`\"from_regex_on_parent_UF\"`][eds_scikit.emergency.emergency_care_site.from_regex_on_parent_UF]: relies on a specific list of regular expressions applied on the description (= simplified care site name) of each UF (Unit\u00e9 Fonctionnelle). The obtained tag is then propagated to every UF's children. Returns ------- care_site: DataFrame Dataframe with 1 to 2 added columns corresponding to the following concepts: - `\"IS_EMERGENCY\"` - `\"EMERGENCY_TYPE\"` (if using algo `\"from_mapping\"`) \"\"\" if algo == \"from_regex_on_parent_UF\" : return from_regex_on_parent_UF ( care_site ) elif algo == \"from_regex_on_care_site_description\" : return from_regex_on_care_site_description ( care_site ) elif algo . startswith ( \"from_mapping\" ): return from_mapping ( care_site , version = versionize ( algo ))","title":"tag_emergency_care_site()"},{"location":"reference/emergency/emergency_care_site/#eds_scikit.emergency.emergency_care_site.from_mapping","text":"from_mapping ( care_site : DataFrame , version : Optional [ str ] = None ) -> DataFrame This algo uses a labelled list of 201 emergency care sites. Those care sites were extracted and verified by Ariel COHEN, Judith LEBLANC, and an ER doctor validated them. Those emergency care sites are further divised into different categories, as defined in the concept 'EMERGENCY_TYPE'. The different categories are: Urgences sp\u00e9cialis\u00e9es UHCD + Post-urgences Urgences p\u00e9diatriques Urgences g\u00e9n\u00e9rales adulte Consultation urgences SAMU / SMUR See the dataset here PARAMETER DESCRIPTION care_site Should at least contains the care_site_source_value column TYPE: DataFrame version Optional version string for the mapping TYPE: Optional [ str ] DEFAULT: None RETURNS DESCRIPTION care_site Dataframe with 2 added columns corresponding to the following concepts: \"IS_EMERGENCY\" \"EMERGENCY_TYPE\" TYPE: DataFrame Source code in eds_scikit/emergency/emergency_care_site.py 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 @concept_checker ( concepts = [ \"IS_EMERGENCY\" , \"EMERGENCY_TYPE\" ]) def from_mapping ( care_site : DataFrame , version : Optional [ str ] = None , ) -> DataFrame : \"\"\"This algo uses a labelled list of 201 emergency care sites. Those care sites were extracted and verified by Ariel COHEN, Judith LEBLANC, and an ER doctor validated them. Those emergency care sites are further divised into different categories, as defined in the concept 'EMERGENCY_TYPE'. The different categories are: - Urgences sp\u00e9cialis\u00e9es - UHCD + Post-urgences - Urgences p\u00e9diatriques - Urgences g\u00e9n\u00e9rales adulte - Consultation urgences - SAMU / SMUR See the dataset [here](/datasets/care-site-emergency) Parameters ---------- care_site: DataFrame Should at least contains the `care_site_source_value` column version: Optional[str] Optional version string for the mapping Returns ------- care_site: DataFrame Dataframe with 2 added columns corresponding to the following concepts: - `\"IS_EMERGENCY\"` - `\"EMERGENCY_TYPE\"` \"\"\" function_name = \"get_care_site_emergency_mapping\" if version is not None : function_name += f \". { version } \" mapping = registry . get ( \"data\" , function_name = function_name )() # Getting the right framework fw = framework . get_framework ( care_site ) mapping = framework . to ( fw , mapping ) care_site = care_site . merge ( mapping , how = \"left\" , on = \"care_site_source_value\" , ) care_site [ \"IS_EMERGENCY\" ] = care_site [ \"EMERGENCY_TYPE\" ] . notna () return care_site","title":"from_mapping()"},{"location":"reference/emergency/emergency_care_site/#eds_scikit.emergency.emergency_care_site.from_regex_on_care_site_description","text":"from_regex_on_care_site_description ( care_site : DataFrame ) -> DataFrame Use regular expressions on care_site_name to decide if it an emergency care site. This relies on this function . The regular expression used to detect emergency status is r\"\bURG|\bSAU\b|\bUHCDb\b|\bZHTCD\b\" PARAMETER DESCRIPTION care_site Should at least contains the care_site_name column TYPE: DataFrame RETURNS DESCRIPTION care_site Dataframe with 1 added column corresponding to the following concept: \"IS_EMERGENCY\" TYPE: DataFrame Source code in eds_scikit/emergency/emergency_care_site.py 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 def from_regex_on_care_site_description ( care_site : DataFrame ) -> DataFrame : \"\"\"Use regular expressions on `care_site_name` to decide if it an emergency care site. This relies on [this function][eds_scikit.structures.attributes.add_care_site_attributes]. The regular expression used to detect emergency status is `r\"\\bURG|\\bSAU\\b|\\bUHCDb\\b|\\bZHTCD\\b\"` Parameters ---------- care_site: DataFrame Should at least contains the `care_site_name` column Returns ------- care_site: DataFrame Dataframe with 1 added column corresponding to the following concept: - `\"IS_EMERGENCY\"` \"\"\" return attributes . add_care_site_attributes ( care_site , only_attributes = [ \"IS_EMERGENCY\" ] )","title":"from_regex_on_care_site_description()"},{"location":"reference/emergency/emergency_care_site/#eds_scikit.emergency.emergency_care_site.from_regex_on_parent_UF","text":"from_regex_on_parent_UF ( care_site : DataFrame ) -> DataFrame Use regular expressions on parent UF (Unit\u00e9 Fonctionnelle) to classify emergency care site. This relies on this function . The regular expression used to detect emergency status is r\"\bURG|\bSAU\b|\bUHCD\b|\bZHTCD\b\" PARAMETER DESCRIPTION care_site Should at least contains the care_site_name column TYPE: DataFrame RETURNS DESCRIPTION care_site Dataframe with 1 added column corresponding to the following concept: 'IS_EMERGENCY' TYPE: DataFrame Source code in eds_scikit/emergency/emergency_care_site.py 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 @concept_checker ( concepts = [ \"IS_EMERGENCY\" ]) def from_regex_on_parent_UF ( care_site : DataFrame ) -> DataFrame : \"\"\"Use regular expressions on parent UF (Unit\u00e9 Fonctionnelle) to classify emergency care site. This relies on [this function][eds_scikit.structures.attributes.get_parent_attributes]. The regular expression used to detect emergency status is `r\"\\bURG|\\bSAU\\b|\\bUHCD\\b|\\bZHTCD\\b\"` Parameters ---------- care_site: DataFrame Should at least contains the `care_site_name` column Returns ------- care_site: DataFrame Dataframe with 1 added column corresponding to the following concept: - 'IS_EMERGENCY' \"\"\" return attributes . get_parent_attributes ( care_site , only_attributes = [ \"IS_EMERGENCY\" ], parent_type = \"Unit\u00e9 Fonctionnelle (UF)\" , )","title":"from_regex_on_parent_UF()"},{"location":"reference/emergency/emergency_visit/","text":"eds_scikit.emergency.emergency_visit tag_emergency_visit tag_emergency_visit ( visit_detail : DataFrame , care_site : Optional [ DataFrame ] = None , visit_occurrence : Optional [ DataFrame ] = None , algo : str = 'from_mapping' ) -> DataFrame Tag visits that correspond to medical emergency units . The tagging is done by adding a \"IS_EMERGENCY\" column to the provided DataFrame. Some algos can add an additional \"EMERGENCY_TYPE\" column to the provided DataFrame, providing a more detailled classification. It works by either tagging each visit detail's care site , or by using the visit_occurrence 's \"visit_source_value\" . PARAMETER DESCRIPTION visit_detail TYPE: DataFrame care_site Isn't necessary if the algo \"from_vo_visit_source_value\" is used TYPE: Optional [ DataFrame ] DEFAULT: None visit_occurrence Is mandatory if the algo \"from_vo_visit_source_value\" is used TYPE: Optional [ DataFrame ] DEFAULT: None algo Possible values are: \"from_mapping\" relies on a list of care_site_source_value extracted by Judith LEBLANC, Ariel COHEN and validated by an ER doctor. The emergency care sites are here further labelled to distinguish the different types of emergency \"from_regex_on_care_site_description\" : relies on a specific list of RegEx applied on the description (= simplified care site name) of each care site. \"from_regex_on_parent_UF\" : relies on a specific list of regular expressions applied on the description (= simplified care site name) of each UF (Unit\u00e9 Fonctionnelle). The obtained tag is then propagated to every UF's children. \"from_vo_visit_source_value\" : relies on the parent visit occurrence of each visit detail: A visit detail will be tagged as emergency if it belongs to a visit occurrence where visit_occurrence.visit_source_value=='urgence' . TYPE: str DEFAULT: 'from_mapping' RETURNS DESCRIPTION care_site Dataframe with 1 to 2 added columns corresponding to the following concepts: \"IS_EMERGENCY\" \"EMERGENCY_TYPE\" (if using algo \"from_mapping\" ) TYPE: DataFrame Source code in eds_scikit/emergency/emergency_visit.py 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 @algo_checker ( algos = ALGOS ) def tag_emergency_visit ( visit_detail : DataFrame , care_site : Optional [ DataFrame ] = None , visit_occurrence : Optional [ DataFrame ] = None , algo : str = \"from_mapping\" , ) -> DataFrame : \"\"\"Tag visits that correspond to **medical emergency units**. The tagging is done by adding a `\"IS_EMERGENCY\"` column to the provided DataFrame. Some algos can add an additional `\"EMERGENCY_TYPE\"` column to the provided DataFrame, providing a more detailled classification. It works by either [tagging each visit detail's care site][eds_scikit.emergency.emergency_care_site.tag_emergency_care_site], or by using the *visit_occurrence*'s `\"visit_source_value\"`. Parameters ---------- visit_detail: DataFrame care_site: DataFrame Isn't necessary if the algo `\"from_vo_visit_source_value\"` is used visit_occurrence: DataFrame, optional. Is mandatory if the algo `\"from_vo_visit_source_value\"` is used algo: str Possible values are: - [`\"from_mapping\"`][eds_scikit.emergency.emergency_care_site.from_mapping] relies on a list of `care_site_source_value` extracted by Judith LEBLANC, Ariel COHEN and validated by an ER doctor. The emergency care sites are here further labelled to distinguish the different types of emergency - [`\"from_regex_on_care_site_description\"`][eds_scikit.emergency.emergency_care_site.from_regex_on_care_site_description]: relies on a specific list of RegEx applied on the description (= simplified care site name) of each care site. - [`\"from_regex_on_parent_UF\"`][eds_scikit.emergency.emergency_care_site.from_regex_on_parent_UF]: relies on a specific list of regular expressions applied on the description (= simplified care site name) of each UF (Unit\u00e9 Fonctionnelle). The obtained tag is then propagated to every UF's children. - [`\"from_vo_visit_source_value\"`][eds_scikit.emergency.emergency_visit.from_vo_visit_source_value]: relies on the parent visit occurrence of each visit detail: A visit detail will be tagged as emergency if it belongs to a visit occurrence where `visit_occurrence.visit_source_value=='urgence'`. Returns ------- care_site: DataFrame Dataframe with 1 to 2 added columns corresponding to the following concepts: - `\"IS_EMERGENCY\"` - `\"EMERGENCY_TYPE\"` (if using algo `\"from_mapping\"`) \"\"\" if algo == \"from_vo_visit_source_value\" : return from_vo_visit_source_value ( visit_detail , visit_occurrence ) else : initial_care_site_columns = set ( care_site . columns ) tagged_care_site = tag_emergency_care_site ( care_site , algo = algo ) to_add_columns = list ( set ( tagged_care_site ) - initial_care_site_columns | set ([ \"care_site_id\" ]) ) return visit_detail . merge ( tagged_care_site [ to_add_columns ], on = \"care_site_id\" , how = \"left\" ) from_vo_visit_source_value from_vo_visit_source_value ( visit_detail : DataFrame , visit_occurrence : DataFrame ) -> DataFrame This algo uses the \"Type de dossier\" of each visit detail's parent visit occurrence. Thus, a visit_detail will be tagged with IS_EMERGENCY=True iff the visit occurrence it belongs to is an emergency-type visit (meaning that visit_occurrence.visit_source_value=='urgence' ) Admission through ICU At AP-HP, when a patient is hospitalized after coming to the ICU, its visit_source_value is set from \"urgence\" to \"hospitalisation compl\u00e8te\" . So you should keep in mind that this method doesn't tag those visits as ICU. PARAMETER DESCRIPTION visit_detail TYPE: DataFrame visit_occurrence TYPE: DataFrame RETURNS DESCRIPTION visit_detail Dataframe with added columns corresponding to the following conceps: \"IS_EMERGENCY\" TYPE: DataFrame Source code in eds_scikit/emergency/emergency_visit.py 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 @concept_checker ( concepts = [ \"IS_EMERGENCY\" ]) def from_vo_visit_source_value ( visit_detail : DataFrame , visit_occurrence : DataFrame , ) -> DataFrame : \"\"\" This algo uses the *\"Type de dossier\"* of each visit detail's parent visit occurrence. Thus, a visit_detail will be tagged with `IS_EMERGENCY=True` iff the visit occurrence it belongs to is an emergency-type visit (meaning that `visit_occurrence.visit_source_value=='urgence'`) !!! aphp \"Admission through ICU\" At AP-HP, when a patient is hospitalized after coming to the ICU, its `visit_source_value` is set from `\"urgence\"` to `\"hospitalisation compl\u00e8te\"`. So you should keep in mind that this method doesn't tag those visits as ICU. Parameters ---------- visit_detail: DataFrame visit_occurrence: DataFrame Returns ------- visit_detail: DataFrame Dataframe with added columns corresponding to the following conceps: - `\"IS_EMERGENCY\"` \"\"\" vo_emergency = visit_occurrence [[ \"visit_occurrence_id\" , \"visit_source_value\" ]] vo_emergency [ \"IS_EMERGENCY\" ] = visit_occurrence . visit_source_value == \"urgence\" return visit_detail . merge ( vo_emergency [[ \"visit_occurrence_id\" , \"IS_EMERGENCY\" ]], on = \"visit_occurrence_id\" , how = \"left\" , )","title":"emergency_visit"},{"location":"reference/emergency/emergency_visit/#eds_scikitemergencyemergency_visit","text":"","title":"eds_scikit.emergency.emergency_visit"},{"location":"reference/emergency/emergency_visit/#eds_scikit.emergency.emergency_visit.tag_emergency_visit","text":"tag_emergency_visit ( visit_detail : DataFrame , care_site : Optional [ DataFrame ] = None , visit_occurrence : Optional [ DataFrame ] = None , algo : str = 'from_mapping' ) -> DataFrame Tag visits that correspond to medical emergency units . The tagging is done by adding a \"IS_EMERGENCY\" column to the provided DataFrame. Some algos can add an additional \"EMERGENCY_TYPE\" column to the provided DataFrame, providing a more detailled classification. It works by either tagging each visit detail's care site , or by using the visit_occurrence 's \"visit_source_value\" . PARAMETER DESCRIPTION visit_detail TYPE: DataFrame care_site Isn't necessary if the algo \"from_vo_visit_source_value\" is used TYPE: Optional [ DataFrame ] DEFAULT: None visit_occurrence Is mandatory if the algo \"from_vo_visit_source_value\" is used TYPE: Optional [ DataFrame ] DEFAULT: None algo Possible values are: \"from_mapping\" relies on a list of care_site_source_value extracted by Judith LEBLANC, Ariel COHEN and validated by an ER doctor. The emergency care sites are here further labelled to distinguish the different types of emergency \"from_regex_on_care_site_description\" : relies on a specific list of RegEx applied on the description (= simplified care site name) of each care site. \"from_regex_on_parent_UF\" : relies on a specific list of regular expressions applied on the description (= simplified care site name) of each UF (Unit\u00e9 Fonctionnelle). The obtained tag is then propagated to every UF's children. \"from_vo_visit_source_value\" : relies on the parent visit occurrence of each visit detail: A visit detail will be tagged as emergency if it belongs to a visit occurrence where visit_occurrence.visit_source_value=='urgence' . TYPE: str DEFAULT: 'from_mapping' RETURNS DESCRIPTION care_site Dataframe with 1 to 2 added columns corresponding to the following concepts: \"IS_EMERGENCY\" \"EMERGENCY_TYPE\" (if using algo \"from_mapping\" ) TYPE: DataFrame Source code in eds_scikit/emergency/emergency_visit.py 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 @algo_checker ( algos = ALGOS ) def tag_emergency_visit ( visit_detail : DataFrame , care_site : Optional [ DataFrame ] = None , visit_occurrence : Optional [ DataFrame ] = None , algo : str = \"from_mapping\" , ) -> DataFrame : \"\"\"Tag visits that correspond to **medical emergency units**. The tagging is done by adding a `\"IS_EMERGENCY\"` column to the provided DataFrame. Some algos can add an additional `\"EMERGENCY_TYPE\"` column to the provided DataFrame, providing a more detailled classification. It works by either [tagging each visit detail's care site][eds_scikit.emergency.emergency_care_site.tag_emergency_care_site], or by using the *visit_occurrence*'s `\"visit_source_value\"`. Parameters ---------- visit_detail: DataFrame care_site: DataFrame Isn't necessary if the algo `\"from_vo_visit_source_value\"` is used visit_occurrence: DataFrame, optional. Is mandatory if the algo `\"from_vo_visit_source_value\"` is used algo: str Possible values are: - [`\"from_mapping\"`][eds_scikit.emergency.emergency_care_site.from_mapping] relies on a list of `care_site_source_value` extracted by Judith LEBLANC, Ariel COHEN and validated by an ER doctor. The emergency care sites are here further labelled to distinguish the different types of emergency - [`\"from_regex_on_care_site_description\"`][eds_scikit.emergency.emergency_care_site.from_regex_on_care_site_description]: relies on a specific list of RegEx applied on the description (= simplified care site name) of each care site. - [`\"from_regex_on_parent_UF\"`][eds_scikit.emergency.emergency_care_site.from_regex_on_parent_UF]: relies on a specific list of regular expressions applied on the description (= simplified care site name) of each UF (Unit\u00e9 Fonctionnelle). The obtained tag is then propagated to every UF's children. - [`\"from_vo_visit_source_value\"`][eds_scikit.emergency.emergency_visit.from_vo_visit_source_value]: relies on the parent visit occurrence of each visit detail: A visit detail will be tagged as emergency if it belongs to a visit occurrence where `visit_occurrence.visit_source_value=='urgence'`. Returns ------- care_site: DataFrame Dataframe with 1 to 2 added columns corresponding to the following concepts: - `\"IS_EMERGENCY\"` - `\"EMERGENCY_TYPE\"` (if using algo `\"from_mapping\"`) \"\"\" if algo == \"from_vo_visit_source_value\" : return from_vo_visit_source_value ( visit_detail , visit_occurrence ) else : initial_care_site_columns = set ( care_site . columns ) tagged_care_site = tag_emergency_care_site ( care_site , algo = algo ) to_add_columns = list ( set ( tagged_care_site ) - initial_care_site_columns | set ([ \"care_site_id\" ]) ) return visit_detail . merge ( tagged_care_site [ to_add_columns ], on = \"care_site_id\" , how = \"left\" )","title":"tag_emergency_visit()"},{"location":"reference/emergency/emergency_visit/#eds_scikit.emergency.emergency_visit.from_vo_visit_source_value","text":"from_vo_visit_source_value ( visit_detail : DataFrame , visit_occurrence : DataFrame ) -> DataFrame This algo uses the \"Type de dossier\" of each visit detail's parent visit occurrence. Thus, a visit_detail will be tagged with IS_EMERGENCY=True iff the visit occurrence it belongs to is an emergency-type visit (meaning that visit_occurrence.visit_source_value=='urgence' ) Admission through ICU At AP-HP, when a patient is hospitalized after coming to the ICU, its visit_source_value is set from \"urgence\" to \"hospitalisation compl\u00e8te\" . So you should keep in mind that this method doesn't tag those visits as ICU. PARAMETER DESCRIPTION visit_detail TYPE: DataFrame visit_occurrence TYPE: DataFrame RETURNS DESCRIPTION visit_detail Dataframe with added columns corresponding to the following conceps: \"IS_EMERGENCY\" TYPE: DataFrame Source code in eds_scikit/emergency/emergency_visit.py 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 @concept_checker ( concepts = [ \"IS_EMERGENCY\" ]) def from_vo_visit_source_value ( visit_detail : DataFrame , visit_occurrence : DataFrame , ) -> DataFrame : \"\"\" This algo uses the *\"Type de dossier\"* of each visit detail's parent visit occurrence. Thus, a visit_detail will be tagged with `IS_EMERGENCY=True` iff the visit occurrence it belongs to is an emergency-type visit (meaning that `visit_occurrence.visit_source_value=='urgence'`) !!! aphp \"Admission through ICU\" At AP-HP, when a patient is hospitalized after coming to the ICU, its `visit_source_value` is set from `\"urgence\"` to `\"hospitalisation compl\u00e8te\"`. So you should keep in mind that this method doesn't tag those visits as ICU. Parameters ---------- visit_detail: DataFrame visit_occurrence: DataFrame Returns ------- visit_detail: DataFrame Dataframe with added columns corresponding to the following conceps: - `\"IS_EMERGENCY\"` \"\"\" vo_emergency = visit_occurrence [[ \"visit_occurrence_id\" , \"visit_source_value\" ]] vo_emergency [ \"IS_EMERGENCY\" ] = visit_occurrence . visit_source_value == \"urgence\" return visit_detail . merge ( vo_emergency [[ \"visit_occurrence_id\" , \"IS_EMERGENCY\" ]], on = \"visit_occurrence_id\" , how = \"left\" , )","title":"from_vo_visit_source_value()"},{"location":"reference/event/","text":"eds_scikit.event","title":"`eds_scikit.event`"},{"location":"reference/event/#eds_scikitevent","text":"","title":"eds_scikit.event"},{"location":"reference/event/ccam/","text":"eds_scikit.event.ccam procedures_from_ccam procedures_from_ccam ( procedure_occurrence : DataFrame , visit_occurrence : Optional [ DataFrame ] = None , codes : Optional [ Dict [ str , Union [ str , List [ str ]]]] = None , date_from_visit : bool = True , additional_filtering = dict (), date_min : Optional [ datetime ] = None , date_max : Optional [ datetime ] = None ) -> DataFrame Phenotyping based on CCAM codes. PARAMETER DESCRIPTION procedure_occurrence procedure_occurrence OMOP DataFrame. TYPE: DataFrame visit_occurrence visit_occurrence OMOP DataFrame, only necessary if date_from_visit is set to True . TYPE: Optional [ DataFrame ] DEFAULT: None codes Dictionary which values are CCAM codes (as a unique string or as a list) and which keys are at least one of the following: exact : To match the codes in codes[\"exact\"] exactly prefix : To match the codes in codes[\"prefix\"] as prefixes regex : To match the codes in codes[\"regex\"] as regexes You can combine any of those keys. TYPE: Dict [ str , Union [ str , List [ str ]]] DEFAULT: None date_from_visit If set to True , uses visit_start_datetime as the code datetime TYPE: bool DEFAULT: True additional_filtering An optional dictionary to filter the resulting DataFrame. Keys should be column names on which to filter, and values should be either A single value A list or set of values. TYPE: Dict [ str , Any ] DEFAULT: dict() date_min The minimum code datetime to keep. Depends on the date_from_visit flag TYPE: Optional [ datetime ] DEFAULT: None date_max The minimum code datetime to keep. Depends on the date_from_visit flag TYPE: Optional [ datetime ] DEFAULT: None RETURNS DESCRIPTION DataFrame \"event\" DataFrame including the following columns: t_start : If date_from_visit is set to False , contains procedure_datetime , else contains visit_start_datetime t_end : If date_from_visit is set to False , contains procedure_datetime , else contains visit_end_datetime concept : contaning values from codes.keys() value : The extracted CCAM code. visit_occurrence_id : the visit_occurrence_id from the visit which contains the CCAM code. Source code in eds_scikit/event/ccam.py 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 def procedures_from_ccam ( procedure_occurrence : DataFrame , visit_occurrence : Optional [ DataFrame ] = None , codes : Optional [ Dict [ str , Union [ str , List [ str ]]]] = None , date_from_visit : bool = True , additional_filtering = dict (), date_min : Optional [ datetime ] = None , date_max : Optional [ datetime ] = None , ) -> DataFrame : \"\"\" Phenotyping based on CCAM codes. Parameters ---------- procedure_occurrence : DataFrame `procedure_occurrence` OMOP DataFrame. visit_occurrence : Optional[DataFrame] `visit_occurrence` OMOP DataFrame, only necessary if `date_from_visit` is set to `True`. codes : Dict[str, Union[str, List[str]]] Dictionary which values are CCAM codes (as a unique string or as a list) and which keys are at least one of the following: - `exact`: To match the codes in `codes[\"exact\"]` **exactly** - `prefix`: To match the codes in `codes[\"prefix\"]` **as prefixes** - `regex`: To match the codes in `codes[\"regex\"]` **as regexes** You can combine any of those keys. date_from_visit : bool If set to `True`, uses `visit_start_datetime` as the code datetime additional_filtering : Dict[str, Any] An optional dictionary to filter the resulting DataFrame. Keys should be column names on which to filter, and values should be either - A single value - A list or set of values. date_min : Optional[datetime] The minimum code datetime to keep. **Depends on the `date_from_visit` flag** date_max : Optional[datetime] The minimum code datetime to keep. **Depends on the `date_from_visit` flag** Returns ------- DataFrame \"event\" DataFrame including the following columns: - `t_start`: If `date_from_visit` is set to `False`, contains `procedure_datetime`, else contains `visit_start_datetime` - `t_end`: If `date_from_visit` is set to `False`, contains `procedure_datetime`, else contains `visit_end_datetime` - `concept` : contaning values from `codes.keys()` - `value` : The extracted CCAM code. - `visit_occurrence_id` : the `visit_occurrence_id` from the visit which contains the CCAM code. \"\"\" # noqa: E501 procedure_columns = dict ( code_source_value = \"procedure_source_value\" , code_start_datetime = \"procedure_datetime\" , code_end_datetime = \"procedure_datetime\" , ) events = [] for concept , code_dict in codes . items (): tmp_df = event_from_code ( df = procedure_occurrence , columns = procedure_columns , visit_occurrence = visit_occurrence , concept = concept , codes = code_dict , date_from_visit = date_from_visit , additional_filtering = additional_filtering , date_min = date_min , date_max = date_max , ) events . append ( tmp_df ) framework = get_framework ( procedure_occurrence ) return framework . concat ( events )","title":"ccam"},{"location":"reference/event/ccam/#eds_scikiteventccam","text":"","title":"eds_scikit.event.ccam"},{"location":"reference/event/ccam/#eds_scikit.event.ccam.procedures_from_ccam","text":"procedures_from_ccam ( procedure_occurrence : DataFrame , visit_occurrence : Optional [ DataFrame ] = None , codes : Optional [ Dict [ str , Union [ str , List [ str ]]]] = None , date_from_visit : bool = True , additional_filtering = dict (), date_min : Optional [ datetime ] = None , date_max : Optional [ datetime ] = None ) -> DataFrame Phenotyping based on CCAM codes. PARAMETER DESCRIPTION procedure_occurrence procedure_occurrence OMOP DataFrame. TYPE: DataFrame visit_occurrence visit_occurrence OMOP DataFrame, only necessary if date_from_visit is set to True . TYPE: Optional [ DataFrame ] DEFAULT: None codes Dictionary which values are CCAM codes (as a unique string or as a list) and which keys are at least one of the following: exact : To match the codes in codes[\"exact\"] exactly prefix : To match the codes in codes[\"prefix\"] as prefixes regex : To match the codes in codes[\"regex\"] as regexes You can combine any of those keys. TYPE: Dict [ str , Union [ str , List [ str ]]] DEFAULT: None date_from_visit If set to True , uses visit_start_datetime as the code datetime TYPE: bool DEFAULT: True additional_filtering An optional dictionary to filter the resulting DataFrame. Keys should be column names on which to filter, and values should be either A single value A list or set of values. TYPE: Dict [ str , Any ] DEFAULT: dict() date_min The minimum code datetime to keep. Depends on the date_from_visit flag TYPE: Optional [ datetime ] DEFAULT: None date_max The minimum code datetime to keep. Depends on the date_from_visit flag TYPE: Optional [ datetime ] DEFAULT: None RETURNS DESCRIPTION DataFrame \"event\" DataFrame including the following columns: t_start : If date_from_visit is set to False , contains procedure_datetime , else contains visit_start_datetime t_end : If date_from_visit is set to False , contains procedure_datetime , else contains visit_end_datetime concept : contaning values from codes.keys() value : The extracted CCAM code. visit_occurrence_id : the visit_occurrence_id from the visit which contains the CCAM code. Source code in eds_scikit/event/ccam.py 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 def procedures_from_ccam ( procedure_occurrence : DataFrame , visit_occurrence : Optional [ DataFrame ] = None , codes : Optional [ Dict [ str , Union [ str , List [ str ]]]] = None , date_from_visit : bool = True , additional_filtering = dict (), date_min : Optional [ datetime ] = None , date_max : Optional [ datetime ] = None , ) -> DataFrame : \"\"\" Phenotyping based on CCAM codes. Parameters ---------- procedure_occurrence : DataFrame `procedure_occurrence` OMOP DataFrame. visit_occurrence : Optional[DataFrame] `visit_occurrence` OMOP DataFrame, only necessary if `date_from_visit` is set to `True`. codes : Dict[str, Union[str, List[str]]] Dictionary which values are CCAM codes (as a unique string or as a list) and which keys are at least one of the following: - `exact`: To match the codes in `codes[\"exact\"]` **exactly** - `prefix`: To match the codes in `codes[\"prefix\"]` **as prefixes** - `regex`: To match the codes in `codes[\"regex\"]` **as regexes** You can combine any of those keys. date_from_visit : bool If set to `True`, uses `visit_start_datetime` as the code datetime additional_filtering : Dict[str, Any] An optional dictionary to filter the resulting DataFrame. Keys should be column names on which to filter, and values should be either - A single value - A list or set of values. date_min : Optional[datetime] The minimum code datetime to keep. **Depends on the `date_from_visit` flag** date_max : Optional[datetime] The minimum code datetime to keep. **Depends on the `date_from_visit` flag** Returns ------- DataFrame \"event\" DataFrame including the following columns: - `t_start`: If `date_from_visit` is set to `False`, contains `procedure_datetime`, else contains `visit_start_datetime` - `t_end`: If `date_from_visit` is set to `False`, contains `procedure_datetime`, else contains `visit_end_datetime` - `concept` : contaning values from `codes.keys()` - `value` : The extracted CCAM code. - `visit_occurrence_id` : the `visit_occurrence_id` from the visit which contains the CCAM code. \"\"\" # noqa: E501 procedure_columns = dict ( code_source_value = \"procedure_source_value\" , code_start_datetime = \"procedure_datetime\" , code_end_datetime = \"procedure_datetime\" , ) events = [] for concept , code_dict in codes . items (): tmp_df = event_from_code ( df = procedure_occurrence , columns = procedure_columns , visit_occurrence = visit_occurrence , concept = concept , codes = code_dict , date_from_visit = date_from_visit , additional_filtering = additional_filtering , date_min = date_min , date_max = date_max , ) events . append ( tmp_df ) framework = get_framework ( procedure_occurrence ) return framework . concat ( events )","title":"procedures_from_ccam()"},{"location":"reference/event/consultations/","text":"eds_scikit.event.consultations get_consultation_dates get_consultation_dates ( vo : DataFrame , note : DataFrame , note_nlp : Optional [ DataFrame ] = None , algo : Union [ str , List [ str ]] = [ 'nlp' ], max_timedelta : timedelta = timedelta ( days = 7 ), structured_config : Dict [ str , Any ] = dict (), nlp_config : Dict [ str , Any ] = dict ()) -> DataFrame Extract consultation dates. See the implementation details of the algo(s) you want to use PARAMETER DESCRIPTION vo visit_occurrence DataFrame TYPE: DataFrame note note DataFrame TYPE: DataFrame note_nlp note_nlp DataFrame, used only with the \"nlp\" algo TYPE: Optional [ DataFrame ] DEFAULT: None algo Algorithm(s) to use to determine consultation dates. Multiple algorithms can be provided as a list. Accepted values are: \"structured\" : See get_consultation_dates_structured() \"nlp\" : See get_consultation_dates_nlp() TYPE: Union [ str , List [ str ]] DEFAULT: ['nlp'] max_timedelta If two extracted consultations are spaced by less than max_timedelta , we consider that they correspond to the same event and only keep the first one. TYPE: timedelta DEFAULT: timedelta(days=7) structured_config A dictionnary of parameters when using the structured algorithm TYPE: Dict [ str , Any ] DEFAULT: dict() nlp_config A dictionnary of parameters when using the nlp algorithm TYPE: Dict [ str , Any ] DEFAULT: dict() RETURNS DESCRIPTION DataFrame Event type DataFrame with the following columns: person_id visit_occurrence_id CONSULTATION_DATE : corresponds to the note_datetime value of a consultation report coming from the considered visit. CONSULTATION_NOTE_ID : the note_id of the corresponding report. CONSULTATION_DATE_EXTRACTION : the method of extraction Source code in eds_scikit/event/consultations.py 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 @concept_checker ( concepts = [ \"CONSULTATION_DATE\" , \"CONSULTATION_ID\" , \"CONSULTATION_DATE_EXTRACTION\" , ] ) def get_consultation_dates ( vo : DataFrame , note : DataFrame , note_nlp : Optional [ DataFrame ] = None , algo : Union [ str , List [ str ]] = [ \"nlp\" ], max_timedelta : timedelta = timedelta ( days = 7 ), structured_config : Dict [ str , Any ] = dict (), nlp_config : Dict [ str , Any ] = dict (), ) -> DataFrame : \"\"\" Extract consultation dates. See the implementation details of the algo(s) you want to use Parameters ---------- vo : DataFrame `visit_occurrence` DataFrame note : DataFrame `note` DataFrame note_nlp : Optional[DataFrame] `note_nlp` DataFrame, used only with the `\"nlp\"` algo algo: Union[str, List[str]] = [\"nlp\"] Algorithm(s) to use to determine consultation dates. Multiple algorithms can be provided as a list. Accepted values are: - `\"structured\"`: See [get_consultation_dates_structured()][eds_scikit.event.consultations.get_consultation_dates_structured] - `\"nlp\"`: See [get_consultation_dates_nlp()][eds_scikit.event.consultations.get_consultation_dates_nlp] max_timedelta: timedelta = timedelta(days=7) If two extracted consultations are spaced by less than `max_timedelta`, we consider that they correspond to the same event and only keep the first one. structured_config : Dict[str, Any] = dict() A dictionnary of parameters when using the [`structured`][eds_scikit.event.consultations.get_consultation_dates_structured] algorithm nlp_config : Dict[str, Any] = dict() A dictionnary of parameters when using the [`nlp`][eds_scikit.event.consultations.get_consultation_dates_nlp] algorithm Returns ------- DataFrame Event type DataFrame with the following columns: - `person_id` - `visit_occurrence_id` - `CONSULTATION_DATE`: corresponds to the `note_datetime` value of a consultation report coming from the considered visit. - `CONSULTATION_NOTE_ID`: the `note_id` of the corresponding report. - `CONSULTATION_DATE_EXTRACTION`: the method of extraction \"\"\" fw = get_framework ( vo ) if type ( algo ) == str : algo = [ algo ] dates = [] for a in algo : if a == \"structured\" : dates . append ( get_consultation_dates_structured ( vo = vo , note = note , ** structured_config , ) ) if a == \"nlp\" : dates . append ( get_consultation_dates_nlp ( note_nlp = note_nlp , ** nlp_config , ) ) dates_per_note = ( fw . concat ( dates ) . reset_index () . merge ( note [[ \"note_id\" , \"visit_occurrence_id\" ]], on = \"note_id\" , how = \"inner\" ) ) # Remove timezone errors from spark dates_per_note [ \"CONSULTATION_DATE\" ] = dates_per_note [ \"CONSULTATION_DATE\" ] . astype ( str ) dates_per_visit = ( dates_per_note . groupby ([ \"visit_occurrence_id\" , \"CONSULTATION_DATE\" ])[ \"CONSULTATION_DATE_EXTRACTION\" ] . unique () . apply ( sorted ) . str . join ( \"+\" ) ) dates_per_visit . name = \"CONSULTATION_DATE_EXTRACTION\" dates_per_visit = bd . add_unique_id ( dates_per_visit . reset_index (), col_name = \"TMP_CONSULTATION_ID\" ) # Convert back to datetime format dates_per_visit [ \"CONSULTATION_DATE\" ] = bd . to_datetime ( dates_per_visit [ \"CONSULTATION_DATE\" ], errors = \"coerce\" ) dates_per_visit = clean_consultations ( dates_per_visit , max_timedelta , ) # Equivalent to df.spark.cache() for ks.DataFrame bd . cache ( dates_per_visit ) return dates_per_visit get_consultation_dates_structured get_consultation_dates_structured ( note : DataFrame , vo : Optional [ DataFrame ] = None , kept_note_class_source_value : Optional [ Union [ str , List [ str ]]] = 'CR-CONS' , kept_visit_source_value : Optional [ Union [ str , List [ str ]]] = 'consultation externe' ) -> DataFrame Uses note_datetime value to infer true consultation dates PARAMETER DESCRIPTION note A note DataFrame with at least the following columns: note_id note_datetime note_source_value if kept_note_class_source_value is not None visit_occurrence_id if kept_visit_source_value is not None TYPE: DataFrame vo A visit_occurrence DataFrame to provide if kept_visit_source_value is not None , with at least the following columns: visit_occurrence_id visit_source_value if kept_visit_source_value is not None TYPE: Optional [ DataFrame ] DEFAULT: None kept_note_class_source_value Value(s) allowed for the note_class_source_value column. TYPE: Optional [ Union [ str , List [ str ]]] DEFAULT: 'CR-CONS' kept_visit_source_value Value(s) allowed for the visit_source_value column. TYPE: Optional [ Union [ str , List [ str ]]], optional DEFAULT: 'consultation externe' RETURNS DESCRIPTION Dataframe With 2 added columns corresponding to the following concept: CONSULTATION_DATE , containing the date CONSULTATION_DATE_EXTRACTION , containing \"STRUCTURED\" Source code in eds_scikit/event/consultations.py 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 def get_consultation_dates_structured ( note : DataFrame , vo : Optional [ DataFrame ] = None , kept_note_class_source_value : Optional [ Union [ str , List [ str ]]] = \"CR-CONS\" , kept_visit_source_value : Optional [ Union [ str , List [ str ]]] = \"consultation externe\" , ) -> DataFrame : \"\"\" Uses `note_datetime` value to infer *true* consultation dates Parameters ---------- note : DataFrame A `note` DataFrame with at least the following columns: - `note_id` - `note_datetime` - `note_source_value` **if** `kept_note_class_source_value is not None` - `visit_occurrence_id` **if** `kept_visit_source_value is not None` vo : Optional[DataFrame] A visit_occurrence DataFrame to provide **if** `kept_visit_source_value is not None`, with at least the following columns: - `visit_occurrence_id` - `visit_source_value` **if** `kept_visit_source_value is not None` kept_note_class_source_value : Optional[Union[str, List[str]]] Value(s) allowed for the `note_class_source_value` column. kept_visit_source_value : Optional[Union[str, List[str]]], optional Value(s) allowed for the `visit_source_value` column. Returns ------- Dataframe With 2 added columns corresponding to the following concept: - `CONSULTATION_DATE`, containing the date - `CONSULTATION_DATE_EXTRACTION`, containing `\"STRUCTURED\"` \"\"\" kept_note = note if kept_note_class_source_value is not None : if type ( kept_note_class_source_value ) == str : kept_note_class_source_value = [ kept_note_class_source_value ] kept_note = note [ note . note_class_source_value . isin ( set ( kept_note_class_source_value )) ] if kept_visit_source_value is not None : if type ( kept_visit_source_value ) == str : kept_visit_source_value = [ kept_visit_source_value ] kept_note = kept_note . merge ( vo [ [ \"visit_occurrence_id\" , \"visit_source_value\" , ] ][ vo . visit_source_value . isin ( set ( kept_visit_source_value ))], on = \"visit_occurrence_id\" , ) dates_per_note = kept_note [[ \"note_datetime\" , \"note_id\" ]] . rename ( columns = { \"note_datetime\" : \"CONSULTATION_DATE\" , } ) dates_per_note [ \"CONSULTATION_DATE_EXTRACTION\" ] = \"STRUCTURED\" return dates_per_note . set_index ( \"note_id\" ) get_consultation_dates_nlp get_consultation_dates_nlp ( note_nlp : DataFrame , dates_to_keep : str = 'min' ) -> DataFrame Uses consultation dates extracted a priori in consultation reports to infer true consultation dates PARAMETER DESCRIPTION note_nlp A DataFrame with (at least) the following columns: note_id consultation_date end if using dates_to_keep=first : end should store the character offset of the extracted date. TYPE: DataFrame dates_to_keep How to handle multiple consultation dates found in the document: min : keep the oldest one first : keep the occurrence that appeared first in the text all : keep all date TYPE: str , optional DEFAULT: 'min' RETURNS DESCRIPTION Dataframe With 2 added columns corresponding to the following concept: CONSULTATION_DATE , containing the date CONSULTATION_DATE_EXTRACTION , containing \"NLP\" Source code in eds_scikit/event/consultations.py 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 def get_consultation_dates_nlp ( note_nlp : DataFrame , dates_to_keep : str = \"min\" , ) -> DataFrame : \"\"\" Uses consultation dates extracted *a priori* in consultation reports to infer *true* consultation dates Parameters ---------- note_nlp : DataFrame A DataFrame with (at least) the following columns: - `note_id` - `consultation_date` - `end` **if** using `dates_to_keep=first`: `end` should store the character offset of the extracted date. dates_to_keep : str, optional How to handle multiple consultation dates found in the document: - `min`: keep the oldest one - `first`: keep the occurrence that appeared first in the text - `all`: keep all date Returns ------- Dataframe With 2 added columns corresponding to the following concept: - `CONSULTATION_DATE`, containing the date - `CONSULTATION_DATE_EXTRACTION`, containing `\"NLP\"` \"\"\" if dates_to_keep == \"min\" : dates_per_note = note_nlp . groupby ( \"note_id\" ) . agg ( CONSULTATION_DATE = ( \"consultation_date\" , \"min\" ), ) elif dates_to_keep == \"first\" : dates_per_note = ( note_nlp . sort_values ( by = \"start\" ) . groupby ( \"note_id\" ) . agg ( CONSULTATION_DATE = ( \"consultation_date\" , \"first\" )) ) elif dates_to_keep == \"all\" : dates_per_note = note_nlp [[ \"consultation_date\" , \"note_id\" ]] . set_index ( \"note_id\" ) dates_per_note = dates_per_note . rename ( columns = { \"consultation_date\" : \"CONSULTATION_DATE\" } ) dates_per_note [ \"CONSULTATION_DATE_EXTRACTION\" ] = \"NLP\" return dates_per_note","title":"consultations"},{"location":"reference/event/consultations/#eds_scikiteventconsultations","text":"","title":"eds_scikit.event.consultations"},{"location":"reference/event/consultations/#eds_scikit.event.consultations.get_consultation_dates","text":"get_consultation_dates ( vo : DataFrame , note : DataFrame , note_nlp : Optional [ DataFrame ] = None , algo : Union [ str , List [ str ]] = [ 'nlp' ], max_timedelta : timedelta = timedelta ( days = 7 ), structured_config : Dict [ str , Any ] = dict (), nlp_config : Dict [ str , Any ] = dict ()) -> DataFrame Extract consultation dates. See the implementation details of the algo(s) you want to use PARAMETER DESCRIPTION vo visit_occurrence DataFrame TYPE: DataFrame note note DataFrame TYPE: DataFrame note_nlp note_nlp DataFrame, used only with the \"nlp\" algo TYPE: Optional [ DataFrame ] DEFAULT: None algo Algorithm(s) to use to determine consultation dates. Multiple algorithms can be provided as a list. Accepted values are: \"structured\" : See get_consultation_dates_structured() \"nlp\" : See get_consultation_dates_nlp() TYPE: Union [ str , List [ str ]] DEFAULT: ['nlp'] max_timedelta If two extracted consultations are spaced by less than max_timedelta , we consider that they correspond to the same event and only keep the first one. TYPE: timedelta DEFAULT: timedelta(days=7) structured_config A dictionnary of parameters when using the structured algorithm TYPE: Dict [ str , Any ] DEFAULT: dict() nlp_config A dictionnary of parameters when using the nlp algorithm TYPE: Dict [ str , Any ] DEFAULT: dict() RETURNS DESCRIPTION DataFrame Event type DataFrame with the following columns: person_id visit_occurrence_id CONSULTATION_DATE : corresponds to the note_datetime value of a consultation report coming from the considered visit. CONSULTATION_NOTE_ID : the note_id of the corresponding report. CONSULTATION_DATE_EXTRACTION : the method of extraction Source code in eds_scikit/event/consultations.py 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 @concept_checker ( concepts = [ \"CONSULTATION_DATE\" , \"CONSULTATION_ID\" , \"CONSULTATION_DATE_EXTRACTION\" , ] ) def get_consultation_dates ( vo : DataFrame , note : DataFrame , note_nlp : Optional [ DataFrame ] = None , algo : Union [ str , List [ str ]] = [ \"nlp\" ], max_timedelta : timedelta = timedelta ( days = 7 ), structured_config : Dict [ str , Any ] = dict (), nlp_config : Dict [ str , Any ] = dict (), ) -> DataFrame : \"\"\" Extract consultation dates. See the implementation details of the algo(s) you want to use Parameters ---------- vo : DataFrame `visit_occurrence` DataFrame note : DataFrame `note` DataFrame note_nlp : Optional[DataFrame] `note_nlp` DataFrame, used only with the `\"nlp\"` algo algo: Union[str, List[str]] = [\"nlp\"] Algorithm(s) to use to determine consultation dates. Multiple algorithms can be provided as a list. Accepted values are: - `\"structured\"`: See [get_consultation_dates_structured()][eds_scikit.event.consultations.get_consultation_dates_structured] - `\"nlp\"`: See [get_consultation_dates_nlp()][eds_scikit.event.consultations.get_consultation_dates_nlp] max_timedelta: timedelta = timedelta(days=7) If two extracted consultations are spaced by less than `max_timedelta`, we consider that they correspond to the same event and only keep the first one. structured_config : Dict[str, Any] = dict() A dictionnary of parameters when using the [`structured`][eds_scikit.event.consultations.get_consultation_dates_structured] algorithm nlp_config : Dict[str, Any] = dict() A dictionnary of parameters when using the [`nlp`][eds_scikit.event.consultations.get_consultation_dates_nlp] algorithm Returns ------- DataFrame Event type DataFrame with the following columns: - `person_id` - `visit_occurrence_id` - `CONSULTATION_DATE`: corresponds to the `note_datetime` value of a consultation report coming from the considered visit. - `CONSULTATION_NOTE_ID`: the `note_id` of the corresponding report. - `CONSULTATION_DATE_EXTRACTION`: the method of extraction \"\"\" fw = get_framework ( vo ) if type ( algo ) == str : algo = [ algo ] dates = [] for a in algo : if a == \"structured\" : dates . append ( get_consultation_dates_structured ( vo = vo , note = note , ** structured_config , ) ) if a == \"nlp\" : dates . append ( get_consultation_dates_nlp ( note_nlp = note_nlp , ** nlp_config , ) ) dates_per_note = ( fw . concat ( dates ) . reset_index () . merge ( note [[ \"note_id\" , \"visit_occurrence_id\" ]], on = \"note_id\" , how = \"inner\" ) ) # Remove timezone errors from spark dates_per_note [ \"CONSULTATION_DATE\" ] = dates_per_note [ \"CONSULTATION_DATE\" ] . astype ( str ) dates_per_visit = ( dates_per_note . groupby ([ \"visit_occurrence_id\" , \"CONSULTATION_DATE\" ])[ \"CONSULTATION_DATE_EXTRACTION\" ] . unique () . apply ( sorted ) . str . join ( \"+\" ) ) dates_per_visit . name = \"CONSULTATION_DATE_EXTRACTION\" dates_per_visit = bd . add_unique_id ( dates_per_visit . reset_index (), col_name = \"TMP_CONSULTATION_ID\" ) # Convert back to datetime format dates_per_visit [ \"CONSULTATION_DATE\" ] = bd . to_datetime ( dates_per_visit [ \"CONSULTATION_DATE\" ], errors = \"coerce\" ) dates_per_visit = clean_consultations ( dates_per_visit , max_timedelta , ) # Equivalent to df.spark.cache() for ks.DataFrame bd . cache ( dates_per_visit ) return dates_per_visit","title":"get_consultation_dates()"},{"location":"reference/event/consultations/#eds_scikit.event.consultations.get_consultation_dates_structured","text":"get_consultation_dates_structured ( note : DataFrame , vo : Optional [ DataFrame ] = None , kept_note_class_source_value : Optional [ Union [ str , List [ str ]]] = 'CR-CONS' , kept_visit_source_value : Optional [ Union [ str , List [ str ]]] = 'consultation externe' ) -> DataFrame Uses note_datetime value to infer true consultation dates PARAMETER DESCRIPTION note A note DataFrame with at least the following columns: note_id note_datetime note_source_value if kept_note_class_source_value is not None visit_occurrence_id if kept_visit_source_value is not None TYPE: DataFrame vo A visit_occurrence DataFrame to provide if kept_visit_source_value is not None , with at least the following columns: visit_occurrence_id visit_source_value if kept_visit_source_value is not None TYPE: Optional [ DataFrame ] DEFAULT: None kept_note_class_source_value Value(s) allowed for the note_class_source_value column. TYPE: Optional [ Union [ str , List [ str ]]] DEFAULT: 'CR-CONS' kept_visit_source_value Value(s) allowed for the visit_source_value column. TYPE: Optional [ Union [ str , List [ str ]]], optional DEFAULT: 'consultation externe' RETURNS DESCRIPTION Dataframe With 2 added columns corresponding to the following concept: CONSULTATION_DATE , containing the date CONSULTATION_DATE_EXTRACTION , containing \"STRUCTURED\" Source code in eds_scikit/event/consultations.py 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 def get_consultation_dates_structured ( note : DataFrame , vo : Optional [ DataFrame ] = None , kept_note_class_source_value : Optional [ Union [ str , List [ str ]]] = \"CR-CONS\" , kept_visit_source_value : Optional [ Union [ str , List [ str ]]] = \"consultation externe\" , ) -> DataFrame : \"\"\" Uses `note_datetime` value to infer *true* consultation dates Parameters ---------- note : DataFrame A `note` DataFrame with at least the following columns: - `note_id` - `note_datetime` - `note_source_value` **if** `kept_note_class_source_value is not None` - `visit_occurrence_id` **if** `kept_visit_source_value is not None` vo : Optional[DataFrame] A visit_occurrence DataFrame to provide **if** `kept_visit_source_value is not None`, with at least the following columns: - `visit_occurrence_id` - `visit_source_value` **if** `kept_visit_source_value is not None` kept_note_class_source_value : Optional[Union[str, List[str]]] Value(s) allowed for the `note_class_source_value` column. kept_visit_source_value : Optional[Union[str, List[str]]], optional Value(s) allowed for the `visit_source_value` column. Returns ------- Dataframe With 2 added columns corresponding to the following concept: - `CONSULTATION_DATE`, containing the date - `CONSULTATION_DATE_EXTRACTION`, containing `\"STRUCTURED\"` \"\"\" kept_note = note if kept_note_class_source_value is not None : if type ( kept_note_class_source_value ) == str : kept_note_class_source_value = [ kept_note_class_source_value ] kept_note = note [ note . note_class_source_value . isin ( set ( kept_note_class_source_value )) ] if kept_visit_source_value is not None : if type ( kept_visit_source_value ) == str : kept_visit_source_value = [ kept_visit_source_value ] kept_note = kept_note . merge ( vo [ [ \"visit_occurrence_id\" , \"visit_source_value\" , ] ][ vo . visit_source_value . isin ( set ( kept_visit_source_value ))], on = \"visit_occurrence_id\" , ) dates_per_note = kept_note [[ \"note_datetime\" , \"note_id\" ]] . rename ( columns = { \"note_datetime\" : \"CONSULTATION_DATE\" , } ) dates_per_note [ \"CONSULTATION_DATE_EXTRACTION\" ] = \"STRUCTURED\" return dates_per_note . set_index ( \"note_id\" )","title":"get_consultation_dates_structured()"},{"location":"reference/event/consultations/#eds_scikit.event.consultations.get_consultation_dates_nlp","text":"get_consultation_dates_nlp ( note_nlp : DataFrame , dates_to_keep : str = 'min' ) -> DataFrame Uses consultation dates extracted a priori in consultation reports to infer true consultation dates PARAMETER DESCRIPTION note_nlp A DataFrame with (at least) the following columns: note_id consultation_date end if using dates_to_keep=first : end should store the character offset of the extracted date. TYPE: DataFrame dates_to_keep How to handle multiple consultation dates found in the document: min : keep the oldest one first : keep the occurrence that appeared first in the text all : keep all date TYPE: str , optional DEFAULT: 'min' RETURNS DESCRIPTION Dataframe With 2 added columns corresponding to the following concept: CONSULTATION_DATE , containing the date CONSULTATION_DATE_EXTRACTION , containing \"NLP\" Source code in eds_scikit/event/consultations.py 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 def get_consultation_dates_nlp ( note_nlp : DataFrame , dates_to_keep : str = \"min\" , ) -> DataFrame : \"\"\" Uses consultation dates extracted *a priori* in consultation reports to infer *true* consultation dates Parameters ---------- note_nlp : DataFrame A DataFrame with (at least) the following columns: - `note_id` - `consultation_date` - `end` **if** using `dates_to_keep=first`: `end` should store the character offset of the extracted date. dates_to_keep : str, optional How to handle multiple consultation dates found in the document: - `min`: keep the oldest one - `first`: keep the occurrence that appeared first in the text - `all`: keep all date Returns ------- Dataframe With 2 added columns corresponding to the following concept: - `CONSULTATION_DATE`, containing the date - `CONSULTATION_DATE_EXTRACTION`, containing `\"NLP\"` \"\"\" if dates_to_keep == \"min\" : dates_per_note = note_nlp . groupby ( \"note_id\" ) . agg ( CONSULTATION_DATE = ( \"consultation_date\" , \"min\" ), ) elif dates_to_keep == \"first\" : dates_per_note = ( note_nlp . sort_values ( by = \"start\" ) . groupby ( \"note_id\" ) . agg ( CONSULTATION_DATE = ( \"consultation_date\" , \"first\" )) ) elif dates_to_keep == \"all\" : dates_per_note = note_nlp [[ \"consultation_date\" , \"note_id\" ]] . set_index ( \"note_id\" ) dates_per_note = dates_per_note . rename ( columns = { \"consultation_date\" : \"CONSULTATION_DATE\" } ) dates_per_note [ \"CONSULTATION_DATE_EXTRACTION\" ] = \"NLP\" return dates_per_note","title":"get_consultation_dates_nlp()"},{"location":"reference/event/diabetes/","text":"eds_scikit.event.diabetes DEFAULT_DIABETE_FROM_ICD10_CONFIG module-attribute DEFAULT_DIABETE_FROM_ICD10_CONFIG = dict ( codes = dict ( DIABETES_TYPE_I = dict ( prefix = 'E10' ), DIABETES_TYPE_II = dict ( prefix = 'E11' ), DIABETES_MALNUTRITION = dict ( prefix = 'E12' ), DIABETES_IN_PREGNANCY = dict ( prefix = 'O24' ), OTHER_DIABETES_MELLITUS = dict ( prefix = [ 'E13' , 'E14' ]), DIABETES_INSIPIDUS = dict ( exact = [ 'E232' , 'N251' ])), date_from_visit = True , additional_filtering = dict ( condition_status_source_value = { 'DP' , 'DAS' })) Default parameters feeded to conditions_from_icd10() diabetes_from_icd10 diabetes_from_icd10 ( condition_occurrence : DataFrame , visit_occurrence : DataFrame , date_min : Optional [ datetime ] = None , date_max : Optional [ datetime ] = None , codes : Dict [ str , Union [ str , List [ str ]]] = DEFAULT_DIABETE_FROM_ICD10_CONFIG [ 'codes' ], date_from_visit : bool = DEFAULT_DIABETE_FROM_ICD10_CONFIG [ 'date_from_visit' ], additional_filtering : Dict [ str , Any ] = DEFAULT_DIABETE_FROM_ICD10_CONFIG [ 'additional_filtering' ]) -> DataFrame Wrapper around the conditions_from_icd10() function. Check the default configuration to see the used parameters PARAMETER DESCRIPTION condition_occurrence OMOP-like condition occurrence DataFrame TYPE: DataFrame visit_occurrence OMOP-like visit_occurrence DataFrame TYPE: Optional [ DataFrame ] date_min Lower temporal bound TYPE: Optional [ datetime ] DEFAULT: None date_max Upper temporal bound TYPE: Optional [ datetime ] DEFAULT: None codes Dictionary of ICD-10 used for phenotyping TYPE: Optional [ Dict [ str , Union [ str , List [ str ]]]] DEFAULT: DEFAULT_DIABETE_FROM_ICD10_CONFIG['codes'] date_from_visit If true, use the visit_[start/end]_datetime for filtering. Else, use condition_start_datetime TYPE: bool, by default True DEFAULT: DEFAULT_DIABETE_FROM_ICD10_CONFIG['date_from_visit'] additional_filtering A dictionary to perform additional filtering. Each key should be a valid column name from condition_occurrence Each value should be a value / set of values / list of values For each pair (key, value), filtering is done as condition_occurrence[condition_occurrence[k].isin(v)] TYPE: Dict [ str , Any ] DEFAULT: DEFAULT_DIABETE_FROM_ICD10_CONFIG['additional_filtering'] RETURNS DESCRIPTION DataFrame Event DataFrame in long format (with a concept and a value column). The concept column contains one of the following: DIABETES_TYPE_I DIABETES_TYPE_II DIABETES_MALNUTRITION DIABETES_IN_PREGNANCY OTHER_DIABETES_MELLITUS DIABETES_INSIPIDUS The value column contains the corresponding ICD-10 code that was extracted Source code in eds_scikit/event/diabetes.py 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 def diabetes_from_icd10 ( condition_occurrence : DataFrame , visit_occurrence : DataFrame , date_min : Optional [ datetime ] = None , date_max : Optional [ datetime ] = None , codes : Dict [ str , Union [ str , List [ str ]]] = DEFAULT_DIABETE_FROM_ICD10_CONFIG [ \"codes\" ], date_from_visit : bool = DEFAULT_DIABETE_FROM_ICD10_CONFIG [ \"date_from_visit\" ], additional_filtering : Dict [ str , Any ] = DEFAULT_DIABETE_FROM_ICD10_CONFIG [ \"additional_filtering\" ], ) -> DataFrame : \"\"\" Wrapper around the [conditions_from_icd10()][eds_scikit.event.icd10.conditions_from_icd10] function. Check the [default configuration][eds_scikit.event.diabetes.DEFAULT_DIABETE_FROM_ICD10_CONFIG] to see the used parameters Parameters ---------- condition_occurrence OMOP-like condition occurrence DataFrame visit_occurrence : Optional[DataFrame] OMOP-like visit_occurrence DataFrame date_min : Optional[datetime] Lower temporal bound date_max : Optional[datetime] Upper temporal bound codes : Optional[Dict[str, Union[str, List[str]]]] Dictionary of ICD-10 used for phenotyping date_from_visit : bool, by default True If true, use the `visit_[start/end]_datetime` for filtering. Else, use `condition_start_datetime` additional_filtering : Dict[str, Any] A dictionary to perform additional filtering. - **Each key** should be a valid column name from `condition_occurrence` - **Each value** should be a value / set of values / list of values For each pair (key, value), filtering is done as `condition_occurrence[condition_occurrence[k].isin(v)]` Returns ------- DataFrame Event DataFrame in **long** format (with a `concept` and a `value` column). The `concept` column contains one of the following: - DIABETES_TYPE_I - DIABETES_TYPE_II - DIABETES_MALNUTRITION - DIABETES_IN_PREGNANCY - OTHER_DIABETES_MELLITUS - DIABETES_INSIPIDUS The `value` column contains the corresponding ICD-10 code that was extracted \"\"\" diabetes = conditions_from_icd10 ( condition_occurrence = condition_occurrence , visit_occurrence = visit_occurrence , date_min = date_min , date_max = date_max , codes = codes , date_from_visit = date_from_visit , additional_filtering = additional_filtering , ) diabetes [ \"value\" ] = diabetes [ \"concept\" ] diabetes [ \"concept\" ] = \"DIABETES_FROM_ICD10\" return diabetes","title":"diabetes"},{"location":"reference/event/diabetes/#eds_scikiteventdiabetes","text":"","title":"eds_scikit.event.diabetes"},{"location":"reference/event/diabetes/#eds_scikit.event.diabetes.DEFAULT_DIABETE_FROM_ICD10_CONFIG","text":"DEFAULT_DIABETE_FROM_ICD10_CONFIG = dict ( codes = dict ( DIABETES_TYPE_I = dict ( prefix = 'E10' ), DIABETES_TYPE_II = dict ( prefix = 'E11' ), DIABETES_MALNUTRITION = dict ( prefix = 'E12' ), DIABETES_IN_PREGNANCY = dict ( prefix = 'O24' ), OTHER_DIABETES_MELLITUS = dict ( prefix = [ 'E13' , 'E14' ]), DIABETES_INSIPIDUS = dict ( exact = [ 'E232' , 'N251' ])), date_from_visit = True , additional_filtering = dict ( condition_status_source_value = { 'DP' , 'DAS' })) Default parameters feeded to conditions_from_icd10()","title":"DEFAULT_DIABETE_FROM_ICD10_CONFIG"},{"location":"reference/event/diabetes/#eds_scikit.event.diabetes.diabetes_from_icd10","text":"diabetes_from_icd10 ( condition_occurrence : DataFrame , visit_occurrence : DataFrame , date_min : Optional [ datetime ] = None , date_max : Optional [ datetime ] = None , codes : Dict [ str , Union [ str , List [ str ]]] = DEFAULT_DIABETE_FROM_ICD10_CONFIG [ 'codes' ], date_from_visit : bool = DEFAULT_DIABETE_FROM_ICD10_CONFIG [ 'date_from_visit' ], additional_filtering : Dict [ str , Any ] = DEFAULT_DIABETE_FROM_ICD10_CONFIG [ 'additional_filtering' ]) -> DataFrame Wrapper around the conditions_from_icd10() function. Check the default configuration to see the used parameters PARAMETER DESCRIPTION condition_occurrence OMOP-like condition occurrence DataFrame TYPE: DataFrame visit_occurrence OMOP-like visit_occurrence DataFrame TYPE: Optional [ DataFrame ] date_min Lower temporal bound TYPE: Optional [ datetime ] DEFAULT: None date_max Upper temporal bound TYPE: Optional [ datetime ] DEFAULT: None codes Dictionary of ICD-10 used for phenotyping TYPE: Optional [ Dict [ str , Union [ str , List [ str ]]]] DEFAULT: DEFAULT_DIABETE_FROM_ICD10_CONFIG['codes'] date_from_visit If true, use the visit_[start/end]_datetime for filtering. Else, use condition_start_datetime TYPE: bool, by default True DEFAULT: DEFAULT_DIABETE_FROM_ICD10_CONFIG['date_from_visit'] additional_filtering A dictionary to perform additional filtering. Each key should be a valid column name from condition_occurrence Each value should be a value / set of values / list of values For each pair (key, value), filtering is done as condition_occurrence[condition_occurrence[k].isin(v)] TYPE: Dict [ str , Any ] DEFAULT: DEFAULT_DIABETE_FROM_ICD10_CONFIG['additional_filtering'] RETURNS DESCRIPTION DataFrame Event DataFrame in long format (with a concept and a value column). The concept column contains one of the following: DIABETES_TYPE_I DIABETES_TYPE_II DIABETES_MALNUTRITION DIABETES_IN_PREGNANCY OTHER_DIABETES_MELLITUS DIABETES_INSIPIDUS The value column contains the corresponding ICD-10 code that was extracted Source code in eds_scikit/event/diabetes.py 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 def diabetes_from_icd10 ( condition_occurrence : DataFrame , visit_occurrence : DataFrame , date_min : Optional [ datetime ] = None , date_max : Optional [ datetime ] = None , codes : Dict [ str , Union [ str , List [ str ]]] = DEFAULT_DIABETE_FROM_ICD10_CONFIG [ \"codes\" ], date_from_visit : bool = DEFAULT_DIABETE_FROM_ICD10_CONFIG [ \"date_from_visit\" ], additional_filtering : Dict [ str , Any ] = DEFAULT_DIABETE_FROM_ICD10_CONFIG [ \"additional_filtering\" ], ) -> DataFrame : \"\"\" Wrapper around the [conditions_from_icd10()][eds_scikit.event.icd10.conditions_from_icd10] function. Check the [default configuration][eds_scikit.event.diabetes.DEFAULT_DIABETE_FROM_ICD10_CONFIG] to see the used parameters Parameters ---------- condition_occurrence OMOP-like condition occurrence DataFrame visit_occurrence : Optional[DataFrame] OMOP-like visit_occurrence DataFrame date_min : Optional[datetime] Lower temporal bound date_max : Optional[datetime] Upper temporal bound codes : Optional[Dict[str, Union[str, List[str]]]] Dictionary of ICD-10 used for phenotyping date_from_visit : bool, by default True If true, use the `visit_[start/end]_datetime` for filtering. Else, use `condition_start_datetime` additional_filtering : Dict[str, Any] A dictionary to perform additional filtering. - **Each key** should be a valid column name from `condition_occurrence` - **Each value** should be a value / set of values / list of values For each pair (key, value), filtering is done as `condition_occurrence[condition_occurrence[k].isin(v)]` Returns ------- DataFrame Event DataFrame in **long** format (with a `concept` and a `value` column). The `concept` column contains one of the following: - DIABETES_TYPE_I - DIABETES_TYPE_II - DIABETES_MALNUTRITION - DIABETES_IN_PREGNANCY - OTHER_DIABETES_MELLITUS - DIABETES_INSIPIDUS The `value` column contains the corresponding ICD-10 code that was extracted \"\"\" diabetes = conditions_from_icd10 ( condition_occurrence = condition_occurrence , visit_occurrence = visit_occurrence , date_min = date_min , date_max = date_max , codes = codes , date_from_visit = date_from_visit , additional_filtering = additional_filtering , ) diabetes [ \"value\" ] = diabetes [ \"concept\" ] diabetes [ \"concept\" ] = \"DIABETES_FROM_ICD10\" return diabetes","title":"diabetes_from_icd10()"},{"location":"reference/event/from_code/","text":"eds_scikit.event.from_code event_from_code event_from_code ( df : DataFrame , columns : Dict [ str , str ], visit_occurrence : Optional [ DataFrame ] = None , concept : str = 'ICD10' , codes : Optional [ Dict [ str , Union [ str , List [ str ]]]] = None , date_from_visit : bool = True , additional_filtering : Dict [ str , Any ] = dict (), date_min : Optional [ datetime ] = None , date_max : Optional [ datetime ] = None ) -> DataFrame Generic function to filter a DataFrame based on one of its column and an ensemble of codes to select from. For instance, this function is called when phenotyping via ICD-10 or CCAM. PARAMETER DESCRIPTION df The DataFrame to filter. TYPE: DataFrame columns Dictionary with the following keys: code_source_value : The column name containing the code to filter code_start_datetime : The column name containing the starting date code_end_datetime : The column name containing the ending date TYPE: Dict [ str , str ] visit_occurrence The visit_occurrence DataFrame, only necessary if date_from_visit is set to True . TYPE: Optional [ DataFrame ] DEFAULT: None concept The name of the extracted concept TYPE: str DEFAULT: 'ICD10' codes Dictionary which values are codes (as a unique string or as a list) and which keys are at least one of the following: exact : To match the codes in codes[\"exact\"] exactly prefix : To match the codes in codes[\"prefix\"] as prefixes regex : To match the codes in codes[\"regex\"] as regexes You can combine any of those keys. TYPE: Dict [ str , Union [ str , List [ str ]]] DEFAULT: None date_from_visit If set to True , uses visit_start_datetime as the code datetime TYPE: bool DEFAULT: True additional_filtering An optional dictionary to filter the resulting DataFrame. Keys should be column names on which too filter, and values should be either A single value A list or set of values. TYPE: Dict [ str , Any ] DEFAULT: dict() date_min The minimum code datetime to keep. Depends on the date_from_visit flag TYPE: Optional [ datetime ] DEFAULT: None date_max The minimum code datetime to keep. Depends on the date_from_visit flag TYPE: Optional [ datetime ] DEFAULT: None RETURNS DESCRIPTION DataFrame A DataFrame containing especially the following columns: t_start t_end concept : The provided concept string value : The matched code Source code in eds_scikit/event/from_code.py 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 def event_from_code ( df : DataFrame , columns : Dict [ str , str ], visit_occurrence : Optional [ DataFrame ] = None , concept : str = \"ICD10\" , codes : Optional [ Dict [ str , Union [ str , List [ str ]]]] = None , date_from_visit : bool = True , additional_filtering : Dict [ str , Any ] = dict (), date_min : Optional [ datetime ] = None , date_max : Optional [ datetime ] = None , ) -> DataFrame : \"\"\" Generic function to filter a DataFrame based on one of its column and an ensemble of codes to select from. For instance, this function is called when phenotyping via ICD-10 or CCAM. Parameters ---------- df : DataFrame The DataFrame to filter. columns : Dict[str, str] Dictionary with the following keys: - `code_source_value` : The column name containing the code to filter - `code_start_datetime` : The column name containing the starting date - `code_end_datetime` : The column name containing the ending date visit_occurrence : Optional[DataFrame] The `visit_occurrence` DataFrame, only necessary if `date_from_visit` is set to `True`. concept : str The name of the extracted concept codes : Dict[str, Union[str, List[str]]] Dictionary which values are codes (as a unique string or as a list) and which keys are at least one of the following: - `exact`: To match the codes in `codes[\"exact\"]` **exactly** - `prefix`: To match the codes in `codes[\"prefix\"]` **as prefixes** - `regex`: To match the codes in `codes[\"regex\"]` **as regexes** You can combine any of those keys. date_from_visit : bool If set to `True`, uses `visit_start_datetime` as the code datetime additional_filtering : Dict[str, Any] An optional dictionary to filter the resulting DataFrame. Keys should be column names on which too filter, and values should be either - A single value - A list or set of values. date_min : Optional[datetime] The minimum code datetime to keep. **Depends on the `date_from_visit` flag** date_max : Optional[datetime] The minimum code datetime to keep. **Depends on the `date_from_visit` flag** Returns ------- DataFrame A DataFrame containing especially the following columns: - `t_start` - `t_end` - `concept` : The provided `concept` string - `value` : The matched code \"\"\" required_columns = list ( columns . values ()) + [ \"visit_occurrence_id\" , \"person_id\" ] check_columns ( df , required_columns = required_columns ) d_format = { \"exact\" : r \" {code} \\b\" , \"regex\" : r \" {code} \" , \"prefix\" : r \"\\b {code} \" } regexes = [] for code_type , code_list in codes . items (): if type ( code_list ) == str : code_list = [ code_list ] codes_formated = [ d_format [ code_type ] . format ( code = code ) for code in code_list ] regexes . append ( r \"(?:\" + \"|\" . join ( codes_formated ) + \")\" ) final_regex = \"|\" . join ( regexes ) mask = df [ columns [ \"code_source_value\" ]] . str . contains ( final_regex ) . fillna ( False ) event = df [ mask ] if date_from_visit : if visit_occurrence is None : raise ValueError ( \"With 'date_from_visit=True', you should provide a 'visit_occurrence' DataFrame.\" ) event = event . merge ( visit_occurrence [ [ \"visit_occurrence_id\" , \"visit_start_datetime\" , \"visit_end_datetime\" ] ], on = \"visit_occurrence_id\" , how = \"inner\" , ) . rename ( columns = { \"visit_start_datetime\" : \"t_start\" , \"visit_end_datetime\" : \"t_end\" , } ) else : event . loc [:, \"t_start\" ] = event . loc [:, columns [ \"code_start_datetime\" ]] event . loc [:, \"t_end\" ] = event . loc [:, columns [ \"code_end_datetime\" ]] event = event . drop ( columns = [ columns [ \"code_start_datetime\" ], columns [ \"code_end_datetime\" ]] ) event = _column_filtering ( event , filtering_dict = additional_filtering ) mask = True # Resetting the mask if date_min is not None : mask = mask & ( event . t_start >= date_min ) if date_max is not None : mask = mask & ( event . t_start <= date_max ) if type ( mask ) != bool : # We have a Series mask event = event [ mask ] event [ \"concept\" ] = concept return event . rename ( columns = { columns [ \"code_source_value\" ]: \"value\" })[ [ \"person_id\" , \"t_start\" , \"t_end\" , \"concept\" , \"value\" , \"visit_occurrence_id\" , ] + list ( additional_filtering . keys ()) ] . reset_index ( drop = True )","title":"from_code"},{"location":"reference/event/from_code/#eds_scikiteventfrom_code","text":"","title":"eds_scikit.event.from_code"},{"location":"reference/event/from_code/#eds_scikit.event.from_code.event_from_code","text":"event_from_code ( df : DataFrame , columns : Dict [ str , str ], visit_occurrence : Optional [ DataFrame ] = None , concept : str = 'ICD10' , codes : Optional [ Dict [ str , Union [ str , List [ str ]]]] = None , date_from_visit : bool = True , additional_filtering : Dict [ str , Any ] = dict (), date_min : Optional [ datetime ] = None , date_max : Optional [ datetime ] = None ) -> DataFrame Generic function to filter a DataFrame based on one of its column and an ensemble of codes to select from. For instance, this function is called when phenotyping via ICD-10 or CCAM. PARAMETER DESCRIPTION df The DataFrame to filter. TYPE: DataFrame columns Dictionary with the following keys: code_source_value : The column name containing the code to filter code_start_datetime : The column name containing the starting date code_end_datetime : The column name containing the ending date TYPE: Dict [ str , str ] visit_occurrence The visit_occurrence DataFrame, only necessary if date_from_visit is set to True . TYPE: Optional [ DataFrame ] DEFAULT: None concept The name of the extracted concept TYPE: str DEFAULT: 'ICD10' codes Dictionary which values are codes (as a unique string or as a list) and which keys are at least one of the following: exact : To match the codes in codes[\"exact\"] exactly prefix : To match the codes in codes[\"prefix\"] as prefixes regex : To match the codes in codes[\"regex\"] as regexes You can combine any of those keys. TYPE: Dict [ str , Union [ str , List [ str ]]] DEFAULT: None date_from_visit If set to True , uses visit_start_datetime as the code datetime TYPE: bool DEFAULT: True additional_filtering An optional dictionary to filter the resulting DataFrame. Keys should be column names on which too filter, and values should be either A single value A list or set of values. TYPE: Dict [ str , Any ] DEFAULT: dict() date_min The minimum code datetime to keep. Depends on the date_from_visit flag TYPE: Optional [ datetime ] DEFAULT: None date_max The minimum code datetime to keep. Depends on the date_from_visit flag TYPE: Optional [ datetime ] DEFAULT: None RETURNS DESCRIPTION DataFrame A DataFrame containing especially the following columns: t_start t_end concept : The provided concept string value : The matched code Source code in eds_scikit/event/from_code.py 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 def event_from_code ( df : DataFrame , columns : Dict [ str , str ], visit_occurrence : Optional [ DataFrame ] = None , concept : str = \"ICD10\" , codes : Optional [ Dict [ str , Union [ str , List [ str ]]]] = None , date_from_visit : bool = True , additional_filtering : Dict [ str , Any ] = dict (), date_min : Optional [ datetime ] = None , date_max : Optional [ datetime ] = None , ) -> DataFrame : \"\"\" Generic function to filter a DataFrame based on one of its column and an ensemble of codes to select from. For instance, this function is called when phenotyping via ICD-10 or CCAM. Parameters ---------- df : DataFrame The DataFrame to filter. columns : Dict[str, str] Dictionary with the following keys: - `code_source_value` : The column name containing the code to filter - `code_start_datetime` : The column name containing the starting date - `code_end_datetime` : The column name containing the ending date visit_occurrence : Optional[DataFrame] The `visit_occurrence` DataFrame, only necessary if `date_from_visit` is set to `True`. concept : str The name of the extracted concept codes : Dict[str, Union[str, List[str]]] Dictionary which values are codes (as a unique string or as a list) and which keys are at least one of the following: - `exact`: To match the codes in `codes[\"exact\"]` **exactly** - `prefix`: To match the codes in `codes[\"prefix\"]` **as prefixes** - `regex`: To match the codes in `codes[\"regex\"]` **as regexes** You can combine any of those keys. date_from_visit : bool If set to `True`, uses `visit_start_datetime` as the code datetime additional_filtering : Dict[str, Any] An optional dictionary to filter the resulting DataFrame. Keys should be column names on which too filter, and values should be either - A single value - A list or set of values. date_min : Optional[datetime] The minimum code datetime to keep. **Depends on the `date_from_visit` flag** date_max : Optional[datetime] The minimum code datetime to keep. **Depends on the `date_from_visit` flag** Returns ------- DataFrame A DataFrame containing especially the following columns: - `t_start` - `t_end` - `concept` : The provided `concept` string - `value` : The matched code \"\"\" required_columns = list ( columns . values ()) + [ \"visit_occurrence_id\" , \"person_id\" ] check_columns ( df , required_columns = required_columns ) d_format = { \"exact\" : r \" {code} \\b\" , \"regex\" : r \" {code} \" , \"prefix\" : r \"\\b {code} \" } regexes = [] for code_type , code_list in codes . items (): if type ( code_list ) == str : code_list = [ code_list ] codes_formated = [ d_format [ code_type ] . format ( code = code ) for code in code_list ] regexes . append ( r \"(?:\" + \"|\" . join ( codes_formated ) + \")\" ) final_regex = \"|\" . join ( regexes ) mask = df [ columns [ \"code_source_value\" ]] . str . contains ( final_regex ) . fillna ( False ) event = df [ mask ] if date_from_visit : if visit_occurrence is None : raise ValueError ( \"With 'date_from_visit=True', you should provide a 'visit_occurrence' DataFrame.\" ) event = event . merge ( visit_occurrence [ [ \"visit_occurrence_id\" , \"visit_start_datetime\" , \"visit_end_datetime\" ] ], on = \"visit_occurrence_id\" , how = \"inner\" , ) . rename ( columns = { \"visit_start_datetime\" : \"t_start\" , \"visit_end_datetime\" : \"t_end\" , } ) else : event . loc [:, \"t_start\" ] = event . loc [:, columns [ \"code_start_datetime\" ]] event . loc [:, \"t_end\" ] = event . loc [:, columns [ \"code_end_datetime\" ]] event = event . drop ( columns = [ columns [ \"code_start_datetime\" ], columns [ \"code_end_datetime\" ]] ) event = _column_filtering ( event , filtering_dict = additional_filtering ) mask = True # Resetting the mask if date_min is not None : mask = mask & ( event . t_start >= date_min ) if date_max is not None : mask = mask & ( event . t_start <= date_max ) if type ( mask ) != bool : # We have a Series mask event = event [ mask ] event [ \"concept\" ] = concept return event . rename ( columns = { columns [ \"code_source_value\" ]: \"value\" })[ [ \"person_id\" , \"t_start\" , \"t_end\" , \"concept\" , \"value\" , \"visit_occurrence_id\" , ] + list ( additional_filtering . keys ()) ] . reset_index ( drop = True )","title":"event_from_code()"},{"location":"reference/event/icd10/","text":"eds_scikit.event.icd10 conditions_from_icd10 conditions_from_icd10 ( condition_occurrence : DataFrame , visit_occurrence : Optional [ DataFrame ] = None , codes : Optional [ Dict [ str , Union [ str , List [ str ]]]] = None , date_from_visit : bool = True , additional_filtering : Dict [ str , Any ] = None , date_min : Optional [ datetime ] = None , date_max : Optional [ datetime ] = None ) -> DataFrame Phenotyping based on ICD-10 codes. PARAMETER DESCRIPTION condition_occurrence condition_occurrence OMOP DataFrame. TYPE: DataFrame visit_occurrence visit_occurrence OMOP DataFrame, only necessary if date_from_visit is set to True . TYPE: Optional [ DataFrame ] DEFAULT: None codes Dictionary which values are ICD-10 codes (as a unique string or as a list) and which keys are at least one of the following: exact : To match the codes in codes[\"exact\"] exactly prefix : To match the codes in codes[\"prefix\"] as prefixes regex : To match the codes in codes[\"regex\"] as regexes You can combine any of those keys. TYPE: Dict [ str , Union [ str , List [ str ]]] DEFAULT: None date_from_visit If set to True , uses visit_start_datetime as the code datetime TYPE: bool DEFAULT: True additional_filtering An optional dictionary to filter the resulting DataFrame. Keys should be column names on which to filter, and values should be either A single value A list or set of values. Default filetring is condition_status_source_value in {\"DP\", \"DAS\", \"DR\"} TYPE: Dict [ str , Any ] DEFAULT: None date_min The minimum code datetime to keep. Depends on the date_from_visit flag TYPE: Optional [ datetime ] DEFAULT: None date_max The minimum code datetime to keep. Depends on the date_from_visit flag TYPE: Optional [ datetime ] DEFAULT: None RETURNS DESCRIPTION DataFrame \"event\" DataFrame including the following columns: t_start : If date_from_visit is set to False , contains condition_start_datetime , else contains visit_start_datetime t_end : If date_from_visit is set to False , contains condition_start_datetime , else contains visit_end_datetime concept : contaning values from codes.keys() value : The extracted ICD-10 code. visit_occurrence_id : the visit_occurrence_id from the visit which contains the ICD-10 code. Source code in eds_scikit/event/icd10.py 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 def conditions_from_icd10 ( condition_occurrence : DataFrame , visit_occurrence : Optional [ DataFrame ] = None , codes : Optional [ Dict [ str , Union [ str , List [ str ]]]] = None , date_from_visit : bool = True , additional_filtering : Dict [ str , Any ] = None , date_min : Optional [ datetime ] = None , date_max : Optional [ datetime ] = None , ) -> DataFrame : \"\"\" Phenotyping based on ICD-10 codes. Parameters ---------- condition_occurrence : DataFrame `condition_occurrence` OMOP DataFrame. visit_occurrence : Optional[DataFrame] `visit_occurrence` OMOP DataFrame, only necessary if `date_from_visit` is set to `True`. codes : Dict[str, Union[str, List[str]]] Dictionary which values are ICD-10 codes (as a unique string or as a list) and which keys are at least one of the following: - `exact`: To match the codes in `codes[\"exact\"]` **exactly** - `prefix`: To match the codes in `codes[\"prefix\"]` **as prefixes** - `regex`: To match the codes in `codes[\"regex\"]` **as regexes** You can combine any of those keys. date_from_visit : bool If set to `True`, uses `visit_start_datetime` as the code datetime additional_filtering : Dict[str, Any] An optional dictionary to filter the resulting DataFrame. Keys should be column names on which to filter, and values should be either - A single value - A list or set of values. Default filetring is condition_status_source_value in {\"DP\", \"DAS\", \"DR\"} date_min : Optional[datetime] The minimum code datetime to keep. **Depends on the `date_from_visit` flag** date_max : Optional[datetime] The minimum code datetime to keep. **Depends on the `date_from_visit` flag** Returns ------- DataFrame \"event\" DataFrame including the following columns: - `t_start`: If `date_from_visit` is set to `False`, contains `condition_start_datetime`, else contains `visit_start_datetime` - `t_end`: If `date_from_visit` is set to `False`, contains `condition_start_datetime`, else contains `visit_end_datetime` - `concept` : contaning values from `codes.keys()` - `value` : The extracted ICD-10 code. - `visit_occurrence_id` : the `visit_occurrence_id` from the visit which contains the ICD-10 code. \"\"\" # noqa: E501 if additional_filtering is None : additional_filtering = dict () DEFAULT_FILTERING = dict ( condition_status_source_value = { \"DP\" , \"DAS\" , \"DR\" }) DEFAULT_FILTERING . update ( additional_filtering ) condition_columns = dict ( code_source_value = \"condition_source_value\" , code_start_datetime = \"condition_start_datetime\" , code_end_datetime = \"condition_start_datetime\" , ) events = [] for concept , code_dict in codes . items (): tmp_df = event_from_code ( df = condition_occurrence , columns = condition_columns , visit_occurrence = visit_occurrence , concept = concept , codes = code_dict , date_from_visit = date_from_visit , additional_filtering = DEFAULT_FILTERING , date_min = date_min , date_max = date_max , ) events . append ( tmp_df ) framework = get_framework ( condition_occurrence ) return framework . concat ( events )","title":"icd10"},{"location":"reference/event/icd10/#eds_scikiteventicd10","text":"","title":"eds_scikit.event.icd10"},{"location":"reference/event/icd10/#eds_scikit.event.icd10.conditions_from_icd10","text":"conditions_from_icd10 ( condition_occurrence : DataFrame , visit_occurrence : Optional [ DataFrame ] = None , codes : Optional [ Dict [ str , Union [ str , List [ str ]]]] = None , date_from_visit : bool = True , additional_filtering : Dict [ str , Any ] = None , date_min : Optional [ datetime ] = None , date_max : Optional [ datetime ] = None ) -> DataFrame Phenotyping based on ICD-10 codes. PARAMETER DESCRIPTION condition_occurrence condition_occurrence OMOP DataFrame. TYPE: DataFrame visit_occurrence visit_occurrence OMOP DataFrame, only necessary if date_from_visit is set to True . TYPE: Optional [ DataFrame ] DEFAULT: None codes Dictionary which values are ICD-10 codes (as a unique string or as a list) and which keys are at least one of the following: exact : To match the codes in codes[\"exact\"] exactly prefix : To match the codes in codes[\"prefix\"] as prefixes regex : To match the codes in codes[\"regex\"] as regexes You can combine any of those keys. TYPE: Dict [ str , Union [ str , List [ str ]]] DEFAULT: None date_from_visit If set to True , uses visit_start_datetime as the code datetime TYPE: bool DEFAULT: True additional_filtering An optional dictionary to filter the resulting DataFrame. Keys should be column names on which to filter, and values should be either A single value A list or set of values. Default filetring is condition_status_source_value in {\"DP\", \"DAS\", \"DR\"} TYPE: Dict [ str , Any ] DEFAULT: None date_min The minimum code datetime to keep. Depends on the date_from_visit flag TYPE: Optional [ datetime ] DEFAULT: None date_max The minimum code datetime to keep. Depends on the date_from_visit flag TYPE: Optional [ datetime ] DEFAULT: None RETURNS DESCRIPTION DataFrame \"event\" DataFrame including the following columns: t_start : If date_from_visit is set to False , contains condition_start_datetime , else contains visit_start_datetime t_end : If date_from_visit is set to False , contains condition_start_datetime , else contains visit_end_datetime concept : contaning values from codes.keys() value : The extracted ICD-10 code. visit_occurrence_id : the visit_occurrence_id from the visit which contains the ICD-10 code. Source code in eds_scikit/event/icd10.py 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 def conditions_from_icd10 ( condition_occurrence : DataFrame , visit_occurrence : Optional [ DataFrame ] = None , codes : Optional [ Dict [ str , Union [ str , List [ str ]]]] = None , date_from_visit : bool = True , additional_filtering : Dict [ str , Any ] = None , date_min : Optional [ datetime ] = None , date_max : Optional [ datetime ] = None , ) -> DataFrame : \"\"\" Phenotyping based on ICD-10 codes. Parameters ---------- condition_occurrence : DataFrame `condition_occurrence` OMOP DataFrame. visit_occurrence : Optional[DataFrame] `visit_occurrence` OMOP DataFrame, only necessary if `date_from_visit` is set to `True`. codes : Dict[str, Union[str, List[str]]] Dictionary which values are ICD-10 codes (as a unique string or as a list) and which keys are at least one of the following: - `exact`: To match the codes in `codes[\"exact\"]` **exactly** - `prefix`: To match the codes in `codes[\"prefix\"]` **as prefixes** - `regex`: To match the codes in `codes[\"regex\"]` **as regexes** You can combine any of those keys. date_from_visit : bool If set to `True`, uses `visit_start_datetime` as the code datetime additional_filtering : Dict[str, Any] An optional dictionary to filter the resulting DataFrame. Keys should be column names on which to filter, and values should be either - A single value - A list or set of values. Default filetring is condition_status_source_value in {\"DP\", \"DAS\", \"DR\"} date_min : Optional[datetime] The minimum code datetime to keep. **Depends on the `date_from_visit` flag** date_max : Optional[datetime] The minimum code datetime to keep. **Depends on the `date_from_visit` flag** Returns ------- DataFrame \"event\" DataFrame including the following columns: - `t_start`: If `date_from_visit` is set to `False`, contains `condition_start_datetime`, else contains `visit_start_datetime` - `t_end`: If `date_from_visit` is set to `False`, contains `condition_start_datetime`, else contains `visit_end_datetime` - `concept` : contaning values from `codes.keys()` - `value` : The extracted ICD-10 code. - `visit_occurrence_id` : the `visit_occurrence_id` from the visit which contains the ICD-10 code. \"\"\" # noqa: E501 if additional_filtering is None : additional_filtering = dict () DEFAULT_FILTERING = dict ( condition_status_source_value = { \"DP\" , \"DAS\" , \"DR\" }) DEFAULT_FILTERING . update ( additional_filtering ) condition_columns = dict ( code_source_value = \"condition_source_value\" , code_start_datetime = \"condition_start_datetime\" , code_end_datetime = \"condition_start_datetime\" , ) events = [] for concept , code_dict in codes . items (): tmp_df = event_from_code ( df = condition_occurrence , columns = condition_columns , visit_occurrence = visit_occurrence , concept = concept , codes = code_dict , date_from_visit = date_from_visit , additional_filtering = DEFAULT_FILTERING , date_min = date_min , date_max = date_max , ) events . append ( tmp_df ) framework = get_framework ( condition_occurrence ) return framework . concat ( events )","title":"conditions_from_icd10()"},{"location":"reference/event/suicide_attempt/","text":"eds_scikit.event.suicide_attempt tag_suicide_attempt tag_suicide_attempt ( visit_occurrence : DataFrame , condition_occurrence : DataFrame , date_min : Optional [ datetime ] = None , date_max : Optional [ datetime ] = None , algo : str = 'X60-X84' ) -> DataFrame Function to return visits that fulfill different definitions of suicide attempt by ICD10. PARAMETER DESCRIPTION visit_occurrence TYPE: DataFrame condition_occurrence TYPE: DataFrame date_min Minimal starting date (on visit_start_datetime ) TYPE: Optional [ datetime ] DEFAULT: None date_max Maximal starting date (on visit_start_datetime ) TYPE: Optional [ datetime ] DEFAULT: None algo Method to use. Available values are: \"X60-X84\" : Will return a the visits that have at least one ICD code that belongs to the range X60 to X84. \"Haguenoer2008\" : Will return a the visits that follow the definiton of \" Haguenoer, Ken, Agn\u00e8s Caille, Marc Fillatre, Anne Isabelle Lecuyer, et Emmanuel Rusch. \u00ab Tentatives de Suicide \u00bb, 2008, 4. \". This rule requires at least one Main Diagnostic (DP) belonging to S00 to T98, and at least one Associated Diagnostic (DAS) that belongs to the range X60 to X84. TYPE: str DEFAULT: 'X60-X84' RETURNS DESCRIPTION visit_occurrence Tagged with an additional column SUICIDE_ATTEMPT TYPE: DataFrame Tip These rules were implemented in the CSE project n\u00b0210013 Source code in eds_scikit/event/suicide_attempt.py 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 @concept_checker ( concepts = [ \"SUICIDE_ATTEMPT\" ]) @algo_checker ( algos = ALGOS ) def tag_suicide_attempt ( visit_occurrence : DataFrame , condition_occurrence : DataFrame , date_min : Optional [ datetime ] = None , date_max : Optional [ datetime ] = None , algo : str = \"X60-X84\" , ) -> DataFrame : \"\"\" Function to return visits that fulfill different definitions of suicide attempt by ICD10. Parameters ---------- visit_occurrence: DataFrame condition_occurrence: DataFrame date_min: datetime Minimal starting date (on `visit_start_datetime`) date_max: datetime Maximal starting date (on `visit_start_datetime`) algo: str Method to use. Available values are: - `\"X60-X84\"`: Will return a the visits that have at least one ICD code that belongs to the range X60 to X84. - `\"Haguenoer2008\"`: Will return a the visits that follow the definiton of \"*Haguenoer, Ken, Agn\u00e8s Caille, Marc Fillatre, Anne Isabelle Lecuyer, et Emmanuel Rusch. \u00ab Tentatives de Suicide \u00bb, 2008, 4.*\". This rule requires at least one Main Diagnostic (DP) belonging to S00 to T98, and at least one Associated Diagnostic (DAS) that belongs to the range X60 to X84. Returns ------- visit_occurrence: DataFrame Tagged with an additional column `SUICIDE_ATTEMPT` !!! tip These rules were implemented in the CSE project n\u00b0210013 \"\"\" events_1 = conditions_from_icd10 ( condition_occurrence , visit_occurrence = visit_occurrence , date_min = date_min , date_max = date_max , ** DEFAULT_CONFIG [ \"X60-X84\" ], ) events_1 = events_1 [ [ \"visit_occurrence_id\" , \"condition_status_source_value\" ] ] . drop_duplicates ( subset = \"visit_occurrence_id\" ) events_1 [ CONCEPT ] = True if algo == \"X60-X84\" : visit_occurrence_tagged = visit_occurrence . merge ( events_1 [[ \"visit_occurrence_id\" , CONCEPT ]], on = \"visit_occurrence_id\" , how = \"left\" , ) visit_occurrence_tagged [ CONCEPT ] . fillna ( False , inplace = True ) return visit_occurrence_tagged if algo == \"Haguenoer2008\" : events_1 = events_1 [ events_1 . condition_status_source_value == \"DAS\" ] events_2 = conditions_from_icd10 ( condition_occurrence , visit_occurrence = visit_occurrence , date_min = date_min , date_max = date_max , ** DEFAULT_CONFIG [ algo ], ) events_2 = events_2 [[ \"visit_occurrence_id\" ]] . drop_duplicates () events_2 [ f \" { CONCEPT } _BIS\" ] = True visit_occurrence_tagged = visit_occurrence . merge ( events_1 [[ \"visit_occurrence_id\" , CONCEPT ]], on = \"visit_occurrence_id\" , how = \"left\" , ) . merge ( events_2 [[ \"visit_occurrence_id\" , f \" { CONCEPT } _BIS\" ]], on = \"visit_occurrence_id\" , how = \"left\" , ) visit_occurrence_tagged [ CONCEPT ] = ( visit_occurrence_tagged [ CONCEPT ] & visit_occurrence_tagged [ f \" { CONCEPT } _BIS\" ] ) visit_occurrence_tagged = visit_occurrence_tagged . drop ( columns = [ f \" { CONCEPT } _BIS\" ] ) return visit_occurrence_tagged","title":"suicide_attempt"},{"location":"reference/event/suicide_attempt/#eds_scikiteventsuicide_attempt","text":"","title":"eds_scikit.event.suicide_attempt"},{"location":"reference/event/suicide_attempt/#eds_scikit.event.suicide_attempt.tag_suicide_attempt","text":"tag_suicide_attempt ( visit_occurrence : DataFrame , condition_occurrence : DataFrame , date_min : Optional [ datetime ] = None , date_max : Optional [ datetime ] = None , algo : str = 'X60-X84' ) -> DataFrame Function to return visits that fulfill different definitions of suicide attempt by ICD10. PARAMETER DESCRIPTION visit_occurrence TYPE: DataFrame condition_occurrence TYPE: DataFrame date_min Minimal starting date (on visit_start_datetime ) TYPE: Optional [ datetime ] DEFAULT: None date_max Maximal starting date (on visit_start_datetime ) TYPE: Optional [ datetime ] DEFAULT: None algo Method to use. Available values are: \"X60-X84\" : Will return a the visits that have at least one ICD code that belongs to the range X60 to X84. \"Haguenoer2008\" : Will return a the visits that follow the definiton of \" Haguenoer, Ken, Agn\u00e8s Caille, Marc Fillatre, Anne Isabelle Lecuyer, et Emmanuel Rusch. \u00ab Tentatives de Suicide \u00bb, 2008, 4. \". This rule requires at least one Main Diagnostic (DP) belonging to S00 to T98, and at least one Associated Diagnostic (DAS) that belongs to the range X60 to X84. TYPE: str DEFAULT: 'X60-X84' RETURNS DESCRIPTION visit_occurrence Tagged with an additional column SUICIDE_ATTEMPT TYPE: DataFrame Tip These rules were implemented in the CSE project n\u00b0210013 Source code in eds_scikit/event/suicide_attempt.py 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 @concept_checker ( concepts = [ \"SUICIDE_ATTEMPT\" ]) @algo_checker ( algos = ALGOS ) def tag_suicide_attempt ( visit_occurrence : DataFrame , condition_occurrence : DataFrame , date_min : Optional [ datetime ] = None , date_max : Optional [ datetime ] = None , algo : str = \"X60-X84\" , ) -> DataFrame : \"\"\" Function to return visits that fulfill different definitions of suicide attempt by ICD10. Parameters ---------- visit_occurrence: DataFrame condition_occurrence: DataFrame date_min: datetime Minimal starting date (on `visit_start_datetime`) date_max: datetime Maximal starting date (on `visit_start_datetime`) algo: str Method to use. Available values are: - `\"X60-X84\"`: Will return a the visits that have at least one ICD code that belongs to the range X60 to X84. - `\"Haguenoer2008\"`: Will return a the visits that follow the definiton of \"*Haguenoer, Ken, Agn\u00e8s Caille, Marc Fillatre, Anne Isabelle Lecuyer, et Emmanuel Rusch. \u00ab Tentatives de Suicide \u00bb, 2008, 4.*\". This rule requires at least one Main Diagnostic (DP) belonging to S00 to T98, and at least one Associated Diagnostic (DAS) that belongs to the range X60 to X84. Returns ------- visit_occurrence: DataFrame Tagged with an additional column `SUICIDE_ATTEMPT` !!! tip These rules were implemented in the CSE project n\u00b0210013 \"\"\" events_1 = conditions_from_icd10 ( condition_occurrence , visit_occurrence = visit_occurrence , date_min = date_min , date_max = date_max , ** DEFAULT_CONFIG [ \"X60-X84\" ], ) events_1 = events_1 [ [ \"visit_occurrence_id\" , \"condition_status_source_value\" ] ] . drop_duplicates ( subset = \"visit_occurrence_id\" ) events_1 [ CONCEPT ] = True if algo == \"X60-X84\" : visit_occurrence_tagged = visit_occurrence . merge ( events_1 [[ \"visit_occurrence_id\" , CONCEPT ]], on = \"visit_occurrence_id\" , how = \"left\" , ) visit_occurrence_tagged [ CONCEPT ] . fillna ( False , inplace = True ) return visit_occurrence_tagged if algo == \"Haguenoer2008\" : events_1 = events_1 [ events_1 . condition_status_source_value == \"DAS\" ] events_2 = conditions_from_icd10 ( condition_occurrence , visit_occurrence = visit_occurrence , date_min = date_min , date_max = date_max , ** DEFAULT_CONFIG [ algo ], ) events_2 = events_2 [[ \"visit_occurrence_id\" ]] . drop_duplicates () events_2 [ f \" { CONCEPT } _BIS\" ] = True visit_occurrence_tagged = visit_occurrence . merge ( events_1 [[ \"visit_occurrence_id\" , CONCEPT ]], on = \"visit_occurrence_id\" , how = \"left\" , ) . merge ( events_2 [[ \"visit_occurrence_id\" , f \" { CONCEPT } _BIS\" ]], on = \"visit_occurrence_id\" , how = \"left\" , ) visit_occurrence_tagged [ CONCEPT ] = ( visit_occurrence_tagged [ CONCEPT ] & visit_occurrence_tagged [ f \" { CONCEPT } _BIS\" ] ) visit_occurrence_tagged = visit_occurrence_tagged . drop ( columns = [ f \" { CONCEPT } _BIS\" ] ) return visit_occurrence_tagged","title":"tag_suicide_attempt()"},{"location":"reference/icu/","text":"eds_scikit.icu","title":"`eds_scikit.icu`"},{"location":"reference/icu/#eds_scikiticu","text":"","title":"eds_scikit.icu"},{"location":"reference/icu/icu_care_site/","text":"eds_scikit.icu.icu_care_site tag_icu_care_site tag_icu_care_site ( care_site : DataFrame , algo : str = 'from_mapping' ) -> DataFrame Tag care sites that correspond to ICU units . The tagging is done by adding a \"IS_ICU\" column to the provided DataFrame. PARAMETER DESCRIPTION care_site TYPE: DataFrame algo Possible values are: \"from_authorisation_type\" \"from_regex_on_care_site_description\" TYPE: str DEFAULT: 'from_mapping' RETURNS DESCRIPTION care_site Dataframe with 1 added column corresponding to the following concept: \"IS_ICU\" TYPE: DataFrame Source code in eds_scikit/icu/icu_care_site.py 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 @algo_checker ( algos = ALGOS ) def tag_icu_care_site ( care_site : DataFrame , algo : str = \"from_mapping\" , ) -> DataFrame : \"\"\"Tag care sites that correspond to **ICU units**. The tagging is done by adding a `\"IS_ICU\"` column to the provided DataFrame. Parameters ---------- care_site: DataFrame algo: str Possible values are: - [`\"from_authorisation_type\"`][eds_scikit.icu.icu_care_site.from_authorisation_type] - [`\"from_regex_on_care_site_description\"`][eds_scikit.icu.icu_care_site.from_regex_on_care_site_description] Returns ------- care_site: DataFrame Dataframe with 1 added column corresponding to the following concept: - `\"IS_ICU\"` \"\"\" if algo == \"from_authorisation_type\" : return from_authorisation_type ( care_site ) elif algo == \"from_regex_on_care_site_description\" : return from_regex_on_care_site_description ( care_site ) from_authorisation_type from_authorisation_type ( care_site : DataFrame ) -> DataFrame This algo uses the care_site.place_of_service_source_value columns to retrieve Intensive Care Units. The following values are used to tag a care site as ICU: \"REA PED\" \"REA\" \"REA ADULTE\" \"REA NEONAT\" \"USI\" \"USI ADULTE\" \"USI NEONAT\" \"SC PED\" \"SC\" \"SC ADULTE\" PARAMETER DESCRIPTION care_site Should at least contains the place_of_service_source_value column TYPE: DataFrame RETURNS DESCRIPTION care_site Dataframe with 1 added column corresponding to the following concepts: \"IS_ICU\" TYPE: DataFrame Source code in eds_scikit/icu/icu_care_site.py 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 @concept_checker ( concepts = [ \"IS_ICU\" ]) def from_authorisation_type ( care_site : DataFrame ) -> DataFrame : \"\"\"This algo uses the `care_site.place_of_service_source_value` columns to retrieve Intensive Care Units. The following values are used to tag a care site as ICU: - `\"REA PED\"` - `\"REA\"` - `\"REA ADULTE\"` - `\"REA NEONAT\"` - `\"USI\"` - `\"USI ADULTE\"` - `\"USI NEONAT\"` - `\"SC PED\"` - `\"SC\"` - `\"SC ADULTE\"` Parameters ---------- care_site: DataFrame Should at least contains the `place_of_service_source_value` column Returns ------- care_site: DataFrame Dataframe with 1 added column corresponding to the following concepts: - `\"IS_ICU\"` \"\"\" icu_units = set ( [ \"REA PED\" , \"USI\" , \"SC PED\" , \"SC\" , \"REA\" , \"SC ADULTE\" , \"USI ADULTE\" , \"REA ADULTE\" , \"USI NEONAT\" , \"REA NEONAT\" , ] ) care_site [ \"IS_ICU\" ] = care_site [ \"place_of_service_source_value\" ] . isin ( icu_units ) return care_site from_regex_on_care_site_description from_regex_on_care_site_description ( care_site : DataFrame , subset_care_site_type_source_value : Union [ list , set ] = { 'UDS' }) -> DataFrame Use regular expressions on care_site_name to decide if it an ICU care site. This relies on this function . The regular expression used to detect ICU is r\"\bUSI|\bREA[N\\s]|\bREA\b|\bUSC\b|SOINS.*INTENSIF|SURV.{0,15}CONT|\bSI\b|\bSC\b\" . Keeping only 'UDS' At AP-HP, all ICU are UDS ( Unit\u00e9 De Soins ). Therefore, this function filters care sites by default to only keep UDS. PARAMETER DESCRIPTION care_site Should at least contains the care_site_name and care_site_type_source_value columns TYPE: DataFrame subset_care_site_type_source_value Acceptable values for care_site_type_source_value TYPE: Union [ list , set ] DEFAULT: {'UDS'} RETURNS DESCRIPTION care_site Dataframe with 1 added column corresponding to the following concept: \"IS_ICU\" TYPE: DataFrame Source code in eds_scikit/icu/icu_care_site.py 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 def from_regex_on_care_site_description ( care_site : DataFrame , subset_care_site_type_source_value : Union [ list , set ] = { \"UDS\" } ) -> DataFrame : \"\"\"Use regular expressions on `care_site_name` to decide if it an ICU care site. This relies on [this function][eds_scikit.structures.attributes.add_care_site_attributes]. The regular expression used to detect ICU is `r\"\\bUSI|\\bREA[N\\s]|\\bREA\\b|\\bUSC\\b|SOINS.*INTENSIF|SURV.{0,15}CONT|\\bSI\\b|\\bSC\\b\"`. !!! aphp \"Keeping only 'UDS'\" At AP-HP, all ICU are **UDS** (*Unit\u00e9 De Soins*). Therefore, this function filters care sites by default to only keep UDS. Parameters ---------- care_site: DataFrame Should at least contains the `care_site_name` and `care_site_type_source_value` columns subset_care_site_type_source_value: Union[list, set] Acceptable values for `care_site_type_source_value` Returns ------- care_site: DataFrame Dataframe with 1 added column corresponding to the following concept: - `\"IS_ICU\"` \"\"\" # noqa care_site = attributes . add_care_site_attributes ( care_site , only_attributes = [ \"IS_ICU\" ] ) # Filtering matches if subset_care_site_type_source_value : care_site [ \"IS_ICU\" ] = care_site [ \"IS_ICU\" ] & ( care_site . care_site_type_source_value . isin ( subset_care_site_type_source_value ) ) return care_site","title":"icu_care_site"},{"location":"reference/icu/icu_care_site/#eds_scikiticuicu_care_site","text":"","title":"eds_scikit.icu.icu_care_site"},{"location":"reference/icu/icu_care_site/#eds_scikit.icu.icu_care_site.tag_icu_care_site","text":"tag_icu_care_site ( care_site : DataFrame , algo : str = 'from_mapping' ) -> DataFrame Tag care sites that correspond to ICU units . The tagging is done by adding a \"IS_ICU\" column to the provided DataFrame. PARAMETER DESCRIPTION care_site TYPE: DataFrame algo Possible values are: \"from_authorisation_type\" \"from_regex_on_care_site_description\" TYPE: str DEFAULT: 'from_mapping' RETURNS DESCRIPTION care_site Dataframe with 1 added column corresponding to the following concept: \"IS_ICU\" TYPE: DataFrame Source code in eds_scikit/icu/icu_care_site.py 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 @algo_checker ( algos = ALGOS ) def tag_icu_care_site ( care_site : DataFrame , algo : str = \"from_mapping\" , ) -> DataFrame : \"\"\"Tag care sites that correspond to **ICU units**. The tagging is done by adding a `\"IS_ICU\"` column to the provided DataFrame. Parameters ---------- care_site: DataFrame algo: str Possible values are: - [`\"from_authorisation_type\"`][eds_scikit.icu.icu_care_site.from_authorisation_type] - [`\"from_regex_on_care_site_description\"`][eds_scikit.icu.icu_care_site.from_regex_on_care_site_description] Returns ------- care_site: DataFrame Dataframe with 1 added column corresponding to the following concept: - `\"IS_ICU\"` \"\"\" if algo == \"from_authorisation_type\" : return from_authorisation_type ( care_site ) elif algo == \"from_regex_on_care_site_description\" : return from_regex_on_care_site_description ( care_site )","title":"tag_icu_care_site()"},{"location":"reference/icu/icu_care_site/#eds_scikit.icu.icu_care_site.from_authorisation_type","text":"from_authorisation_type ( care_site : DataFrame ) -> DataFrame This algo uses the care_site.place_of_service_source_value columns to retrieve Intensive Care Units. The following values are used to tag a care site as ICU: \"REA PED\" \"REA\" \"REA ADULTE\" \"REA NEONAT\" \"USI\" \"USI ADULTE\" \"USI NEONAT\" \"SC PED\" \"SC\" \"SC ADULTE\" PARAMETER DESCRIPTION care_site Should at least contains the place_of_service_source_value column TYPE: DataFrame RETURNS DESCRIPTION care_site Dataframe with 1 added column corresponding to the following concepts: \"IS_ICU\" TYPE: DataFrame Source code in eds_scikit/icu/icu_care_site.py 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 @concept_checker ( concepts = [ \"IS_ICU\" ]) def from_authorisation_type ( care_site : DataFrame ) -> DataFrame : \"\"\"This algo uses the `care_site.place_of_service_source_value` columns to retrieve Intensive Care Units. The following values are used to tag a care site as ICU: - `\"REA PED\"` - `\"REA\"` - `\"REA ADULTE\"` - `\"REA NEONAT\"` - `\"USI\"` - `\"USI ADULTE\"` - `\"USI NEONAT\"` - `\"SC PED\"` - `\"SC\"` - `\"SC ADULTE\"` Parameters ---------- care_site: DataFrame Should at least contains the `place_of_service_source_value` column Returns ------- care_site: DataFrame Dataframe with 1 added column corresponding to the following concepts: - `\"IS_ICU\"` \"\"\" icu_units = set ( [ \"REA PED\" , \"USI\" , \"SC PED\" , \"SC\" , \"REA\" , \"SC ADULTE\" , \"USI ADULTE\" , \"REA ADULTE\" , \"USI NEONAT\" , \"REA NEONAT\" , ] ) care_site [ \"IS_ICU\" ] = care_site [ \"place_of_service_source_value\" ] . isin ( icu_units ) return care_site","title":"from_authorisation_type()"},{"location":"reference/icu/icu_care_site/#eds_scikit.icu.icu_care_site.from_regex_on_care_site_description","text":"from_regex_on_care_site_description ( care_site : DataFrame , subset_care_site_type_source_value : Union [ list , set ] = { 'UDS' }) -> DataFrame Use regular expressions on care_site_name to decide if it an ICU care site. This relies on this function . The regular expression used to detect ICU is r\"\bUSI|\bREA[N\\s]|\bREA\b|\bUSC\b|SOINS.*INTENSIF|SURV.{0,15}CONT|\bSI\b|\bSC\b\" . Keeping only 'UDS' At AP-HP, all ICU are UDS ( Unit\u00e9 De Soins ). Therefore, this function filters care sites by default to only keep UDS. PARAMETER DESCRIPTION care_site Should at least contains the care_site_name and care_site_type_source_value columns TYPE: DataFrame subset_care_site_type_source_value Acceptable values for care_site_type_source_value TYPE: Union [ list , set ] DEFAULT: {'UDS'} RETURNS DESCRIPTION care_site Dataframe with 1 added column corresponding to the following concept: \"IS_ICU\" TYPE: DataFrame Source code in eds_scikit/icu/icu_care_site.py 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 def from_regex_on_care_site_description ( care_site : DataFrame , subset_care_site_type_source_value : Union [ list , set ] = { \"UDS\" } ) -> DataFrame : \"\"\"Use regular expressions on `care_site_name` to decide if it an ICU care site. This relies on [this function][eds_scikit.structures.attributes.add_care_site_attributes]. The regular expression used to detect ICU is `r\"\\bUSI|\\bREA[N\\s]|\\bREA\\b|\\bUSC\\b|SOINS.*INTENSIF|SURV.{0,15}CONT|\\bSI\\b|\\bSC\\b\"`. !!! aphp \"Keeping only 'UDS'\" At AP-HP, all ICU are **UDS** (*Unit\u00e9 De Soins*). Therefore, this function filters care sites by default to only keep UDS. Parameters ---------- care_site: DataFrame Should at least contains the `care_site_name` and `care_site_type_source_value` columns subset_care_site_type_source_value: Union[list, set] Acceptable values for `care_site_type_source_value` Returns ------- care_site: DataFrame Dataframe with 1 added column corresponding to the following concept: - `\"IS_ICU\"` \"\"\" # noqa care_site = attributes . add_care_site_attributes ( care_site , only_attributes = [ \"IS_ICU\" ] ) # Filtering matches if subset_care_site_type_source_value : care_site [ \"IS_ICU\" ] = care_site [ \"IS_ICU\" ] & ( care_site . care_site_type_source_value . isin ( subset_care_site_type_source_value ) ) return care_site","title":"from_regex_on_care_site_description()"},{"location":"reference/icu/icu_visit/","text":"eds_scikit.icu.icu_visit tag_icu_visit tag_icu_visit ( visit_detail : DataFrame , care_site : DataFrame , algo : str = 'from_authorisation_type' ) -> DataFrame Tag care_sites that correspond to ICU units . The tagging is done by adding a \"IS_ICU\" column to the provided DataFrame. It works by tagging each visit detail's care site . PARAMETER DESCRIPTION visit_detail TYPE: DataFrame care_site TYPE: DataFrame algo Possible values are: \"from_authorisation_type\" \"from_regex_on_care_site_description\" TYPE: str DEFAULT: 'from_authorisation_type' RETURNS DESCRIPTION visit_detail Dataframe with 1 added column corresponding to the following concept: \"IS_ICU\" TYPE: DataFrame Source code in eds_scikit/icu/icu_visit.py 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 @algo_checker ( algos = ALGOS ) def tag_icu_visit ( visit_detail : DataFrame , care_site : DataFrame , algo : str = \"from_authorisation_type\" , ) -> DataFrame : \"\"\"Tag care_sites that correspond to **ICU units**. The tagging is done by adding a `\"IS_ICU\"` column to the provided DataFrame. It works by [tagging each visit detail's care site][eds_scikit.icu.icu_care_site.tag_icu_care_site]. Parameters ---------- visit_detail: DataFrame care_site: DataFrame algo: str Possible values are: - [`\"from_authorisation_type\"`][eds_scikit.icu.icu_care_site.from_authorisation_type] - [`\"from_regex_on_care_site_description\"`][eds_scikit.icu.icu_care_site.from_regex_on_care_site_description] Returns ------- visit_detail: DataFrame Dataframe with 1 added column corresponding to the following concept: - `\"IS_ICU\"` \"\"\" tagged_care_site = tag_icu_care_site ( care_site , algo = algo ) return visit_detail . merge ( tagged_care_site [[ \"care_site_id\" , \"IS_ICU\" ]], on = \"care_site_id\" , how = \"left\" )","title":"icu_visit"},{"location":"reference/icu/icu_visit/#eds_scikiticuicu_visit","text":"","title":"eds_scikit.icu.icu_visit"},{"location":"reference/icu/icu_visit/#eds_scikit.icu.icu_visit.tag_icu_visit","text":"tag_icu_visit ( visit_detail : DataFrame , care_site : DataFrame , algo : str = 'from_authorisation_type' ) -> DataFrame Tag care_sites that correspond to ICU units . The tagging is done by adding a \"IS_ICU\" column to the provided DataFrame. It works by tagging each visit detail's care site . PARAMETER DESCRIPTION visit_detail TYPE: DataFrame care_site TYPE: DataFrame algo Possible values are: \"from_authorisation_type\" \"from_regex_on_care_site_description\" TYPE: str DEFAULT: 'from_authorisation_type' RETURNS DESCRIPTION visit_detail Dataframe with 1 added column corresponding to the following concept: \"IS_ICU\" TYPE: DataFrame Source code in eds_scikit/icu/icu_visit.py 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 @algo_checker ( algos = ALGOS ) def tag_icu_visit ( visit_detail : DataFrame , care_site : DataFrame , algo : str = \"from_authorisation_type\" , ) -> DataFrame : \"\"\"Tag care_sites that correspond to **ICU units**. The tagging is done by adding a `\"IS_ICU\"` column to the provided DataFrame. It works by [tagging each visit detail's care site][eds_scikit.icu.icu_care_site.tag_icu_care_site]. Parameters ---------- visit_detail: DataFrame care_site: DataFrame algo: str Possible values are: - [`\"from_authorisation_type\"`][eds_scikit.icu.icu_care_site.from_authorisation_type] - [`\"from_regex_on_care_site_description\"`][eds_scikit.icu.icu_care_site.from_regex_on_care_site_description] Returns ------- visit_detail: DataFrame Dataframe with 1 added column corresponding to the following concept: - `\"IS_ICU\"` \"\"\" tagged_care_site = tag_icu_care_site ( care_site , algo = algo ) return visit_detail . merge ( tagged_care_site [[ \"care_site_id\" , \"IS_ICU\" ]], on = \"care_site_id\" , how = \"left\" )","title":"tag_icu_visit()"},{"location":"reference/io/","text":"eds_scikit.io PandasData PandasData ( folder : str ) Bases: BaseData Pandas interface to OMOP data stored as local parquet files/folders. PARAMETER DESCRIPTION folder absolute path to a folder containing several parquet files with OMOP data TYPE: str Examples: >>> data = PandasData ( folder = \"/export/home/USER/my_data/\" ) >>> person = data . person >>> person . shape (100, 10) Source code in eds_scikit/io/files.py 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 def __init__ ( self , folder : str , ): \"\"\"Pandas interface to OMOP data stored as local parquet files/folders. Parameters ---------- folder: str absolute path to a folder containing several parquet files with OMOP data Examples -------- >>> data = PandasData(folder=\"/export/home/USER/my_data/\") >>> person = data.person >>> person.shape (100, 10) \"\"\" super () . __init__ () self . folder = folder self . available_tables = self . list_available_tables () self . tables_paths = self . get_table_path () if not self . available_tables : raise ValueError ( f \"Folder { folder } does not contain any parquet omop data.\" ) PostgresData PostgresData ( dbname : Optional [ str ] = None , schema : Optional [ str ] = None , user : Optional [ str ] = None , host : Optional [ str ] = None , port : Optional [ int ] = None ) Bases: BaseData Source code in eds_scikit/io/postgres.py 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 def __init__ ( self , dbname : Optional [ str ] = None , schema : Optional [ str ] = None , user : Optional [ str ] = None , host : Optional [ str ] = None , port : Optional [ int ] = None , ): ( self . host , self . port , self . dbname , self . user , ) = self . _find_matching_pgpass_params ( host , port , dbname , user ) self . schema = schema read_sql read_sql ( sql_query : str , ** kwargs ) -> pd . DataFrame Execute pandas.read_sql() on the database. PARAMETER DESCRIPTION sql_query SQL query (postgres flavor) TYPE: str **kwargs additional arguments passed to pandas.read_sql() DEFAULT: {} RETURNS DESCRIPTION df TYPE: pandas . DataFrame Source code in eds_scikit/io/postgres.py 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 def read_sql ( self , sql_query : str , ** kwargs ) -> pd . DataFrame : \"\"\"Execute pandas.read_sql() on the database. Parameters ---------- sql_query : str SQL query (postgres flavor) **kwargs additional arguments passed to pandas.read_sql() Returns ------- df : pandas.DataFrame \"\"\" connection_infos = { param : getattr ( self , param ) for param in [ \"host\" , \"port\" , \"dbname\" , \"user\" ] } connection_infos [ \"password\" ] = pgpasslib . getpass ( ** connection_infos ) connection = pg . connect ( ** connection_infos ) if self . schema : connection . cursor () . execute ( f \"SET SCHEMA ' { self . schema } '\" ) df = pd . read_sql ( sql_query , con = connection , ** kwargs ) connection . close () return df HiveData HiveData ( database_name : str , spark_session : Optional [ SparkSession ] = None , person_ids : Optional [ Iterable [ int ]] = None , tables_to_load : Optional [ Union [ Dict [ str , Optional [ List [ str ]]], List [ str ]]] = None , columns_to_load : Optional [ Union [ Dict [ str , Optional [ List [ str ]]], List [ str ]]] = None , database_type : Optional [ str ] = 'OMOP' ) Bases: BaseData Spark interface for OMOP data stored in a Hive database. This class provides a simple access to data stored in Hive. Data is returned as koalas dataframes that match the tables stored in Hive. PARAMETER DESCRIPTION database_name The name of you database in Hive. Ex: \"cse_82727572\" TYPE: str spark_session If None, a SparkSession will be retrieved or created via SparkSession.builder.enableHiveSupport().getOrCreate() TYPE: pyspark . sql . SparkSession DEFAULT: None person_ids An iterable of person_id that is used to define a subset of the database. TYPE: Optional [ Iterable [ int ]] DEFAULT: None tables_to_load deprecated TYPE: dict , default DEFAULT: None columns_to_load deprecated TYPE: dict , default DEFAULT: None database_type Whether to use the native OMOP schema or to convert I2B2 inputs to OMOP. TYPE: Optional [ str ] DEFAULT: 'OMOP' ATTRIBUTE DESCRIPTION person Hive data for table person as a koalas dataframe. Other OMOP tables can also be accessed as attributes TYPE: koalas dataframe available_tables names of OMOP tables that can be accessed as attributes with this HiveData object. TYPE: list of str Examples: data = HiveData ( database_name = \"edsomop_prod_a\" ) data . available_tables # Out: [\"person\", \"care_site\", \"condition_occurrence\", ... ] person = data . person type ( person ) # Out: databricks.koalas.frame.DataFrame person [ \"person_id\" ] . count () # Out: 12670874 This class can be used to create a subset of data for a given list of person_id . This is useful because the smaller dataset can then be used to prototype more rapidly. my_person_ids = [ 9226726 , 2092082 , ... ] data = HiveData ( spark_session = spark , database_name = \"edsomop_prod_a\" , person_ids = my_person_ids ) data . person [ \"person_id\" ] . count () # Out: 1000 tables_to_save = [ \"person\" , \"visit_occurrence\" ] data . persist_tables_to_folder ( \"./cohort_sample_1000\" , table_names = tables_to_save ) # Out: writing /export/home/USER/cohort_sample_1000/person.parquet # Out: writing /export/home/USER/cohort_sample_1000/visit_occurrence.parquet # Out: ... Source code in eds_scikit/io/hive.py 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 def __init__ ( self , database_name : str , spark_session : Optional [ SparkSession ] = None , person_ids : Optional [ Iterable [ int ]] = None , tables_to_load : Optional [ Union [ Dict [ str , Optional [ List [ str ]]], List [ str ]] ] = None , columns_to_load : Optional [ Union [ Dict [ str , Optional [ List [ str ]]], List [ str ]] ] = None , database_type : Optional [ str ] = \"OMOP\" , ): \"\"\"Spark interface for OMOP data stored in a Hive database. This class provides a simple access to data stored in Hive. Data is returned as koalas dataframes that match the tables stored in Hive. Parameters ---------- database_name : str The name of you database in Hive. Ex: \"cse_82727572\" spark_session : pyspark.sql.SparkSession If None, a SparkSession will be retrieved or created via `SparkSession.builder.enableHiveSupport().getOrCreate()` person_ids : Optional[Iterable[int]] An iterable of `person_id` that is used to define a subset of the database. tables_to_load : dict, default=None *deprecated* columns_to_load : dict, default=None *deprecated* database_type: Optional[str] = 'OMOP'. Must be 'OMOP' or 'I2B2' Whether to use the native OMOP schema or to convert I2B2 inputs to OMOP. Attributes ---------- person : koalas dataframe Hive data for table `person` as a koalas dataframe. Other OMOP tables can also be accessed as attributes available_tables : list of str names of OMOP tables that can be accessed as attributes with this HiveData object. Examples -------- ```python data = HiveData(database_name=\"edsomop_prod_a\") data.available_tables # Out: [\"person\", \"care_site\", \"condition_occurrence\", ... ] person = data.person type(person) # Out: databricks.koalas.frame.DataFrame person[\"person_id\"].count() # Out: 12670874 ``` This class can be used to create a subset of data for a given list of `person_id`. This is useful because the smaller dataset can then be used to prototype more rapidly. ```python my_person_ids = [9226726, 2092082, ...] data = HiveData( spark_session=spark, database_name=\"edsomop_prod_a\", person_ids=my_person_ids ) data.person[\"person_id\"].count() # Out: 1000 tables_to_save = [\"person\", \"visit_occurrence\"] data.persist_tables_to_folder(\"./cohort_sample_1000\", table_names=tables_to_save) # Out: writing /export/home/USER/cohort_sample_1000/person.parquet # Out: writing /export/home/USER/cohort_sample_1000/visit_occurrence.parquet # Out: ... ``` \"\"\" super () . __init__ () if columns_to_load is not None : logger . warning ( \"'columns_to_load' is deprecated and won't be used\" ) if tables_to_load is not None : logger . warning ( \"'tables_to_load' is deprecated and won't be used\" ) self . spark_session = ( spark_session or SparkSession . builder . enableHiveSupport () . getOrCreate () ) self . database_name = database_name if database_type not in [ \"I2B2\" , \"OMOP\" ]: raise ValueError ( f \"`database_type` must be either 'I2B2' or 'OMOP'. Got { database_type } \" ) self . database_type = database_type if self . database_type == \"I2B2\" : self . database_source = \"cse\" if \"cse\" in self . database_name else \"edsprod\" self . omop_to_i2b2 = settings . i2b2_tables [ self . database_source ] self . i2b2_to_omop = defaultdict ( list ) for omop_table , i2b2_table in self . omop_to_i2b2 . items (): self . i2b2_to_omop [ i2b2_table ] . append ( omop_table ) self . user = os . environ [ \"USER\" ] self . person_ids , self . person_ids_df = self . _prepare_person_ids ( person_ids ) self . available_tables = self . list_available_tables () self . _tables = {} persist_tables_to_folder persist_tables_to_folder ( folder : str , person_ids : Optional [ Iterable [ int ]] = None , tables : List [ str ] = None , overwrite : bool = False ) -> None Save OMOP tables as parquet files in a given folder. PARAMETER DESCRIPTION folder path to folder where the tables will be written. TYPE: str person_ids : iterable person_ids to keep in the subcohort. tables : list of str, default None list of table names to save. Default value is data: ~eds_scikit.io.settings.default_tables_to_save . overwrite : bool, default=False whether to overwrite files if 'folder' already exists. Source code in eds_scikit/io/hive.py 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 def persist_tables_to_folder ( self , folder : str , person_ids : Optional [ Iterable [ int ]] = None , tables : List [ str ] = None , overwrite : bool = False , ) -> None : \"\"\"Save OMOP tables as parquet files in a given folder. Parameters ---------- folder : str path to folder where the tables will be written. person_ids : iterable person_ids to keep in the subcohort. tables : list of str, default None list of table names to save. Default value is :py:data:`~eds_scikit.io.settings.default_tables_to_save`. overwrite : bool, default=False whether to overwrite files if 'folder' already exists. \"\"\" # Manage tables if tables is None : tables = settings . default_tables_to_save unknown_tables = [ table for table in tables if table not in self . available_tables ] if unknown_tables : raise ValueError ( f \"The following tables are not available : { str ( unknown_tables ) } \" ) # Create folder folder = Path ( folder ) . absolute () if folder . exists () and overwrite : shutil . rmtree ( folder ) folder . mkdir ( parents = True , mode = 0o766 ) assert os . path . exists ( folder ) and os . path . isdir ( folder ), f \"Folder { folder } not found.\" # TODO: remove everything in this folder that is a valid # omop table. This prevents a user from having a # folder containing datasets generated from different # patient subsets. # TODO: maybe check how much the user wants to persist # to disk. Set a limit on the number of patients in the cohort ? if person_ids is not None : person_ids = self . _prepare_person_ids ( person_ids , return_df = False ) database_path = self . get_db_path () for idx , table in enumerate ( tables ): if self . database_type == \"I2B2\" : table_path = self . _hdfs_write_orc_to_parquet ( table , person_ids , overwrite ) else : table_path = os . path . join ( database_path , table ) df = self . get_table_from_parquet ( table_path , person_ids = person_ids ) local_file_path = os . path . join ( folder , f \" { table } .parquet\" ) df . to_parquet ( local_file_path , allow_truncated_timestamps = True , coerce_timestamps = \"ms\" , ) logger . info ( f \"( { idx + 1 } / { len ( tables ) } ) Table { table } saved at \" f \" { local_file_path } (N= { len ( df ) } ).\" ) get_db_path get_db_path () Get the HDFS path of the database Source code in eds_scikit/io/hive.py 343 344 345 346 347 348 349 350 def get_db_path ( self ): \"\"\"Get the HDFS path of the database\"\"\" return ( self . spark_session . sql ( f \"DESCRIBE DATABASE EXTENDED { self . database_name } \" ) . filter ( \"database_description_item=='Location'\" ) . collect ()[ 0 ] . database_description_value )","title":"`eds_scikit.io`"},{"location":"reference/io/#eds_scikitio","text":"","title":"eds_scikit.io"},{"location":"reference/io/#eds_scikit.io.PandasData","text":"PandasData ( folder : str ) Bases: BaseData Pandas interface to OMOP data stored as local parquet files/folders. PARAMETER DESCRIPTION folder absolute path to a folder containing several parquet files with OMOP data TYPE: str Examples: >>> data = PandasData ( folder = \"/export/home/USER/my_data/\" ) >>> person = data . person >>> person . shape (100, 10) Source code in eds_scikit/io/files.py 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 def __init__ ( self , folder : str , ): \"\"\"Pandas interface to OMOP data stored as local parquet files/folders. Parameters ---------- folder: str absolute path to a folder containing several parquet files with OMOP data Examples -------- >>> data = PandasData(folder=\"/export/home/USER/my_data/\") >>> person = data.person >>> person.shape (100, 10) \"\"\" super () . __init__ () self . folder = folder self . available_tables = self . list_available_tables () self . tables_paths = self . get_table_path () if not self . available_tables : raise ValueError ( f \"Folder { folder } does not contain any parquet omop data.\" )","title":"PandasData"},{"location":"reference/io/#eds_scikit.io.PostgresData","text":"PostgresData ( dbname : Optional [ str ] = None , schema : Optional [ str ] = None , user : Optional [ str ] = None , host : Optional [ str ] = None , port : Optional [ int ] = None ) Bases: BaseData Source code in eds_scikit/io/postgres.py 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 def __init__ ( self , dbname : Optional [ str ] = None , schema : Optional [ str ] = None , user : Optional [ str ] = None , host : Optional [ str ] = None , port : Optional [ int ] = None , ): ( self . host , self . port , self . dbname , self . user , ) = self . _find_matching_pgpass_params ( host , port , dbname , user ) self . schema = schema","title":"PostgresData"},{"location":"reference/io/#eds_scikit.io.postgres.PostgresData.read_sql","text":"read_sql ( sql_query : str , ** kwargs ) -> pd . DataFrame Execute pandas.read_sql() on the database. PARAMETER DESCRIPTION sql_query SQL query (postgres flavor) TYPE: str **kwargs additional arguments passed to pandas.read_sql() DEFAULT: {} RETURNS DESCRIPTION df TYPE: pandas . DataFrame Source code in eds_scikit/io/postgres.py 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 def read_sql ( self , sql_query : str , ** kwargs ) -> pd . DataFrame : \"\"\"Execute pandas.read_sql() on the database. Parameters ---------- sql_query : str SQL query (postgres flavor) **kwargs additional arguments passed to pandas.read_sql() Returns ------- df : pandas.DataFrame \"\"\" connection_infos = { param : getattr ( self , param ) for param in [ \"host\" , \"port\" , \"dbname\" , \"user\" ] } connection_infos [ \"password\" ] = pgpasslib . getpass ( ** connection_infos ) connection = pg . connect ( ** connection_infos ) if self . schema : connection . cursor () . execute ( f \"SET SCHEMA ' { self . schema } '\" ) df = pd . read_sql ( sql_query , con = connection , ** kwargs ) connection . close () return df","title":"read_sql()"},{"location":"reference/io/#eds_scikit.io.HiveData","text":"HiveData ( database_name : str , spark_session : Optional [ SparkSession ] = None , person_ids : Optional [ Iterable [ int ]] = None , tables_to_load : Optional [ Union [ Dict [ str , Optional [ List [ str ]]], List [ str ]]] = None , columns_to_load : Optional [ Union [ Dict [ str , Optional [ List [ str ]]], List [ str ]]] = None , database_type : Optional [ str ] = 'OMOP' ) Bases: BaseData Spark interface for OMOP data stored in a Hive database. This class provides a simple access to data stored in Hive. Data is returned as koalas dataframes that match the tables stored in Hive. PARAMETER DESCRIPTION database_name The name of you database in Hive. Ex: \"cse_82727572\" TYPE: str spark_session If None, a SparkSession will be retrieved or created via SparkSession.builder.enableHiveSupport().getOrCreate() TYPE: pyspark . sql . SparkSession DEFAULT: None person_ids An iterable of person_id that is used to define a subset of the database. TYPE: Optional [ Iterable [ int ]] DEFAULT: None tables_to_load deprecated TYPE: dict , default DEFAULT: None columns_to_load deprecated TYPE: dict , default DEFAULT: None database_type Whether to use the native OMOP schema or to convert I2B2 inputs to OMOP. TYPE: Optional [ str ] DEFAULT: 'OMOP' ATTRIBUTE DESCRIPTION person Hive data for table person as a koalas dataframe. Other OMOP tables can also be accessed as attributes TYPE: koalas dataframe available_tables names of OMOP tables that can be accessed as attributes with this HiveData object. TYPE: list of str Examples: data = HiveData ( database_name = \"edsomop_prod_a\" ) data . available_tables # Out: [\"person\", \"care_site\", \"condition_occurrence\", ... ] person = data . person type ( person ) # Out: databricks.koalas.frame.DataFrame person [ \"person_id\" ] . count () # Out: 12670874 This class can be used to create a subset of data for a given list of person_id . This is useful because the smaller dataset can then be used to prototype more rapidly. my_person_ids = [ 9226726 , 2092082 , ... ] data = HiveData ( spark_session = spark , database_name = \"edsomop_prod_a\" , person_ids = my_person_ids ) data . person [ \"person_id\" ] . count () # Out: 1000 tables_to_save = [ \"person\" , \"visit_occurrence\" ] data . persist_tables_to_folder ( \"./cohort_sample_1000\" , table_names = tables_to_save ) # Out: writing /export/home/USER/cohort_sample_1000/person.parquet # Out: writing /export/home/USER/cohort_sample_1000/visit_occurrence.parquet # Out: ... Source code in eds_scikit/io/hive.py 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 def __init__ ( self , database_name : str , spark_session : Optional [ SparkSession ] = None , person_ids : Optional [ Iterable [ int ]] = None , tables_to_load : Optional [ Union [ Dict [ str , Optional [ List [ str ]]], List [ str ]] ] = None , columns_to_load : Optional [ Union [ Dict [ str , Optional [ List [ str ]]], List [ str ]] ] = None , database_type : Optional [ str ] = \"OMOP\" , ): \"\"\"Spark interface for OMOP data stored in a Hive database. This class provides a simple access to data stored in Hive. Data is returned as koalas dataframes that match the tables stored in Hive. Parameters ---------- database_name : str The name of you database in Hive. Ex: \"cse_82727572\" spark_session : pyspark.sql.SparkSession If None, a SparkSession will be retrieved or created via `SparkSession.builder.enableHiveSupport().getOrCreate()` person_ids : Optional[Iterable[int]] An iterable of `person_id` that is used to define a subset of the database. tables_to_load : dict, default=None *deprecated* columns_to_load : dict, default=None *deprecated* database_type: Optional[str] = 'OMOP'. Must be 'OMOP' or 'I2B2' Whether to use the native OMOP schema or to convert I2B2 inputs to OMOP. Attributes ---------- person : koalas dataframe Hive data for table `person` as a koalas dataframe. Other OMOP tables can also be accessed as attributes available_tables : list of str names of OMOP tables that can be accessed as attributes with this HiveData object. Examples -------- ```python data = HiveData(database_name=\"edsomop_prod_a\") data.available_tables # Out: [\"person\", \"care_site\", \"condition_occurrence\", ... ] person = data.person type(person) # Out: databricks.koalas.frame.DataFrame person[\"person_id\"].count() # Out: 12670874 ``` This class can be used to create a subset of data for a given list of `person_id`. This is useful because the smaller dataset can then be used to prototype more rapidly. ```python my_person_ids = [9226726, 2092082, ...] data = HiveData( spark_session=spark, database_name=\"edsomop_prod_a\", person_ids=my_person_ids ) data.person[\"person_id\"].count() # Out: 1000 tables_to_save = [\"person\", \"visit_occurrence\"] data.persist_tables_to_folder(\"./cohort_sample_1000\", table_names=tables_to_save) # Out: writing /export/home/USER/cohort_sample_1000/person.parquet # Out: writing /export/home/USER/cohort_sample_1000/visit_occurrence.parquet # Out: ... ``` \"\"\" super () . __init__ () if columns_to_load is not None : logger . warning ( \"'columns_to_load' is deprecated and won't be used\" ) if tables_to_load is not None : logger . warning ( \"'tables_to_load' is deprecated and won't be used\" ) self . spark_session = ( spark_session or SparkSession . builder . enableHiveSupport () . getOrCreate () ) self . database_name = database_name if database_type not in [ \"I2B2\" , \"OMOP\" ]: raise ValueError ( f \"`database_type` must be either 'I2B2' or 'OMOP'. Got { database_type } \" ) self . database_type = database_type if self . database_type == \"I2B2\" : self . database_source = \"cse\" if \"cse\" in self . database_name else \"edsprod\" self . omop_to_i2b2 = settings . i2b2_tables [ self . database_source ] self . i2b2_to_omop = defaultdict ( list ) for omop_table , i2b2_table in self . omop_to_i2b2 . items (): self . i2b2_to_omop [ i2b2_table ] . append ( omop_table ) self . user = os . environ [ \"USER\" ] self . person_ids , self . person_ids_df = self . _prepare_person_ids ( person_ids ) self . available_tables = self . list_available_tables () self . _tables = {}","title":"HiveData"},{"location":"reference/io/#eds_scikit.io.hive.HiveData.persist_tables_to_folder","text":"persist_tables_to_folder ( folder : str , person_ids : Optional [ Iterable [ int ]] = None , tables : List [ str ] = None , overwrite : bool = False ) -> None Save OMOP tables as parquet files in a given folder. PARAMETER DESCRIPTION folder path to folder where the tables will be written. TYPE: str person_ids : iterable person_ids to keep in the subcohort. tables : list of str, default None list of table names to save. Default value is data: ~eds_scikit.io.settings.default_tables_to_save . overwrite : bool, default=False whether to overwrite files if 'folder' already exists. Source code in eds_scikit/io/hive.py 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 def persist_tables_to_folder ( self , folder : str , person_ids : Optional [ Iterable [ int ]] = None , tables : List [ str ] = None , overwrite : bool = False , ) -> None : \"\"\"Save OMOP tables as parquet files in a given folder. Parameters ---------- folder : str path to folder where the tables will be written. person_ids : iterable person_ids to keep in the subcohort. tables : list of str, default None list of table names to save. Default value is :py:data:`~eds_scikit.io.settings.default_tables_to_save`. overwrite : bool, default=False whether to overwrite files if 'folder' already exists. \"\"\" # Manage tables if tables is None : tables = settings . default_tables_to_save unknown_tables = [ table for table in tables if table not in self . available_tables ] if unknown_tables : raise ValueError ( f \"The following tables are not available : { str ( unknown_tables ) } \" ) # Create folder folder = Path ( folder ) . absolute () if folder . exists () and overwrite : shutil . rmtree ( folder ) folder . mkdir ( parents = True , mode = 0o766 ) assert os . path . exists ( folder ) and os . path . isdir ( folder ), f \"Folder { folder } not found.\" # TODO: remove everything in this folder that is a valid # omop table. This prevents a user from having a # folder containing datasets generated from different # patient subsets. # TODO: maybe check how much the user wants to persist # to disk. Set a limit on the number of patients in the cohort ? if person_ids is not None : person_ids = self . _prepare_person_ids ( person_ids , return_df = False ) database_path = self . get_db_path () for idx , table in enumerate ( tables ): if self . database_type == \"I2B2\" : table_path = self . _hdfs_write_orc_to_parquet ( table , person_ids , overwrite ) else : table_path = os . path . join ( database_path , table ) df = self . get_table_from_parquet ( table_path , person_ids = person_ids ) local_file_path = os . path . join ( folder , f \" { table } .parquet\" ) df . to_parquet ( local_file_path , allow_truncated_timestamps = True , coerce_timestamps = \"ms\" , ) logger . info ( f \"( { idx + 1 } / { len ( tables ) } ) Table { table } saved at \" f \" { local_file_path } (N= { len ( df ) } ).\" )","title":"persist_tables_to_folder()"},{"location":"reference/io/#eds_scikit.io.hive.HiveData.get_db_path","text":"get_db_path () Get the HDFS path of the database Source code in eds_scikit/io/hive.py 343 344 345 346 347 348 349 350 def get_db_path ( self ): \"\"\"Get the HDFS path of the database\"\"\" return ( self . spark_session . sql ( f \"DESCRIBE DATABASE EXTENDED { self . database_name } \" ) . filter ( \"database_description_item=='Location'\" ) . collect ()[ 0 ] . database_description_value )","title":"get_db_path()"},{"location":"reference/io/base/","text":"eds_scikit.io.base","title":"base"},{"location":"reference/io/base/#eds_scikitiobase","text":"","title":"eds_scikit.io.base"},{"location":"reference/io/data_quality/","text":"eds_scikit.io.data_quality","title":"data_quality"},{"location":"reference/io/data_quality/#eds_scikitiodata_quality","text":"","title":"eds_scikit.io.data_quality"},{"location":"reference/io/files/","text":"eds_scikit.io.files PandasData PandasData ( folder : str ) Bases: BaseData Pandas interface to OMOP data stored as local parquet files/folders. PARAMETER DESCRIPTION folder absolute path to a folder containing several parquet files with OMOP data TYPE: str Examples: >>> data = PandasData ( folder = \"/export/home/USER/my_data/\" ) >>> person = data . person >>> person . shape (100, 10) Source code in eds_scikit/io/files.py 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 def __init__ ( self , folder : str , ): \"\"\"Pandas interface to OMOP data stored as local parquet files/folders. Parameters ---------- folder: str absolute path to a folder containing several parquet files with OMOP data Examples -------- >>> data = PandasData(folder=\"/export/home/USER/my_data/\") >>> person = data.person >>> person.shape (100, 10) \"\"\" super () . __init__ () self . folder = folder self . available_tables = self . list_available_tables () self . tables_paths = self . get_table_path () if not self . available_tables : raise ValueError ( f \"Folder { folder } does not contain any parquet omop data.\" )","title":"files"},{"location":"reference/io/files/#eds_scikitiofiles","text":"","title":"eds_scikit.io.files"},{"location":"reference/io/files/#eds_scikit.io.files.PandasData","text":"PandasData ( folder : str ) Bases: BaseData Pandas interface to OMOP data stored as local parquet files/folders. PARAMETER DESCRIPTION folder absolute path to a folder containing several parquet files with OMOP data TYPE: str Examples: >>> data = PandasData ( folder = \"/export/home/USER/my_data/\" ) >>> person = data . person >>> person . shape (100, 10) Source code in eds_scikit/io/files.py 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 def __init__ ( self , folder : str , ): \"\"\"Pandas interface to OMOP data stored as local parquet files/folders. Parameters ---------- folder: str absolute path to a folder containing several parquet files with OMOP data Examples -------- >>> data = PandasData(folder=\"/export/home/USER/my_data/\") >>> person = data.person >>> person.shape (100, 10) \"\"\" super () . __init__ () self . folder = folder self . available_tables = self . list_available_tables () self . tables_paths = self . get_table_path () if not self . available_tables : raise ValueError ( f \"Folder { folder } does not contain any parquet omop data.\" )","title":"PandasData"},{"location":"reference/io/hive/","text":"eds_scikit.io.hive HiveData HiveData ( database_name : str , spark_session : Optional [ SparkSession ] = None , person_ids : Optional [ Iterable [ int ]] = None , tables_to_load : Optional [ Union [ Dict [ str , Optional [ List [ str ]]], List [ str ]]] = None , columns_to_load : Optional [ Union [ Dict [ str , Optional [ List [ str ]]], List [ str ]]] = None , database_type : Optional [ str ] = 'OMOP' ) Bases: BaseData Spark interface for OMOP data stored in a Hive database. This class provides a simple access to data stored in Hive. Data is returned as koalas dataframes that match the tables stored in Hive. PARAMETER DESCRIPTION database_name The name of you database in Hive. Ex: \"cse_82727572\" TYPE: str spark_session If None, a SparkSession will be retrieved or created via SparkSession.builder.enableHiveSupport().getOrCreate() TYPE: pyspark . sql . SparkSession DEFAULT: None person_ids An iterable of person_id that is used to define a subset of the database. TYPE: Optional [ Iterable [ int ]] DEFAULT: None tables_to_load deprecated TYPE: dict , default DEFAULT: None columns_to_load deprecated TYPE: dict , default DEFAULT: None database_type Whether to use the native OMOP schema or to convert I2B2 inputs to OMOP. TYPE: Optional [ str ] DEFAULT: 'OMOP' ATTRIBUTE DESCRIPTION person Hive data for table person as a koalas dataframe. Other OMOP tables can also be accessed as attributes TYPE: koalas dataframe available_tables names of OMOP tables that can be accessed as attributes with this HiveData object. TYPE: list of str Examples: data = HiveData ( database_name = \"edsomop_prod_a\" ) data . available_tables # Out: [\"person\", \"care_site\", \"condition_occurrence\", ... ] person = data . person type ( person ) # Out: databricks.koalas.frame.DataFrame person [ \"person_id\" ] . count () # Out: 12670874 This class can be used to create a subset of data for a given list of person_id . This is useful because the smaller dataset can then be used to prototype more rapidly. my_person_ids = [ 9226726 , 2092082 , ... ] data = HiveData ( spark_session = spark , database_name = \"edsomop_prod_a\" , person_ids = my_person_ids ) data . person [ \"person_id\" ] . count () # Out: 1000 tables_to_save = [ \"person\" , \"visit_occurrence\" ] data . persist_tables_to_folder ( \"./cohort_sample_1000\" , table_names = tables_to_save ) # Out: writing /export/home/USER/cohort_sample_1000/person.parquet # Out: writing /export/home/USER/cohort_sample_1000/visit_occurrence.parquet # Out: ... Source code in eds_scikit/io/hive.py 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 def __init__ ( self , database_name : str , spark_session : Optional [ SparkSession ] = None , person_ids : Optional [ Iterable [ int ]] = None , tables_to_load : Optional [ Union [ Dict [ str , Optional [ List [ str ]]], List [ str ]] ] = None , columns_to_load : Optional [ Union [ Dict [ str , Optional [ List [ str ]]], List [ str ]] ] = None , database_type : Optional [ str ] = \"OMOP\" , ): \"\"\"Spark interface for OMOP data stored in a Hive database. This class provides a simple access to data stored in Hive. Data is returned as koalas dataframes that match the tables stored in Hive. Parameters ---------- database_name : str The name of you database in Hive. Ex: \"cse_82727572\" spark_session : pyspark.sql.SparkSession If None, a SparkSession will be retrieved or created via `SparkSession.builder.enableHiveSupport().getOrCreate()` person_ids : Optional[Iterable[int]] An iterable of `person_id` that is used to define a subset of the database. tables_to_load : dict, default=None *deprecated* columns_to_load : dict, default=None *deprecated* database_type: Optional[str] = 'OMOP'. Must be 'OMOP' or 'I2B2' Whether to use the native OMOP schema or to convert I2B2 inputs to OMOP. Attributes ---------- person : koalas dataframe Hive data for table `person` as a koalas dataframe. Other OMOP tables can also be accessed as attributes available_tables : list of str names of OMOP tables that can be accessed as attributes with this HiveData object. Examples -------- ```python data = HiveData(database_name=\"edsomop_prod_a\") data.available_tables # Out: [\"person\", \"care_site\", \"condition_occurrence\", ... ] person = data.person type(person) # Out: databricks.koalas.frame.DataFrame person[\"person_id\"].count() # Out: 12670874 ``` This class can be used to create a subset of data for a given list of `person_id`. This is useful because the smaller dataset can then be used to prototype more rapidly. ```python my_person_ids = [9226726, 2092082, ...] data = HiveData( spark_session=spark, database_name=\"edsomop_prod_a\", person_ids=my_person_ids ) data.person[\"person_id\"].count() # Out: 1000 tables_to_save = [\"person\", \"visit_occurrence\"] data.persist_tables_to_folder(\"./cohort_sample_1000\", table_names=tables_to_save) # Out: writing /export/home/USER/cohort_sample_1000/person.parquet # Out: writing /export/home/USER/cohort_sample_1000/visit_occurrence.parquet # Out: ... ``` \"\"\" super () . __init__ () if columns_to_load is not None : logger . warning ( \"'columns_to_load' is deprecated and won't be used\" ) if tables_to_load is not None : logger . warning ( \"'tables_to_load' is deprecated and won't be used\" ) self . spark_session = ( spark_session or SparkSession . builder . enableHiveSupport () . getOrCreate () ) self . database_name = database_name if database_type not in [ \"I2B2\" , \"OMOP\" ]: raise ValueError ( f \"`database_type` must be either 'I2B2' or 'OMOP'. Got { database_type } \" ) self . database_type = database_type if self . database_type == \"I2B2\" : self . database_source = \"cse\" if \"cse\" in self . database_name else \"edsprod\" self . omop_to_i2b2 = settings . i2b2_tables [ self . database_source ] self . i2b2_to_omop = defaultdict ( list ) for omop_table , i2b2_table in self . omop_to_i2b2 . items (): self . i2b2_to_omop [ i2b2_table ] . append ( omop_table ) self . user = os . environ [ \"USER\" ] self . person_ids , self . person_ids_df = self . _prepare_person_ids ( person_ids ) self . available_tables = self . list_available_tables () self . _tables = {} persist_tables_to_folder persist_tables_to_folder ( folder : str , person_ids : Optional [ Iterable [ int ]] = None , tables : List [ str ] = None , overwrite : bool = False ) -> None Save OMOP tables as parquet files in a given folder. PARAMETER DESCRIPTION folder path to folder where the tables will be written. TYPE: str person_ids : iterable person_ids to keep in the subcohort. tables : list of str, default None list of table names to save. Default value is data: ~eds_scikit.io.settings.default_tables_to_save . overwrite : bool, default=False whether to overwrite files if 'folder' already exists. Source code in eds_scikit/io/hive.py 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 def persist_tables_to_folder ( self , folder : str , person_ids : Optional [ Iterable [ int ]] = None , tables : List [ str ] = None , overwrite : bool = False , ) -> None : \"\"\"Save OMOP tables as parquet files in a given folder. Parameters ---------- folder : str path to folder where the tables will be written. person_ids : iterable person_ids to keep in the subcohort. tables : list of str, default None list of table names to save. Default value is :py:data:`~eds_scikit.io.settings.default_tables_to_save`. overwrite : bool, default=False whether to overwrite files if 'folder' already exists. \"\"\" # Manage tables if tables is None : tables = settings . default_tables_to_save unknown_tables = [ table for table in tables if table not in self . available_tables ] if unknown_tables : raise ValueError ( f \"The following tables are not available : { str ( unknown_tables ) } \" ) # Create folder folder = Path ( folder ) . absolute () if folder . exists () and overwrite : shutil . rmtree ( folder ) folder . mkdir ( parents = True , mode = 0o766 ) assert os . path . exists ( folder ) and os . path . isdir ( folder ), f \"Folder { folder } not found.\" # TODO: remove everything in this folder that is a valid # omop table. This prevents a user from having a # folder containing datasets generated from different # patient subsets. # TODO: maybe check how much the user wants to persist # to disk. Set a limit on the number of patients in the cohort ? if person_ids is not None : person_ids = self . _prepare_person_ids ( person_ids , return_df = False ) database_path = self . get_db_path () for idx , table in enumerate ( tables ): if self . database_type == \"I2B2\" : table_path = self . _hdfs_write_orc_to_parquet ( table , person_ids , overwrite ) else : table_path = os . path . join ( database_path , table ) df = self . get_table_from_parquet ( table_path , person_ids = person_ids ) local_file_path = os . path . join ( folder , f \" { table } .parquet\" ) df . to_parquet ( local_file_path , allow_truncated_timestamps = True , coerce_timestamps = \"ms\" , ) logger . info ( f \"( { idx + 1 } / { len ( tables ) } ) Table { table } saved at \" f \" { local_file_path } (N= { len ( df ) } ).\" ) get_db_path get_db_path () Get the HDFS path of the database Source code in eds_scikit/io/hive.py 343 344 345 346 347 348 349 350 def get_db_path ( self ): \"\"\"Get the HDFS path of the database\"\"\" return ( self . spark_session . sql ( f \"DESCRIBE DATABASE EXTENDED { self . database_name } \" ) . filter ( \"database_description_item=='Location'\" ) . collect ()[ 0 ] . database_description_value )","title":"hive"},{"location":"reference/io/hive/#eds_scikitiohive","text":"","title":"eds_scikit.io.hive"},{"location":"reference/io/hive/#eds_scikit.io.hive.HiveData","text":"HiveData ( database_name : str , spark_session : Optional [ SparkSession ] = None , person_ids : Optional [ Iterable [ int ]] = None , tables_to_load : Optional [ Union [ Dict [ str , Optional [ List [ str ]]], List [ str ]]] = None , columns_to_load : Optional [ Union [ Dict [ str , Optional [ List [ str ]]], List [ str ]]] = None , database_type : Optional [ str ] = 'OMOP' ) Bases: BaseData Spark interface for OMOP data stored in a Hive database. This class provides a simple access to data stored in Hive. Data is returned as koalas dataframes that match the tables stored in Hive. PARAMETER DESCRIPTION database_name The name of you database in Hive. Ex: \"cse_82727572\" TYPE: str spark_session If None, a SparkSession will be retrieved or created via SparkSession.builder.enableHiveSupport().getOrCreate() TYPE: pyspark . sql . SparkSession DEFAULT: None person_ids An iterable of person_id that is used to define a subset of the database. TYPE: Optional [ Iterable [ int ]] DEFAULT: None tables_to_load deprecated TYPE: dict , default DEFAULT: None columns_to_load deprecated TYPE: dict , default DEFAULT: None database_type Whether to use the native OMOP schema or to convert I2B2 inputs to OMOP. TYPE: Optional [ str ] DEFAULT: 'OMOP' ATTRIBUTE DESCRIPTION person Hive data for table person as a koalas dataframe. Other OMOP tables can also be accessed as attributes TYPE: koalas dataframe available_tables names of OMOP tables that can be accessed as attributes with this HiveData object. TYPE: list of str Examples: data = HiveData ( database_name = \"edsomop_prod_a\" ) data . available_tables # Out: [\"person\", \"care_site\", \"condition_occurrence\", ... ] person = data . person type ( person ) # Out: databricks.koalas.frame.DataFrame person [ \"person_id\" ] . count () # Out: 12670874 This class can be used to create a subset of data for a given list of person_id . This is useful because the smaller dataset can then be used to prototype more rapidly. my_person_ids = [ 9226726 , 2092082 , ... ] data = HiveData ( spark_session = spark , database_name = \"edsomop_prod_a\" , person_ids = my_person_ids ) data . person [ \"person_id\" ] . count () # Out: 1000 tables_to_save = [ \"person\" , \"visit_occurrence\" ] data . persist_tables_to_folder ( \"./cohort_sample_1000\" , table_names = tables_to_save ) # Out: writing /export/home/USER/cohort_sample_1000/person.parquet # Out: writing /export/home/USER/cohort_sample_1000/visit_occurrence.parquet # Out: ... Source code in eds_scikit/io/hive.py 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 def __init__ ( self , database_name : str , spark_session : Optional [ SparkSession ] = None , person_ids : Optional [ Iterable [ int ]] = None , tables_to_load : Optional [ Union [ Dict [ str , Optional [ List [ str ]]], List [ str ]] ] = None , columns_to_load : Optional [ Union [ Dict [ str , Optional [ List [ str ]]], List [ str ]] ] = None , database_type : Optional [ str ] = \"OMOP\" , ): \"\"\"Spark interface for OMOP data stored in a Hive database. This class provides a simple access to data stored in Hive. Data is returned as koalas dataframes that match the tables stored in Hive. Parameters ---------- database_name : str The name of you database in Hive. Ex: \"cse_82727572\" spark_session : pyspark.sql.SparkSession If None, a SparkSession will be retrieved or created via `SparkSession.builder.enableHiveSupport().getOrCreate()` person_ids : Optional[Iterable[int]] An iterable of `person_id` that is used to define a subset of the database. tables_to_load : dict, default=None *deprecated* columns_to_load : dict, default=None *deprecated* database_type: Optional[str] = 'OMOP'. Must be 'OMOP' or 'I2B2' Whether to use the native OMOP schema or to convert I2B2 inputs to OMOP. Attributes ---------- person : koalas dataframe Hive data for table `person` as a koalas dataframe. Other OMOP tables can also be accessed as attributes available_tables : list of str names of OMOP tables that can be accessed as attributes with this HiveData object. Examples -------- ```python data = HiveData(database_name=\"edsomop_prod_a\") data.available_tables # Out: [\"person\", \"care_site\", \"condition_occurrence\", ... ] person = data.person type(person) # Out: databricks.koalas.frame.DataFrame person[\"person_id\"].count() # Out: 12670874 ``` This class can be used to create a subset of data for a given list of `person_id`. This is useful because the smaller dataset can then be used to prototype more rapidly. ```python my_person_ids = [9226726, 2092082, ...] data = HiveData( spark_session=spark, database_name=\"edsomop_prod_a\", person_ids=my_person_ids ) data.person[\"person_id\"].count() # Out: 1000 tables_to_save = [\"person\", \"visit_occurrence\"] data.persist_tables_to_folder(\"./cohort_sample_1000\", table_names=tables_to_save) # Out: writing /export/home/USER/cohort_sample_1000/person.parquet # Out: writing /export/home/USER/cohort_sample_1000/visit_occurrence.parquet # Out: ... ``` \"\"\" super () . __init__ () if columns_to_load is not None : logger . warning ( \"'columns_to_load' is deprecated and won't be used\" ) if tables_to_load is not None : logger . warning ( \"'tables_to_load' is deprecated and won't be used\" ) self . spark_session = ( spark_session or SparkSession . builder . enableHiveSupport () . getOrCreate () ) self . database_name = database_name if database_type not in [ \"I2B2\" , \"OMOP\" ]: raise ValueError ( f \"`database_type` must be either 'I2B2' or 'OMOP'. Got { database_type } \" ) self . database_type = database_type if self . database_type == \"I2B2\" : self . database_source = \"cse\" if \"cse\" in self . database_name else \"edsprod\" self . omop_to_i2b2 = settings . i2b2_tables [ self . database_source ] self . i2b2_to_omop = defaultdict ( list ) for omop_table , i2b2_table in self . omop_to_i2b2 . items (): self . i2b2_to_omop [ i2b2_table ] . append ( omop_table ) self . user = os . environ [ \"USER\" ] self . person_ids , self . person_ids_df = self . _prepare_person_ids ( person_ids ) self . available_tables = self . list_available_tables () self . _tables = {}","title":"HiveData"},{"location":"reference/io/hive/#eds_scikit.io.hive.HiveData.persist_tables_to_folder","text":"persist_tables_to_folder ( folder : str , person_ids : Optional [ Iterable [ int ]] = None , tables : List [ str ] = None , overwrite : bool = False ) -> None Save OMOP tables as parquet files in a given folder. PARAMETER DESCRIPTION folder path to folder where the tables will be written. TYPE: str person_ids : iterable person_ids to keep in the subcohort. tables : list of str, default None list of table names to save. Default value is data: ~eds_scikit.io.settings.default_tables_to_save . overwrite : bool, default=False whether to overwrite files if 'folder' already exists. Source code in eds_scikit/io/hive.py 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 def persist_tables_to_folder ( self , folder : str , person_ids : Optional [ Iterable [ int ]] = None , tables : List [ str ] = None , overwrite : bool = False , ) -> None : \"\"\"Save OMOP tables as parquet files in a given folder. Parameters ---------- folder : str path to folder where the tables will be written. person_ids : iterable person_ids to keep in the subcohort. tables : list of str, default None list of table names to save. Default value is :py:data:`~eds_scikit.io.settings.default_tables_to_save`. overwrite : bool, default=False whether to overwrite files if 'folder' already exists. \"\"\" # Manage tables if tables is None : tables = settings . default_tables_to_save unknown_tables = [ table for table in tables if table not in self . available_tables ] if unknown_tables : raise ValueError ( f \"The following tables are not available : { str ( unknown_tables ) } \" ) # Create folder folder = Path ( folder ) . absolute () if folder . exists () and overwrite : shutil . rmtree ( folder ) folder . mkdir ( parents = True , mode = 0o766 ) assert os . path . exists ( folder ) and os . path . isdir ( folder ), f \"Folder { folder } not found.\" # TODO: remove everything in this folder that is a valid # omop table. This prevents a user from having a # folder containing datasets generated from different # patient subsets. # TODO: maybe check how much the user wants to persist # to disk. Set a limit on the number of patients in the cohort ? if person_ids is not None : person_ids = self . _prepare_person_ids ( person_ids , return_df = False ) database_path = self . get_db_path () for idx , table in enumerate ( tables ): if self . database_type == \"I2B2\" : table_path = self . _hdfs_write_orc_to_parquet ( table , person_ids , overwrite ) else : table_path = os . path . join ( database_path , table ) df = self . get_table_from_parquet ( table_path , person_ids = person_ids ) local_file_path = os . path . join ( folder , f \" { table } .parquet\" ) df . to_parquet ( local_file_path , allow_truncated_timestamps = True , coerce_timestamps = \"ms\" , ) logger . info ( f \"( { idx + 1 } / { len ( tables ) } ) Table { table } saved at \" f \" { local_file_path } (N= { len ( df ) } ).\" )","title":"persist_tables_to_folder()"},{"location":"reference/io/hive/#eds_scikit.io.hive.HiveData.get_db_path","text":"get_db_path () Get the HDFS path of the database Source code in eds_scikit/io/hive.py 343 344 345 346 347 348 349 350 def get_db_path ( self ): \"\"\"Get the HDFS path of the database\"\"\" return ( self . spark_session . sql ( f \"DESCRIBE DATABASE EXTENDED { self . database_name } \" ) . filter ( \"database_description_item=='Location'\" ) . collect ()[ 0 ] . database_description_value )","title":"get_db_path()"},{"location":"reference/io/i2b2_mapping/","text":"eds_scikit.io.i2b2_mapping get_i2b2_table get_i2b2_table ( spark_session : SparkSession , db_name : str , db_source : str , table : str ) -> SparkDataFrame Convert a Spark table from i2b2 to OMOP format. PARAMETER DESCRIPTION db_name Name of the database where the data is stored. TYPE: str table Name of the table to extract. TYPE: str RETURNS DESCRIPTION df Spark DataFrame extracted from the i2b2 database given and converted to OMOP standard. TYPE: Spark DataFrame Source code in eds_scikit/io/i2b2_mapping.py 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 def get_i2b2_table ( spark_session : SparkSession , db_name : str , db_source : str , table : str ) -> SparkDataFrame : \"\"\" Convert a Spark table from i2b2 to OMOP format. Parameters ---------- db_name: str Name of the database where the data is stored. table: str Name of the table to extract. Returns ------- df: Spark DataFrame Spark DataFrame extracted from the i2b2 database given and converted to OMOP standard. \"\"\" i2b2_table_name = i2b2_tables [ db_source ][ table ] # Dictionary of omop_col -> i2b2_col columns = i2b2_renaming . get ( table ) # Can be None if creating a table from scratch (e.g. concept_relationship if columns is not None : query = f \"describe { db_name } . { i2b2_table_name } \" available_columns = set ( spark_session . sql ( query ) . toPandas () . col_name . tolist ()) if db_source == \"cse\" : columns . pop ( \"i2b2_action\" , None ) cols = \", \" . join ( [ f \" { i2b2 } AS { omop } \" for omop , i2b2 in columns . items () if i2b2 in available_columns ] ) query = f \"SELECT { cols } FROM { db_name } . { i2b2_table_name } \" df = spark_session . sql ( query ) # Special mapping for i2b2 : # CIM10 if table == \"condition_occurrence\" : df = df . withColumn ( \"condition_source_value\" , F . substring ( F . col ( \"condition_source_value\" ), 7 , 20 ), ) # CCAM elif table == \"procedure_occurrence\" : df = df . withColumn ( \"procedure_source_value\" , F . substring ( F . col ( \"procedure_source_value\" ), 6 , 20 ), ) # Visits elif table == \"visit_occurrence\" : df = df . withColumn ( \"visit_source_value\" , mapping_dict ( visit_type_mapping , \"Non Renseign\u00e9\" )( F . col ( \"visit_source_value\" ) ), ) if db_source == \"cse\" : df = df . withColumn ( \"row_status_source_value\" , F . lit ( \"Actif\" )) df = df . withColumn ( \"visit_occurrence_source_value\" , df [ \"visit_occurrence_id\" ] ) else : df = df . withColumn ( \"row_status_source_value\" , F . when ( F . col ( \"row_status_source_value\" ) . isin ([ - 1 , - 2 ]), \"supprim\u00e9\" ) . otherwise ( \"Actif\" ), ) # Retrieve Hospital trigram ufr = spark_session . sql ( f \"SELECT * FROM { db_name } . { i2b2_tables [ db_source ][ 'visit_detail' ] } \" ) ufr = ufr . withColumn ( \"care_site_id\" , F . substring ( F . split ( F . col ( \"concept_cd\" ), \":\" ) . getItem ( 1 ), 1 , 3 ), ) ufr = ufr . withColumnRenamed ( \"encounter_num\" , \"visit_occurrence_id\" ) ufr = ufr . drop_duplicates ( subset = [ \"visit_occurrence_id\" ]) ufr = ufr . select ([ \"visit_occurrence_id\" , \"care_site_id\" ]) df = df . join ( ufr , how = \"inner\" , on = [ \"visit_occurrence_id\" ]) # Patients elif table == \"person\" : df = df . withColumn ( \"gender_source_value\" , mapping_dict ( sex_cd_mapping , \"Non Renseign\u00e9\" )( F . col ( \"gender_source_value\" )), ) # Documents elif table . startswith ( \"note\" ): df = df . withColumn ( \"note_class_source_value\" , F . substring ( F . col ( \"note_class_source_value\" ), 4 , 100 ), ) if db_source == \"cse\" : df = df . withColumn ( \"row_status_source_value\" , F . lit ( \"Actif\" )) else : df = df . withColumn ( \"row_status_source_value\" , F . when ( F . col ( \"row_status_source_value\" ) < 0 , \"SUPP\" ) . otherwise ( \"Actif\" ), ) # Hospital trigrams elif table == \"care_site\" : df = df . withColumn ( \"care_site_type_source_value\" , F . lit ( \"H\u00f4pital\" )) df = df . withColumn ( \"care_site_source_value\" , F . split ( F . col ( \"care_site_source_value\" ), \":\" ) . getItem ( 1 ), ) df = df . withColumn ( \"care_site_id\" , F . substring ( F . col ( \"care_site_source_value\" ), 1 , 3 ) ) df = df . drop_duplicates ( subset = [ \"care_site_id\" ]) df = df . withColumn ( \"care_site_short_name\" , mapping_dict ( dict_code_UFR , \"Non Renseign\u00e9\" )( F . col ( \"care_site_id\" )), ) # UFR elif table == \"visit_detail\" : df = df . withColumn ( \"care_site_id\" , F . split ( F . col ( \"care_site_id\" ), \":\" ) . getItem ( 1 ) ) df = df . withColumn ( \"visit_detail_type_source_value\" , F . lit ( \"PASS\" )) df = df . withColumn ( \"row_status_source_value\" , F . lit ( \"Actif\" )) # measurement elif table == \"measurement\" : df = df . withColumn ( \"measurement_source_concept_id\" , F . substring ( F . col ( \"measurement_source_concept_id\" ), 5 , 20 ), ) . withColumn ( \"row_status_source_value\" , F . lit ( \"Valid\u00e9\" )) # concept elif table == \"concept\" : df = ( df . withColumn ( \"concept_source_value\" , F . substring ( F . col ( \"concept_source_value\" ), 5 , 20 ), # TODO: use regexp_extract to take substring after ':' ) . withColumn ( \"concept_id\" , F . col ( \"concept_source_value\" )) . withColumn ( \"concept_code\" , F . col ( \"concept_id\" )) . withColumn ( \"vocabulary_id\" , F . lit ( \"ANABIO\" )) ) # Adding LOINC if \"get_additional_i2b2_concept\" in registry . data . get_all (): loinc_pd = registry . get ( \"data\" , \"get_additional_i2b2_concept\" )() assert len ( loinc_pd . columns ) == len ( df . columns ) loinc_pd = loinc_pd [ df . columns ] # for columns ordering df = df . union ( spark_session . createDataFrame ( loinc_pd , df . schema , verifySchema = False ) ) . cache () # fact_relationship elif table == \"fact_relationship\" : # Retrieve UF information df = df . withColumn ( \"fact_id_1\" , F . split ( F . col ( \"care_site_source_value\" ), \":\" ) . getItem ( 1 ), ) df = df . withColumn ( \"domain_concept_id_1\" , F . lit ( 57 )) # Care_site domain # Retrieve hospital information df = df . withColumn ( \"fact_id_2\" , F . substring ( F . col ( \"fact_id_1\" ), 1 , 3 )) df = df . withColumn ( \"domain_concept_id_2\" , F . lit ( 57 )) # Care_site domain df = df . drop_duplicates ( subset = [ \"fact_id_1\" , \"fact_id_2\" ]) # Only UF-Hospital relationships in i2b2 df = df . withColumn ( \"relationship_concept_id\" , F . lit ( 46233688 )) # Included in elif table == \"concept_relationship\" : data = [] schema = T . StructType ( [ T . StructField ( \"concept_id_1\" , T . StringType (), True ), T . StructField ( \"concept_id_2\" , T . StringType (), True ), T . StructField ( \"relationship_id\" , T . StringType (), True ), ] ) if \"get_additional_i2b2_concept_relationship\" in registry . data . get_all (): data = registry . get ( \"data\" , \"get_additional_i2b2_concept_relationship\" )() df = spark_session . createDataFrame ( data , schema ) . cache () return df mapping_dict mapping_dict ( mapping : Dict [ str , str ], default : str ) -> FunctionUDF Returns a function that maps data according to a mapping dictionnary in a Spark DataFrame. PARAMETER DESCRIPTION mapping Mapping dictionnary TYPE: Dict [ str , str ] default Value to return if the function input is not find in the mapping dictionnary. TYPE: str RETURNS DESCRIPTION Callable Function that maps the values of Spark DataFrame column. Source code in eds_scikit/io/i2b2_mapping.py 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 def mapping_dict ( mapping : Dict [ str , str ], default : str ) -> FunctionUDF : \"\"\" Returns a function that maps data according to a mapping dictionnary in a Spark DataFrame. Parameters ---------- mapping: Dict Mapping dictionnary default: str Value to return if the function input is not find in the mapping dictionnary. Returns ------- Callable Function that maps the values of Spark DataFrame column. \"\"\" def f ( x ): return mapping . get ( x , default ) return F . udf ( f )","title":"i2b2_mapping"},{"location":"reference/io/i2b2_mapping/#eds_scikitioi2b2_mapping","text":"","title":"eds_scikit.io.i2b2_mapping"},{"location":"reference/io/i2b2_mapping/#eds_scikit.io.i2b2_mapping.get_i2b2_table","text":"get_i2b2_table ( spark_session : SparkSession , db_name : str , db_source : str , table : str ) -> SparkDataFrame Convert a Spark table from i2b2 to OMOP format. PARAMETER DESCRIPTION db_name Name of the database where the data is stored. TYPE: str table Name of the table to extract. TYPE: str RETURNS DESCRIPTION df Spark DataFrame extracted from the i2b2 database given and converted to OMOP standard. TYPE: Spark DataFrame Source code in eds_scikit/io/i2b2_mapping.py 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 def get_i2b2_table ( spark_session : SparkSession , db_name : str , db_source : str , table : str ) -> SparkDataFrame : \"\"\" Convert a Spark table from i2b2 to OMOP format. Parameters ---------- db_name: str Name of the database where the data is stored. table: str Name of the table to extract. Returns ------- df: Spark DataFrame Spark DataFrame extracted from the i2b2 database given and converted to OMOP standard. \"\"\" i2b2_table_name = i2b2_tables [ db_source ][ table ] # Dictionary of omop_col -> i2b2_col columns = i2b2_renaming . get ( table ) # Can be None if creating a table from scratch (e.g. concept_relationship if columns is not None : query = f \"describe { db_name } . { i2b2_table_name } \" available_columns = set ( spark_session . sql ( query ) . toPandas () . col_name . tolist ()) if db_source == \"cse\" : columns . pop ( \"i2b2_action\" , None ) cols = \", \" . join ( [ f \" { i2b2 } AS { omop } \" for omop , i2b2 in columns . items () if i2b2 in available_columns ] ) query = f \"SELECT { cols } FROM { db_name } . { i2b2_table_name } \" df = spark_session . sql ( query ) # Special mapping for i2b2 : # CIM10 if table == \"condition_occurrence\" : df = df . withColumn ( \"condition_source_value\" , F . substring ( F . col ( \"condition_source_value\" ), 7 , 20 ), ) # CCAM elif table == \"procedure_occurrence\" : df = df . withColumn ( \"procedure_source_value\" , F . substring ( F . col ( \"procedure_source_value\" ), 6 , 20 ), ) # Visits elif table == \"visit_occurrence\" : df = df . withColumn ( \"visit_source_value\" , mapping_dict ( visit_type_mapping , \"Non Renseign\u00e9\" )( F . col ( \"visit_source_value\" ) ), ) if db_source == \"cse\" : df = df . withColumn ( \"row_status_source_value\" , F . lit ( \"Actif\" )) df = df . withColumn ( \"visit_occurrence_source_value\" , df [ \"visit_occurrence_id\" ] ) else : df = df . withColumn ( \"row_status_source_value\" , F . when ( F . col ( \"row_status_source_value\" ) . isin ([ - 1 , - 2 ]), \"supprim\u00e9\" ) . otherwise ( \"Actif\" ), ) # Retrieve Hospital trigram ufr = spark_session . sql ( f \"SELECT * FROM { db_name } . { i2b2_tables [ db_source ][ 'visit_detail' ] } \" ) ufr = ufr . withColumn ( \"care_site_id\" , F . substring ( F . split ( F . col ( \"concept_cd\" ), \":\" ) . getItem ( 1 ), 1 , 3 ), ) ufr = ufr . withColumnRenamed ( \"encounter_num\" , \"visit_occurrence_id\" ) ufr = ufr . drop_duplicates ( subset = [ \"visit_occurrence_id\" ]) ufr = ufr . select ([ \"visit_occurrence_id\" , \"care_site_id\" ]) df = df . join ( ufr , how = \"inner\" , on = [ \"visit_occurrence_id\" ]) # Patients elif table == \"person\" : df = df . withColumn ( \"gender_source_value\" , mapping_dict ( sex_cd_mapping , \"Non Renseign\u00e9\" )( F . col ( \"gender_source_value\" )), ) # Documents elif table . startswith ( \"note\" ): df = df . withColumn ( \"note_class_source_value\" , F . substring ( F . col ( \"note_class_source_value\" ), 4 , 100 ), ) if db_source == \"cse\" : df = df . withColumn ( \"row_status_source_value\" , F . lit ( \"Actif\" )) else : df = df . withColumn ( \"row_status_source_value\" , F . when ( F . col ( \"row_status_source_value\" ) < 0 , \"SUPP\" ) . otherwise ( \"Actif\" ), ) # Hospital trigrams elif table == \"care_site\" : df = df . withColumn ( \"care_site_type_source_value\" , F . lit ( \"H\u00f4pital\" )) df = df . withColumn ( \"care_site_source_value\" , F . split ( F . col ( \"care_site_source_value\" ), \":\" ) . getItem ( 1 ), ) df = df . withColumn ( \"care_site_id\" , F . substring ( F . col ( \"care_site_source_value\" ), 1 , 3 ) ) df = df . drop_duplicates ( subset = [ \"care_site_id\" ]) df = df . withColumn ( \"care_site_short_name\" , mapping_dict ( dict_code_UFR , \"Non Renseign\u00e9\" )( F . col ( \"care_site_id\" )), ) # UFR elif table == \"visit_detail\" : df = df . withColumn ( \"care_site_id\" , F . split ( F . col ( \"care_site_id\" ), \":\" ) . getItem ( 1 ) ) df = df . withColumn ( \"visit_detail_type_source_value\" , F . lit ( \"PASS\" )) df = df . withColumn ( \"row_status_source_value\" , F . lit ( \"Actif\" )) # measurement elif table == \"measurement\" : df = df . withColumn ( \"measurement_source_concept_id\" , F . substring ( F . col ( \"measurement_source_concept_id\" ), 5 , 20 ), ) . withColumn ( \"row_status_source_value\" , F . lit ( \"Valid\u00e9\" )) # concept elif table == \"concept\" : df = ( df . withColumn ( \"concept_source_value\" , F . substring ( F . col ( \"concept_source_value\" ), 5 , 20 ), # TODO: use regexp_extract to take substring after ':' ) . withColumn ( \"concept_id\" , F . col ( \"concept_source_value\" )) . withColumn ( \"concept_code\" , F . col ( \"concept_id\" )) . withColumn ( \"vocabulary_id\" , F . lit ( \"ANABIO\" )) ) # Adding LOINC if \"get_additional_i2b2_concept\" in registry . data . get_all (): loinc_pd = registry . get ( \"data\" , \"get_additional_i2b2_concept\" )() assert len ( loinc_pd . columns ) == len ( df . columns ) loinc_pd = loinc_pd [ df . columns ] # for columns ordering df = df . union ( spark_session . createDataFrame ( loinc_pd , df . schema , verifySchema = False ) ) . cache () # fact_relationship elif table == \"fact_relationship\" : # Retrieve UF information df = df . withColumn ( \"fact_id_1\" , F . split ( F . col ( \"care_site_source_value\" ), \":\" ) . getItem ( 1 ), ) df = df . withColumn ( \"domain_concept_id_1\" , F . lit ( 57 )) # Care_site domain # Retrieve hospital information df = df . withColumn ( \"fact_id_2\" , F . substring ( F . col ( \"fact_id_1\" ), 1 , 3 )) df = df . withColumn ( \"domain_concept_id_2\" , F . lit ( 57 )) # Care_site domain df = df . drop_duplicates ( subset = [ \"fact_id_1\" , \"fact_id_2\" ]) # Only UF-Hospital relationships in i2b2 df = df . withColumn ( \"relationship_concept_id\" , F . lit ( 46233688 )) # Included in elif table == \"concept_relationship\" : data = [] schema = T . StructType ( [ T . StructField ( \"concept_id_1\" , T . StringType (), True ), T . StructField ( \"concept_id_2\" , T . StringType (), True ), T . StructField ( \"relationship_id\" , T . StringType (), True ), ] ) if \"get_additional_i2b2_concept_relationship\" in registry . data . get_all (): data = registry . get ( \"data\" , \"get_additional_i2b2_concept_relationship\" )() df = spark_session . createDataFrame ( data , schema ) . cache () return df","title":"get_i2b2_table()"},{"location":"reference/io/i2b2_mapping/#eds_scikit.io.i2b2_mapping.mapping_dict","text":"mapping_dict ( mapping : Dict [ str , str ], default : str ) -> FunctionUDF Returns a function that maps data according to a mapping dictionnary in a Spark DataFrame. PARAMETER DESCRIPTION mapping Mapping dictionnary TYPE: Dict [ str , str ] default Value to return if the function input is not find in the mapping dictionnary. TYPE: str RETURNS DESCRIPTION Callable Function that maps the values of Spark DataFrame column. Source code in eds_scikit/io/i2b2_mapping.py 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 def mapping_dict ( mapping : Dict [ str , str ], default : str ) -> FunctionUDF : \"\"\" Returns a function that maps data according to a mapping dictionnary in a Spark DataFrame. Parameters ---------- mapping: Dict Mapping dictionnary default: str Value to return if the function input is not find in the mapping dictionnary. Returns ------- Callable Function that maps the values of Spark DataFrame column. \"\"\" def f ( x ): return mapping . get ( x , default ) return F . udf ( f )","title":"mapping_dict()"},{"location":"reference/io/improve_performance/","text":"eds_scikit.io.improve_performance koalas_options koalas_options () -> None Set necessary options to optimise Koalas Source code in eds_scikit/io/improve_performance.py 30 31 32 33 34 35 36 37 38 39 40 def koalas_options () -> None : \"\"\" Set necessary options to optimise Koalas \"\"\" # Reloading Koalas to use the new configuration ks = load_koalas () ks . set_option ( \"compute.default_index_type\" , \"distributed\" ) ks . set_option ( \"compute.ops_on_diff_frames\" , True ) ks . set_option ( \"display.max_rows\" , 50 ) improve_performances improve_performances ( to_add_conf : List [ Tuple [ str , str ]] = [], quiet_spark : bool = True , app_name : str = '' ) -> Tuple [ SparkSession , SparkContext , SparkSession . sql ] (Re)defines various Spark variable with some configuration changes to improve performances by enabling Arrow This has to be done - Before launching a SparkCOntext - Before importing Koalas Those two points are being taken care on this function. If a SparkSession already exists, it will copy its configuration before creating a new one RETURNS DESCRIPTION Tuple of - A SparkSession - The associated SparkContext - The associated Source code in eds_scikit/io/improve_performance.py 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 def improve_performances ( to_add_conf : List [ Tuple [ str , str ]] = [], quiet_spark : bool = True , app_name : str = \"\" , ) -> Tuple [ SparkSession , SparkContext , SparkSession . sql ]: \"\"\" (Re)defines various Spark variable with some configuration changes to improve performances by enabling Arrow This has to be done - Before launching a SparkCOntext - Before importing Koalas Those two points are being taken care on this function. If a SparkSession already exists, it will copy its configuration before creating a new one Returns ------- Tuple of - A SparkSession - The associated SparkContext - The associated ``sql`` object to run SQL queries \"\"\" # Check if a spark Session is up global spark , sc , sql spark = SparkSession . builder . getOrCreate () sc = spark . sparkContext if quiet_spark : sc . setLogLevel ( \"ERROR\" ) conf = sc . getConf () # Synchronizing TimeZone tz = os . environ . get ( \"TZ\" , \"UTC\" ) os . environ [ \"TZ\" ] = tz time . tzset () to_add_conf . extend ( [ ( \"spark.app.name\" , f \" { os . environ . get ( 'USER' ) } _ { app_name } _scikit\" ), ( \"spark.sql.session.timeZone\" , tz ), ( \"spark.sql.execution.arrow.enabled\" , \"true\" ), ( \"spark.sql.execution.arrow.pyspark.enabled\" , \"true\" ), ] ) for key , value in to_add_conf : conf . set ( key , value ) # Stopping context to add necessary env variables sc . stop () spark . stop () set_env_variables () spark = SparkSession . builder . enableHiveSupport () . config ( conf = conf ) . getOrCreate () sc = spark . sparkContext if quiet_spark : sc . setLogLevel ( \"ERROR\" ) sql = spark . sql koalas_options () return spark , sc , sql","title":"improve_performance"},{"location":"reference/io/improve_performance/#eds_scikitioimprove_performance","text":"","title":"eds_scikit.io.improve_performance"},{"location":"reference/io/improve_performance/#eds_scikit.io.improve_performance.koalas_options","text":"koalas_options () -> None Set necessary options to optimise Koalas Source code in eds_scikit/io/improve_performance.py 30 31 32 33 34 35 36 37 38 39 40 def koalas_options () -> None : \"\"\" Set necessary options to optimise Koalas \"\"\" # Reloading Koalas to use the new configuration ks = load_koalas () ks . set_option ( \"compute.default_index_type\" , \"distributed\" ) ks . set_option ( \"compute.ops_on_diff_frames\" , True ) ks . set_option ( \"display.max_rows\" , 50 )","title":"koalas_options()"},{"location":"reference/io/improve_performance/#eds_scikit.io.improve_performance.improve_performances","text":"improve_performances ( to_add_conf : List [ Tuple [ str , str ]] = [], quiet_spark : bool = True , app_name : str = '' ) -> Tuple [ SparkSession , SparkContext , SparkSession . sql ] (Re)defines various Spark variable with some configuration changes to improve performances by enabling Arrow This has to be done - Before launching a SparkCOntext - Before importing Koalas Those two points are being taken care on this function. If a SparkSession already exists, it will copy its configuration before creating a new one RETURNS DESCRIPTION Tuple of - A SparkSession - The associated SparkContext - The associated Source code in eds_scikit/io/improve_performance.py 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 def improve_performances ( to_add_conf : List [ Tuple [ str , str ]] = [], quiet_spark : bool = True , app_name : str = \"\" , ) -> Tuple [ SparkSession , SparkContext , SparkSession . sql ]: \"\"\" (Re)defines various Spark variable with some configuration changes to improve performances by enabling Arrow This has to be done - Before launching a SparkCOntext - Before importing Koalas Those two points are being taken care on this function. If a SparkSession already exists, it will copy its configuration before creating a new one Returns ------- Tuple of - A SparkSession - The associated SparkContext - The associated ``sql`` object to run SQL queries \"\"\" # Check if a spark Session is up global spark , sc , sql spark = SparkSession . builder . getOrCreate () sc = spark . sparkContext if quiet_spark : sc . setLogLevel ( \"ERROR\" ) conf = sc . getConf () # Synchronizing TimeZone tz = os . environ . get ( \"TZ\" , \"UTC\" ) os . environ [ \"TZ\" ] = tz time . tzset () to_add_conf . extend ( [ ( \"spark.app.name\" , f \" { os . environ . get ( 'USER' ) } _ { app_name } _scikit\" ), ( \"spark.sql.session.timeZone\" , tz ), ( \"spark.sql.execution.arrow.enabled\" , \"true\" ), ( \"spark.sql.execution.arrow.pyspark.enabled\" , \"true\" ), ] ) for key , value in to_add_conf : conf . set ( key , value ) # Stopping context to add necessary env variables sc . stop () spark . stop () set_env_variables () spark = SparkSession . builder . enableHiveSupport () . config ( conf = conf ) . getOrCreate () sc = spark . sparkContext if quiet_spark : sc . setLogLevel ( \"ERROR\" ) sql = spark . sql koalas_options () return spark , sc , sql","title":"improve_performances()"},{"location":"reference/io/postgres/","text":"eds_scikit.io.postgres PostgresData PostgresData ( dbname : Optional [ str ] = None , schema : Optional [ str ] = None , user : Optional [ str ] = None , host : Optional [ str ] = None , port : Optional [ int ] = None ) Bases: BaseData Source code in eds_scikit/io/postgres.py 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 def __init__ ( self , dbname : Optional [ str ] = None , schema : Optional [ str ] = None , user : Optional [ str ] = None , host : Optional [ str ] = None , port : Optional [ int ] = None , ): ( self . host , self . port , self . dbname , self . user , ) = self . _find_matching_pgpass_params ( host , port , dbname , user ) self . schema = schema read_sql read_sql ( sql_query : str , ** kwargs ) -> pd . DataFrame Execute pandas.read_sql() on the database. PARAMETER DESCRIPTION sql_query SQL query (postgres flavor) TYPE: str **kwargs additional arguments passed to pandas.read_sql() DEFAULT: {} RETURNS DESCRIPTION df TYPE: pandas . DataFrame Source code in eds_scikit/io/postgres.py 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 def read_sql ( self , sql_query : str , ** kwargs ) -> pd . DataFrame : \"\"\"Execute pandas.read_sql() on the database. Parameters ---------- sql_query : str SQL query (postgres flavor) **kwargs additional arguments passed to pandas.read_sql() Returns ------- df : pandas.DataFrame \"\"\" connection_infos = { param : getattr ( self , param ) for param in [ \"host\" , \"port\" , \"dbname\" , \"user\" ] } connection_infos [ \"password\" ] = pgpasslib . getpass ( ** connection_infos ) connection = pg . connect ( ** connection_infos ) if self . schema : connection . cursor () . execute ( f \"SET SCHEMA ' { self . schema } '\" ) df = pd . read_sql ( sql_query , con = connection , ** kwargs ) connection . close () return df","title":"postgres"},{"location":"reference/io/postgres/#eds_scikitiopostgres","text":"","title":"eds_scikit.io.postgres"},{"location":"reference/io/postgres/#eds_scikit.io.postgres.PostgresData","text":"PostgresData ( dbname : Optional [ str ] = None , schema : Optional [ str ] = None , user : Optional [ str ] = None , host : Optional [ str ] = None , port : Optional [ int ] = None ) Bases: BaseData Source code in eds_scikit/io/postgres.py 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 def __init__ ( self , dbname : Optional [ str ] = None , schema : Optional [ str ] = None , user : Optional [ str ] = None , host : Optional [ str ] = None , port : Optional [ int ] = None , ): ( self . host , self . port , self . dbname , self . user , ) = self . _find_matching_pgpass_params ( host , port , dbname , user ) self . schema = schema","title":"PostgresData"},{"location":"reference/io/postgres/#eds_scikit.io.postgres.PostgresData.read_sql","text":"read_sql ( sql_query : str , ** kwargs ) -> pd . DataFrame Execute pandas.read_sql() on the database. PARAMETER DESCRIPTION sql_query SQL query (postgres flavor) TYPE: str **kwargs additional arguments passed to pandas.read_sql() DEFAULT: {} RETURNS DESCRIPTION df TYPE: pandas . DataFrame Source code in eds_scikit/io/postgres.py 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 def read_sql ( self , sql_query : str , ** kwargs ) -> pd . DataFrame : \"\"\"Execute pandas.read_sql() on the database. Parameters ---------- sql_query : str SQL query (postgres flavor) **kwargs additional arguments passed to pandas.read_sql() Returns ------- df : pandas.DataFrame \"\"\" connection_infos = { param : getattr ( self , param ) for param in [ \"host\" , \"port\" , \"dbname\" , \"user\" ] } connection_infos [ \"password\" ] = pgpasslib . getpass ( ** connection_infos ) connection = pg . connect ( ** connection_infos ) if self . schema : connection . cursor () . execute ( f \"SET SCHEMA ' { self . schema } '\" ) df = pd . read_sql ( sql_query , con = connection , ** kwargs ) connection . close () return df","title":"read_sql()"},{"location":"reference/io/settings/","text":"eds_scikit.io.settings default_tables_to_save module-attribute default_tables_to_save = [ 'person' , 'visit_occurrence' , 'visit_detail' , 'condition_occurrence' , 'procedure_occurrence' , 'care_site' , 'concept' ] The default tables loaded when instanciating a HiveData or a PostgresData tables_to_load module-attribute tables_to_load = { 'person' : [ 'person_id' , 'location_id' , 'year_of_birth' , 'month_of_birth' , 'day_of_birth' , 'birth_datetime' , 'death_datetime' , 'gender_source_value' , 'gender_source_concept_id' , 'cdm_source' ], 'visit_occurrence' : [ 'visit_occurrence_id' , 'person_id' , 'visit_occurrence_source_value' , 'preceding_visit_occurrence_id' , 'care_site_id' , 'visit_start_datetime' , 'visit_end_datetime' , 'visit_source_value' , 'visit_source_concept_id' , 'visit_type_source_value' , 'visit_type_source_concept_id' , 'admitted_from_source_value' , 'admitted_from_source_concept_id' , 'discharge_to_source_value' , 'discharge_to_source_concept_id' , 'row_status_source_value' , 'stay_source_value' , 'stay_source_concept_id' , 'cdm_source' ], 'care_site' : [ 'care_site_id' , 'care_site_source_value' , 'care_site_name' , 'care_site_short_name' , 'place_of_service_source_value' , 'care_site_type_source_value' , 'valid_start_date' , 'valid_end_date' ], 'visit_detail' : [ 'visit_detail_id' , 'visit_occurrence_id' , 'person_id' , 'preceding_visit_detail_id' , 'visit_detail_parent_id' , 'care_site_id' , 'visit_detail_start_date' , 'visit_detail_start_datetime' , 'visit_detail_end_date' , 'visit_detail_end_datetime' , 'visit_detail_source_value' , 'visit_detail_source_concept_id' , 'visit_detail_type_source_value' , 'visit_detail_type_source_concept_id' , 'admitted_from_source_value' , 'admitted_from_source_concept_id' , 'discharge_to_source_value' , 'discharge_to_source_concept_id' , 'cdm_source' ], 'condition_occurrence' : [ 'condition_occurrence_id' , 'person_id' , 'visit_occurrence_id' , 'visit_detail_id' , 'condition_start_datetime' , 'condition_source_value' , 'condition_source_concept_id' , 'condition_status_source_value' , 'condition_status_source_concept_id' , 'cdm_source' ], 'procedure_occurrence' : [ 'procedure_occurrence_id' , 'person_id' , 'visit_occurrence_id' , 'visit_detail_id' , 'procedure_datetime' , 'procedure_source_value' , 'procedure_source_concept_id' , 'cdm_source' ], 'concept' : [ 'concept_id' , 'concept_name' , 'domain_id' , 'vocabulary_id' , 'concept_class_id' , 'standard_concept' , 'concept_code' , 'valid_start_date' , 'valid_end_date' , 'invalid_reason' ]} The default columns loaded when instanciating a HiveData or a PostgresData measurement_config module-attribute measurement_config = dict ( standard_terminologies = [ 'LOINC' , 'AnaBio' , 'ANABIO' , 'ANALYSES_LABORATOIRE' ], standard_concept_regex = { 'LOINC' : '[0-9]{2,5}[-][0-9]' , 'AnaBio' : '[A-Z][0-9] {4} ' , 'ANABIO' : '[A-Z][0-9] {4} ' }, source_terminologies = { 'ANALYSES_LABORATOIRE' : 'Analyses Laboratoire' , 'GLIMS_ANABIO' : 'GLIMS.{0,20}Anabio' , 'GLIMS_LOINC' : 'GLIMS.{0,20}LOINC' , 'ITM_ANABIO' : 'ITM - ANABIO' , 'ITM_LOINC' : 'ITM - LOINC' }, mapping = [( 'ANALYSES_LABORATOIRE' , 'GLIMS_ANABIO' , 'Maps to' ), ( 'ANALYSES_LABORATOIRE' , 'GLIMS_LOINC' , 'Maps to' ), ( 'GLIMS_ANABIO' , 'ITM_ANABIO' , 'Mapped from' ), ( 'ITM_ANABIO' , 'ITM_LOINC' , 'Maps to' )]) AP-HP specific configuration. ITM and GLIMS do not share the same ANABIO-to-LOINC mapping. ITM referential is more reliable but covers less ANABIO codes the GLIMS referential.","title":"settings"},{"location":"reference/io/settings/#eds_scikitiosettings","text":"","title":"eds_scikit.io.settings"},{"location":"reference/io/settings/#eds_scikit.io.settings.default_tables_to_save","text":"default_tables_to_save = [ 'person' , 'visit_occurrence' , 'visit_detail' , 'condition_occurrence' , 'procedure_occurrence' , 'care_site' , 'concept' ] The default tables loaded when instanciating a HiveData or a PostgresData","title":"default_tables_to_save"},{"location":"reference/io/settings/#eds_scikit.io.settings.tables_to_load","text":"tables_to_load = { 'person' : [ 'person_id' , 'location_id' , 'year_of_birth' , 'month_of_birth' , 'day_of_birth' , 'birth_datetime' , 'death_datetime' , 'gender_source_value' , 'gender_source_concept_id' , 'cdm_source' ], 'visit_occurrence' : [ 'visit_occurrence_id' , 'person_id' , 'visit_occurrence_source_value' , 'preceding_visit_occurrence_id' , 'care_site_id' , 'visit_start_datetime' , 'visit_end_datetime' , 'visit_source_value' , 'visit_source_concept_id' , 'visit_type_source_value' , 'visit_type_source_concept_id' , 'admitted_from_source_value' , 'admitted_from_source_concept_id' , 'discharge_to_source_value' , 'discharge_to_source_concept_id' , 'row_status_source_value' , 'stay_source_value' , 'stay_source_concept_id' , 'cdm_source' ], 'care_site' : [ 'care_site_id' , 'care_site_source_value' , 'care_site_name' , 'care_site_short_name' , 'place_of_service_source_value' , 'care_site_type_source_value' , 'valid_start_date' , 'valid_end_date' ], 'visit_detail' : [ 'visit_detail_id' , 'visit_occurrence_id' , 'person_id' , 'preceding_visit_detail_id' , 'visit_detail_parent_id' , 'care_site_id' , 'visit_detail_start_date' , 'visit_detail_start_datetime' , 'visit_detail_end_date' , 'visit_detail_end_datetime' , 'visit_detail_source_value' , 'visit_detail_source_concept_id' , 'visit_detail_type_source_value' , 'visit_detail_type_source_concept_id' , 'admitted_from_source_value' , 'admitted_from_source_concept_id' , 'discharge_to_source_value' , 'discharge_to_source_concept_id' , 'cdm_source' ], 'condition_occurrence' : [ 'condition_occurrence_id' , 'person_id' , 'visit_occurrence_id' , 'visit_detail_id' , 'condition_start_datetime' , 'condition_source_value' , 'condition_source_concept_id' , 'condition_status_source_value' , 'condition_status_source_concept_id' , 'cdm_source' ], 'procedure_occurrence' : [ 'procedure_occurrence_id' , 'person_id' , 'visit_occurrence_id' , 'visit_detail_id' , 'procedure_datetime' , 'procedure_source_value' , 'procedure_source_concept_id' , 'cdm_source' ], 'concept' : [ 'concept_id' , 'concept_name' , 'domain_id' , 'vocabulary_id' , 'concept_class_id' , 'standard_concept' , 'concept_code' , 'valid_start_date' , 'valid_end_date' , 'invalid_reason' ]} The default columns loaded when instanciating a HiveData or a PostgresData","title":"tables_to_load"},{"location":"reference/io/settings/#eds_scikit.io.settings.measurement_config","text":"measurement_config = dict ( standard_terminologies = [ 'LOINC' , 'AnaBio' , 'ANABIO' , 'ANALYSES_LABORATOIRE' ], standard_concept_regex = { 'LOINC' : '[0-9]{2,5}[-][0-9]' , 'AnaBio' : '[A-Z][0-9] {4} ' , 'ANABIO' : '[A-Z][0-9] {4} ' }, source_terminologies = { 'ANALYSES_LABORATOIRE' : 'Analyses Laboratoire' , 'GLIMS_ANABIO' : 'GLIMS.{0,20}Anabio' , 'GLIMS_LOINC' : 'GLIMS.{0,20}LOINC' , 'ITM_ANABIO' : 'ITM - ANABIO' , 'ITM_LOINC' : 'ITM - LOINC' }, mapping = [( 'ANALYSES_LABORATOIRE' , 'GLIMS_ANABIO' , 'Maps to' ), ( 'ANALYSES_LABORATOIRE' , 'GLIMS_LOINC' , 'Maps to' ), ( 'GLIMS_ANABIO' , 'ITM_ANABIO' , 'Mapped from' ), ( 'ITM_ANABIO' , 'ITM_LOINC' , 'Maps to' )]) AP-HP specific configuration. ITM and GLIMS do not share the same ANABIO-to-LOINC mapping. ITM referential is more reliable but covers less ANABIO codes the GLIMS referential.","title":"measurement_config"},{"location":"reference/io/table_viz_default_config/","text":"eds_scikit.io.table_viz_default_config","title":"table_viz_default_config"},{"location":"reference/io/table_viz_default_config/#eds_scikitiotable_viz_default_config","text":"","title":"eds_scikit.io.table_viz_default_config"},{"location":"reference/period/","text":"eds_scikit.period tagging tagging ( tag_to_df : DataFrame , tag_from_df : DataFrame , concept_to_tag : str , tag_to_date_cols : List [ str ] = [ 't_start' , 't_end' ], tag_from_date_cols : List [ str ] = [ 't_start' , 't_end' ], algo : str = 'intersection' ) -> DataFrame PARAMETER DESCRIPTION tag_to_df TYPE: DataFrame tag_from_df TYPE: DataFrame concept_to_tag TYPE: str tag_to_date_cols TYPE: List [ str ], optional DEFAULT: ['t_start', 't_end'] tag_from_date_cols TYPE: List [ str ], optional DEFAULT: ['t_start', 't_end'] algo TYPE: str , optional DEFAULT: 'intersection' RETURNS DESCRIPTION DataFrame Source code in eds_scikit/period/tagging_functions.py 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 def tagging ( tag_to_df : DataFrame , tag_from_df : DataFrame , concept_to_tag : str , tag_to_date_cols : List [ str ] = [ \"t_start\" , \"t_end\" ], tag_from_date_cols : List [ str ] = [ \"t_start\" , \"t_end\" ], algo : str = \"intersection\" , ) -> DataFrame : \"\"\" Parameters ---------- tag_to_df : DataFrame tag_from_df : DataFrame concept_to_tag : str tag_to_date_cols : List[str], optional tag_from_date_cols : List[str], optional algo : str, optional Returns ------- DataFrame \"\"\" framework = get_framework ( tag_to_df ) tag_to_df = tag_to_df . assign ( event_id = tag_to_df . index ) tag_from = tag_from_df . loc [ tag_from_df . concept == concept_to_tag , [ \"person_id\" , \"value\" ] + [ \"t_start\" , \"t_end\" ], ] tmp = ( tag_to_df . rename ( columns = { tag_to_date_cols [ 0 ]: \"t_start_x\" , tag_to_date_cols [ 1 ]: \"t_end_x\" } ) . merge ( tag_from . rename ( columns = { tag_from_date_cols [ 0 ]: \"t_start_y\" , tag_from_date_cols [ 1 ]: \"t_end_y\" , } ), on = \"person_id\" , how = \"left\" , ) . dropna ( subset = [ \"t_start_x\" , \"t_end_x\" , \"t_start_y\" , \"t_end_y\" ]) ) if len ( tmp ) == 0 : # TODO: is this necessary ? logger . warning ( \"No matching were found between the 2 DataFrames\" ) return framework . DataFrame ( columns = [ \"person_id\" , \"t_start\" , \"t_end\" , \"concept\" , \"value\" ] ) tmp [ \"tag\" ] = compare_intervals ( tmp [ \"t_start_x\" ], tmp [ \"t_end_x\" ], tmp [ \"t_start_y\" ], tmp [ \"t_end_y\" ], algo = algo , ) value_col = ( \"value_y\" if (( \"value\" in tag_to_df . columns ) and ( \"value\" in tag_from_df . columns )) else \"value\" ) tags = ( tmp . groupby ([ \"event_id\" , value_col ]) . tag . any () . unstack () . fillna ( False ) . reset_index () ) tags = tag_to_df [[ \"event_id\" ]] . merge ( tags , on = \"event_id\" , how = \"left\" ) . fillna ( False ) tags = tag_to_df . merge ( tags , on = \"event_id\" , how = \"left\" ) . drop ( columns = \"event_id\" ) return tags","title":"`eds_scikit.period`"},{"location":"reference/period/#eds_scikitperiod","text":"","title":"eds_scikit.period"},{"location":"reference/period/#eds_scikit.period.tagging","text":"tagging ( tag_to_df : DataFrame , tag_from_df : DataFrame , concept_to_tag : str , tag_to_date_cols : List [ str ] = [ 't_start' , 't_end' ], tag_from_date_cols : List [ str ] = [ 't_start' , 't_end' ], algo : str = 'intersection' ) -> DataFrame PARAMETER DESCRIPTION tag_to_df TYPE: DataFrame tag_from_df TYPE: DataFrame concept_to_tag TYPE: str tag_to_date_cols TYPE: List [ str ], optional DEFAULT: ['t_start', 't_end'] tag_from_date_cols TYPE: List [ str ], optional DEFAULT: ['t_start', 't_end'] algo TYPE: str , optional DEFAULT: 'intersection' RETURNS DESCRIPTION DataFrame Source code in eds_scikit/period/tagging_functions.py 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 def tagging ( tag_to_df : DataFrame , tag_from_df : DataFrame , concept_to_tag : str , tag_to_date_cols : List [ str ] = [ \"t_start\" , \"t_end\" ], tag_from_date_cols : List [ str ] = [ \"t_start\" , \"t_end\" ], algo : str = \"intersection\" , ) -> DataFrame : \"\"\" Parameters ---------- tag_to_df : DataFrame tag_from_df : DataFrame concept_to_tag : str tag_to_date_cols : List[str], optional tag_from_date_cols : List[str], optional algo : str, optional Returns ------- DataFrame \"\"\" framework = get_framework ( tag_to_df ) tag_to_df = tag_to_df . assign ( event_id = tag_to_df . index ) tag_from = tag_from_df . loc [ tag_from_df . concept == concept_to_tag , [ \"person_id\" , \"value\" ] + [ \"t_start\" , \"t_end\" ], ] tmp = ( tag_to_df . rename ( columns = { tag_to_date_cols [ 0 ]: \"t_start_x\" , tag_to_date_cols [ 1 ]: \"t_end_x\" } ) . merge ( tag_from . rename ( columns = { tag_from_date_cols [ 0 ]: \"t_start_y\" , tag_from_date_cols [ 1 ]: \"t_end_y\" , } ), on = \"person_id\" , how = \"left\" , ) . dropna ( subset = [ \"t_start_x\" , \"t_end_x\" , \"t_start_y\" , \"t_end_y\" ]) ) if len ( tmp ) == 0 : # TODO: is this necessary ? logger . warning ( \"No matching were found between the 2 DataFrames\" ) return framework . DataFrame ( columns = [ \"person_id\" , \"t_start\" , \"t_end\" , \"concept\" , \"value\" ] ) tmp [ \"tag\" ] = compare_intervals ( tmp [ \"t_start_x\" ], tmp [ \"t_end_x\" ], tmp [ \"t_start_y\" ], tmp [ \"t_end_y\" ], algo = algo , ) value_col = ( \"value_y\" if (( \"value\" in tag_to_df . columns ) and ( \"value\" in tag_from_df . columns )) else \"value\" ) tags = ( tmp . groupby ([ \"event_id\" , value_col ]) . tag . any () . unstack () . fillna ( False ) . reset_index () ) tags = tag_to_df [[ \"event_id\" ]] . merge ( tags , on = \"event_id\" , how = \"left\" ) . fillna ( False ) tags = tag_to_df . merge ( tags , on = \"event_id\" , how = \"left\" ) . drop ( columns = \"event_id\" ) return tags","title":"tagging()"},{"location":"reference/period/stays/","text":"eds_scikit.period.stays cleaning cleaning ( vo , long_stay_threshold : timedelta , long_stay_filtering : Union [ str , None ], remove_deleted_visits : bool , open_stay_end_datetime : datetime ) -> Tuple [ DataFrame , DataFrame ] Preprocessing of visits before merging them in stays. The function will split the input vo DataFrame into 2, one that should undergo the merging procedure, and one that shouldn't. Depending on the input parameters, 3 type of visits can be prevented to undergo the merging procedure: Too long visits Too long AND unclosed visits Removed visits See the merge_visits() function for details of the parameters Source code in eds_scikit/period/stays.py 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 def cleaning ( vo , long_stay_threshold : timedelta , long_stay_filtering : Union [ str , None ], remove_deleted_visits : bool , open_stay_end_datetime : datetime , ) -> Tuple [ DataFrame , DataFrame ]: \"\"\" Preprocessing of visits before merging them in stays. The function will split the input `vo` DataFrame into 2, one that should undergo the merging procedure, and one that shouldn't. Depending on the input parameters, 3 type of visits can be prevented to undergo the merging procedure: - Too long visits - Too long AND unclosed visits - Removed visits See the [merge_visits()][eds_scikit.period.stays.merge_visits] function for details of the parameters \"\"\" LONG_STAY_FILTERING_VALUES = [ \"all\" , \"open\" , None ] DELETED_ROW_VALUE = \"supprim\u00e9\" if long_stay_filtering not in LONG_STAY_FILTERING_VALUES : raise ValueError ( f \"\"\"Unknown value for `long_stay_filtering`. Accepted values are { LONG_STAY_FILTERING_VALUES } \"\"\" ) if remove_deleted_visits : deleted_visit_mask = vo [ \"row_status_source_value\" ] == DELETED_ROW_VALUE no_starting_date_mask = vo [ \"visit_start_datetime\" ] . isna () no_ending_date_mask = vo [ \"visit_end_datetime\" ] . isna () vo [ \"visit_end_datetime_calc\" ] = open_stay_end_datetime # Cannot use fillna() with datetime in Koalas vo [ \"visit_end_datetime_calc\" ] = vo [ \"visit_end_datetime\" ] . combine_first ( vo [ \"visit_end_datetime_calc\" ] ) too_long_stays_mask = ( substract_datetime ( vo [ \"visit_end_datetime_calc\" ], vo [ \"visit_start_datetime\" ]) >= long_stay_threshold . total_seconds () ) mask = no_starting_date_mask if long_stay_filtering == \"all\" : mask = mask | too_long_stays_mask elif long_stay_filtering == \"open\" : mask = mask | ( too_long_stays_mask & no_ending_date_mask ) if remove_deleted_visits : mask = ( mask ) | deleted_visit_mask return vo [ ~ mask ], vo [ mask ] merge_visits merge_visits ( vo : DataFrame , remove_deleted_visits : bool = True , long_stay_threshold : timedelta = timedelta ( days = 365 ), long_stay_filtering : Optional [ str ] = 'all' , open_stay_end_datetime : Optional [ datetime ] = None , max_timedelta : timedelta = timedelta ( days = 2 ), merge_different_hospitals : bool = False , merge_different_source_values : Union [ bool , List [ str ]] = [ 'hospitalis\u00e9s' , 'urgence' ]) -> DataFrame Merge \"close\" visit occurrences to consider them as a single stay by adding a STAY_ID and CONTIGUOUS_STAY_ID columns to the DataFrame. The value of these columns will be the visit_occurrence_id of the first (meaning the oldest) visit of the stay. From a temporal point of view, we consider that two visits belong to the same stay if either: They intersect The time difference between the end of the most recent and the start of the oldest is lower than max_timedelta (for STAY_ID ) or 0 (for CONTIGUOUS_STAY_ID ) Additionally, other parameters are available to further adjust the merging rules. See below. PARAMETER DESCRIPTION vo The visit_occurrence DataFrame, with at least the following columns: - visit_occurrence_id - person_id - visit_start_datetime_calc (from preprocessing) - visit_end_datetime (from preprocessing) Depending on the input parameters, additional columns may be required: - care_site_id (if merge_different_hospitals == True ) - visit_source_value (if merge_different_source_values != False ) - row_status_source_value (if remove_deleted_visits= True ) TYPE: DataFrame remove_deleted_visits Wether to remove deleted visits from the merging procedure. Deleted visits are extracted via the row_status_source_value column TYPE: bool DEFAULT: True long_stay_filtering Filtering method for long and/or non-closed visits. First of all, visits with no starting date won't be merged with any other visit, and visits with no ending date will have a temporary \"theoretical\" ending date set by datetime.now() . That being said, some visits are sometimes years long because they weren't closed at time. If other visits occurred during this timespan, they could be all merged into the same stay. To avoid this issue, filtering can be done depending on the long_stay_filtering value: all : All long stays (closed and open) are removed from the merging procedure open : Only long open stays are removed from the merging procedure None : No filtering is done on long visits Long stays are determined by the long_stay_threshold value. TYPE: Optional [ str ] DEFAULT: 'all' long_stay_threshold Minimum visit duration value to consider a visit as candidate for \"long visits filtering\" TYPE: timedelta DEFAULT: timedelta(days=365) open_stay_end_datetime Datetime to use in order to fill the visit_end_datetime of open visits. This is necessary in order to compute stay duration and to filter long stays. If not provided datetime.now() will be used. You might provide the extraction date of your data here. TYPE: Optional [ datetime ] DEFAULT: None max_timedelta Maximum time difference between the end of a visit and the start of another to consider them as belonging to the same stay. This duration is internally converted in seconds before comparing. Thus, if you want e.g. to merge visits happening in two consecutive days, you should use timedelta(days=2) and NOT timedelta(days=1) in order to take into account extreme cases such as an first visit ending on Monday at 00h01 AM and another one starting at 23h59 PM on Tuesday TYPE: timedelta DEFAULT: timedelta(days=2) merge_different_hospitals Wether to allow visits occurring in different hospitals to be merged into a same stay TYPE: bool DEFAULT: False merge_different_source_values Wether to allow visits with different visit_source_value to be merged into a same stay. Values can be: True : the visit_source_value isn't taken into account for the merging False : only visits with the same visit_source_value can be merged into a same stay List[str] : only visits which visit_source_value is in the provided list can be merged together. Warning : You should avoid merging visits where visit_source_value == \"hospitalisation incompl\u00e8te\" , because those stays are often never closed. TYPE: Union [ bool , List [ str ]] DEFAULT: ['hospitalis\u00e9s', 'urgence'] RETURNS DESCRIPTION vo Visit occurrence DataFrame with additional STAY_ID column TYPE: DataFrame Examples: >>> import pandas as pd >>> from datetime import datetime , timedelta >>> data = { 1 : ['A', 999, datetime(2021,1,1), datetime(2021,1,5), 'hospitalis\u00e9s'], 2 : ['B', 999, datetime(2021,1,4), datetime(2021,1,8), 'hospitalis\u00e9s'], 3 : ['C', 999, datetime(2021,1,12), datetime(2021,1,18), 'hospitalis\u00e9s'], 4 : ['D', 999, datetime(2021,1,13), datetime(2021,1,14), 'urgence'], 5 : ['E', 999, datetime(2021,1,19), datetime(2021,1,21), 'hospitalis\u00e9s'], 6 : ['F', 999, datetime(2021,1,25), datetime(2021,1,27), 'hospitalis\u00e9s'], 7 : ['G', 999, datetime(2017,1,1), None, \"hospitalis\u00e9s\"] } >>> vo = pd . DataFrame . from_dict ( data, orient=\"index\", columns=[ \"visit_occurrence_id\", \"person_id\", \"visit_start_datetime\", \"visit_end_datetime\", \"visit_source_value\", ], ) >>> vo visit_occurrence_id person_id visit_start_datetime visit_end_datetime visit_source_value 1 A 999 2021-01-01 2021-01-05 hospitalis\u00e9s 2 B 999 2021-01-04 2021-01-08 hospitalis\u00e9s 3 C 999 2021-01-12 2021-01-18 hospitalis\u00e9s 4 D 999 2021-01-13 2021-01-14 urgence 5 E 999 2021-01-19 2021-01-21 hospitalis\u00e9s 6 F 999 2021-01-25 2021-01-27 hospitalis\u00e9s 7 G 999 2017-01-01 NaT hospitalis\u00e9s >>> vo = merge_visits ( vo, remove_deleted_visits=True, long_stay_threshold=timedelta(days=365), long_stay_filtering=\"all\", max_timedelta=timedelta(hours=24), merge_different_hospitals=False, merge_different_source_values=[\"hospitalis\u00e9s\", \"urgence\"], ) >>> vo visit_occurrence_id person_id visit_start_datetime visit_end_datetime visit_source_value STAY_ID CONTIGUOUS_STAY_ID 1 A 999 2021-01-01 2021-01-05 hospitalis\u00e9s A A 2 B 999 2021-01-04 2021-01-08 hospitalis\u00e9s A A 3 C 999 2021-01-12 2021-01-18 hospitalis\u00e9s C C 4 D 999 2021-01-13 2021-01-14 urgence C C 5 E 999 2021-01-19 2021-01-21 hospitalis\u00e9s C E 6 F 999 2021-01-25 2021-01-27 hospitalis\u00e9s F F 7 G 999 2017-01-01 NaT hospitalis\u00e9s G G Source code in eds_scikit/period/stays.py 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 @concept_checker ( concepts = [ \"STAY_ID\" , \"CONTIGUOUS_STAY_ID\" ]) def merge_visits ( vo : DataFrame , remove_deleted_visits : bool = True , long_stay_threshold : timedelta = timedelta ( days = 365 ), long_stay_filtering : Optional [ str ] = \"all\" , open_stay_end_datetime : Optional [ datetime ] = None , max_timedelta : timedelta = timedelta ( days = 2 ), merge_different_hospitals : bool = False , merge_different_source_values : Union [ bool , List [ str ]] = [ \"hospitalis\u00e9s\" , \"urgence\" ], ) -> DataFrame : \"\"\" Merge \"close\" visit occurrences to consider them as a single stay by adding a ``STAY_ID`` and ``CONTIGUOUS_STAY_ID`` columns to the DataFrame. The value of these columns will be the `visit_occurrence_id` of the first (meaning the oldest) visit of the stay. From a temporal point of view, we consider that two visits belong to the same stay if either: - They intersect - The time difference between the end of the most recent and the start of the oldest is lower than ``max_timedelta`` (for ``STAY_ID``) or 0 (for ``CONTIGUOUS_STAY_ID``) Additionally, other parameters are available to further adjust the merging rules. See below. Parameters ---------- vo : DataFrame The ``visit_occurrence`` DataFrame, with at least the following columns: - visit_occurrence_id - person_id - visit_start_datetime_calc (from preprocessing) - visit_end_datetime (from preprocessing) Depending on the input parameters, additional columns may be required: - care_site_id (if ``merge_different_hospitals == True``) - visit_source_value (if ``merge_different_source_values != False``) - row_status_source_value (if ``remove_deleted_visits= True``) remove_deleted_visits: bool Wether to remove deleted visits from the merging procedure. Deleted visits are extracted via the `row_status_source_value` column long_stay_filtering : Optional[str] Filtering method for long and/or non-closed visits. First of all, visits with no starting date won't be merged with any other visit, and visits with no ending date will have a temporary \"theoretical\" ending date set by ``datetime.now()``. That being said, some visits are sometimes years long because they weren't closed at time. If other visits occurred during this timespan, they could be all merged into the same stay. To avoid this issue, filtering can be done depending on the ``long_stay_filtering`` value: - ``all``: All long stays (closed and open) are removed from the merging procedure - ``open``: Only long open stays are removed from the merging procedure - ``None``: No filtering is done on long visits Long stays are determined by the ``long_stay_threshold`` value. long_stay_threshold : timedelta Minimum visit duration value to consider a visit as candidate for \"long visits filtering\" open_stay_end_datetime: Optional[datetime] Datetime to use in order to fill the `visit_end_datetime` of open visits. This is necessary in order to compute stay duration and to filter long stays. If not provided `datetime.now()` will be used. You might provide the extraction date of your data here. max_timedelta : timedelta Maximum time difference between the end of a visit and the start of another to consider them as belonging to the same stay. This duration is internally converted in seconds before comparing. Thus, if you want e.g. to merge visits happening in two consecutive days, you should use `timedelta(days=2)` and NOT `timedelta(days=1)` in order to take into account extreme cases such as an first visit ending on Monday at 00h01 AM and another one starting at 23h59 PM on Tuesday merge_different_hospitals : bool Wether to allow visits occurring in different hospitals to be merged into a same stay merge_different_source_values : Union[bool, List[str]] Wether to allow visits with different `visit_source_value` to be merged into a same stay. Values can be: - `True`: the `visit_source_value` isn't taken into account for the merging - `False`: only visits with the same `visit_source_value` can be merged into a same stay - `List[str]`: only visits which `visit_source_value` is in the provided list can be merged together. **Warning**: You should avoid merging visits where `visit_source_value == \"hospitalisation incompl\u00e8te\"`, because those stays are often never closed. Returns ------- vo : DataFrame Visit occurrence DataFrame with additional `STAY_ID` column Examples -------- >>> import pandas as pd >>> from datetime import datetime, timedelta >>> data = { 1 : ['A', 999, datetime(2021,1,1), datetime(2021,1,5), 'hospitalis\u00e9s'], 2 : ['B', 999, datetime(2021,1,4), datetime(2021,1,8), 'hospitalis\u00e9s'], 3 : ['C', 999, datetime(2021,1,12), datetime(2021,1,18), 'hospitalis\u00e9s'], 4 : ['D', 999, datetime(2021,1,13), datetime(2021,1,14), 'urgence'], 5 : ['E', 999, datetime(2021,1,19), datetime(2021,1,21), 'hospitalis\u00e9s'], 6 : ['F', 999, datetime(2021,1,25), datetime(2021,1,27), 'hospitalis\u00e9s'], 7 : ['G', 999, datetime(2017,1,1), None, \"hospitalis\u00e9s\"] } >>> vo = pd.DataFrame.from_dict( data, orient=\"index\", columns=[ \"visit_occurrence_id\", \"person_id\", \"visit_start_datetime\", \"visit_end_datetime\", \"visit_source_value\", ], ) >>> vo visit_occurrence_id person_id visit_start_datetime visit_end_datetime visit_source_value 1 A 999 2021-01-01 2021-01-05 hospitalis\u00e9s 2 B 999 2021-01-04 2021-01-08 hospitalis\u00e9s 3 C 999 2021-01-12 2021-01-18 hospitalis\u00e9s 4 D 999 2021-01-13 2021-01-14 urgence 5 E 999 2021-01-19 2021-01-21 hospitalis\u00e9s 6 F 999 2021-01-25 2021-01-27 hospitalis\u00e9s 7 G 999 2017-01-01 NaT hospitalis\u00e9s >>> vo = merge_visits( vo, remove_deleted_visits=True, long_stay_threshold=timedelta(days=365), long_stay_filtering=\"all\", max_timedelta=timedelta(hours=24), merge_different_hospitals=False, merge_different_source_values=[\"hospitalis\u00e9s\", \"urgence\"], ) >>> vo visit_occurrence_id person_id visit_start_datetime visit_end_datetime visit_source_value STAY_ID CONTIGUOUS_STAY_ID 1 A 999 2021-01-01 2021-01-05 hospitalis\u00e9s A A 2 B 999 2021-01-04 2021-01-08 hospitalis\u00e9s A A 3 C 999 2021-01-12 2021-01-18 hospitalis\u00e9s C C 4 D 999 2021-01-13 2021-01-14 urgence C C 5 E 999 2021-01-19 2021-01-21 hospitalis\u00e9s C E 6 F 999 2021-01-25 2021-01-27 hospitalis\u00e9s F F 7 G 999 2017-01-01 NaT hospitalis\u00e9s G G \"\"\" # Preprocessing vo_to_merge , vo_to_not_merge = cleaning ( vo , remove_deleted_visits = remove_deleted_visits , long_stay_threshold = long_stay_threshold , long_stay_filtering = long_stay_filtering , open_stay_end_datetime = open_stay_end_datetime if open_stay_end_datetime is not None else datetime . now (), ) fw = get_framework ( vo_to_merge ) grouping_keys = [ \"person_id\" ] if not merge_different_hospitals : grouping_keys . append ( \"care_site_id\" ) if not merge_different_source_values : grouping_keys . append ( \"visit_source_value\" ) elif type ( merge_different_source_values ) == list : tmp = fw . DataFrame ( data = dict ( visit_source_value = merge_different_source_values , grouped_visit_source_value = True , ) ) vo_to_merge = vo_to_merge . merge ( tmp , on = \"visit_source_value\" , how = \"left\" ) vo_to_merge [ \"grouped_visit_source_value\" ] = vo_to_merge [ \"grouped_visit_source_value\" ] . fillna ( value = False ) grouping_keys . append ( \"grouped_visit_source_value\" ) # Cartesian product merged = vo_to_merge . merge ( vo_to_merge , on = grouping_keys , how = \"inner\" , suffixes = ( \"_1\" , \"_2\" ), ) # Keeping only visits where 1 occurs before 2 merged = merged [ merged [ \"visit_start_datetime_1\" ] <= merged [ \"visit_start_datetime_2\" ] ] # Checking correct overlap th = max_timedelta . total_seconds () merged [ \"overlap\" ] = substract_datetime ( merged [ \"visit_start_datetime_2\" ], merged [ \"visit_end_datetime_calc_1\" ] ) merged [ \"to_merge\" ] = ( merged [ \"overlap\" ] <= th ) . astype ( int ) merged [ \"contiguous\" ] = ( merged [ \"overlap\" ] <= 0 ) . astype ( int ) def get_first ( merged : DataFrame , contiguous_only : bool = False , ) -> Tuple [ DataFrame , DataFrame ]: \"\"\" Returns a boolean flag for each visit, telling if the visit if the first of a stay. The ``contiguous_only`` parameter controls if the visits have to be contiguous in the stay \"\"\" flag_col = \"contiguous\" if contiguous_only else \"to_merge\" flag_name = \"1_is_first_contiguous\" if contiguous_only else \"1_is_first\" concept_prefix = \"CONTIGUOUS_\" if contiguous_only else \"\" # If the only previous visit to be merged with is itself, we found our first visit ! first_visits = merged . groupby ( \"visit_occurrence_id_2\" )[ flag_col ] . sum () == 1 first_visits . name = flag_name # Adding this boolean flag to the merged DataFrame merged = merged . merge ( first_visits , left_on = \"visit_occurrence_id_1\" , right_index = True , how = \"inner\" , ) # Getting the corresponding first visit first_visit = ( merged . sort_values ( by = [ flag_name , \"visit_start_datetime_1\" ], ascending = [ False , False ] ) . groupby ( \"visit_occurrence_id_2\" ) . first ()[ \"visit_occurrence_id_1\" ] . reset_index () . rename ( columns = { \"visit_occurrence_id_1\" : f \" { concept_prefix } STAY_ID\" , \"visit_occurrence_id_2\" : \"visit_occurrence_id\" , } ) ) return merged , first_visit merged , first_contiguous_visit = get_first ( merged , contiguous_only = True ) merged , first_visit = get_first ( merged , contiguous_only = False ) # Concatenating merge visits with previously discarded ones results = fw . concat ( [ vo_to_merge . merge ( first_visit , on = \"visit_occurrence_id\" , how = \"inner\" , ) . merge ( first_contiguous_visit , on = \"visit_occurrence_id\" , how = \"inner\" , ), vo_to_not_merge , ] ) # Adding visit_occurrence_id as STAY_ID and CONTIGUOUS_STAY_ID to discarded visits results [ \"STAY_ID\" ] = results [ \"STAY_ID\" ] . combine_first ( results [ \"visit_occurrence_id\" ] ) results [ \"CONTIGUOUS_STAY_ID\" ] = results [ \"CONTIGUOUS_STAY_ID\" ] . combine_first ( results [ \"visit_occurrence_id\" ] ) # Removing tmp columns vo = vo . drop ( columns = [ \"visit_end_datetime_calc\" ]) return results . drop ( columns = ( set ( results . columns ) & set ([ \"visit_end_datetime_calc\" , \"grouped_visit_source_value\" ]) ) ) get_stays_duration get_stays_duration ( vo : DataFrame , algo : str = 'sum_of_visits_duration' , missing_end_date_handling : str = 'fill' , open_stay_end_datetime : Optional [ datetime ] = None ) -> DataFrame Computes stay duration. The input DataFrame should contain the STAY_ID and CONTIGUOUS_STAY_ID columns, that can be computed via the merge_visits() function. PARAMETER DESCRIPTION vo visit occurrence DataFrame with the STAY_ID and CONTIGUOUS_STAY_ID columns TYPE: DataFrame algo Which algo to use for computing stay durations. Available values are: \"sum_of_visits_duration\" : The stay duration will correspond to the sum of each visit duration in the stay. \"visits_date_difference\" : The stay duration will correspond to the difference between the end date of the last visit and the start date of the first visit. TYPE: str DEFAULT: 'sum_of_visits_duration' missing_end_date_handling How to handle visits with no end date. Available values are: \"fill\" : Missing values are filled with datetime.now() \"coerce\" : Missing values are handled as such, so duration of stays with open visits will be NaN. TYPE: str DEFAULT: 'fill' open_stay_end_datetime Used if missing_end_date_handling == \"fill\" . Provide the datetime with which open stays should be ended. Leave to None in order to used datetime.now() TYPE: Optional [ datetime ] DEFAULT: None RETURNS DESCRIPTION DataFrame stay DataFrame with STAY_ID as index, and the following columns: \"person_id\" \"t_start\" : The start date of the first visit of the stay \"t_end\" : The end date of the last visit of the stay \"STAY_DURATION\" : The duration (in hours) of the stay RAISES DESCRIPTION MissingConceptError If STAY_ID and CONTIGUOUS_STAY_ID are not in the input columns. Source code in eds_scikit/period/stays.py 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 @algo_checker ( algos = [ \"sum_of_visits_duration\" , \"visits_date_difference\" ]) @concept_checker ( concepts = [ \"STAY_DURATION\" ], only_adds_concepts = False ) def get_stays_duration ( vo : DataFrame , algo : str = \"sum_of_visits_duration\" , missing_end_date_handling : str = \"fill\" , open_stay_end_datetime : Optional [ datetime ] = None , ) -> DataFrame : \"\"\" Computes stay duration. The input DataFrame should contain the `STAY_ID` and `CONTIGUOUS_STAY_ID` columns, that can be computed via the `merge_visits()` function. Parameters ---------- vo : DataFrame visit occurrence DataFrame with the `STAY_ID` and `CONTIGUOUS_STAY_ID` columns algo : str Which algo to use for computing stay durations. Available values are: - `\"sum_of_visits_duration\"`: The stay duration will correspond to the sum of each visit duration in the stay. - `\"visits_date_difference\"`: The stay duration will correspond to the difference between the end date of the last visit and the start date of the first visit. missing_end_date_handling : str How to handle visits with no end date. Available values are: - `\"fill\"`: Missing values are filled with `datetime.now()` - `\"coerce\"`: Missing values are handled as such, so duration of stays with open visits will be NaN. open_stay_end_datetime: Optional[datetime] Used if `missing_end_date_handling == \"fill\"`. Provide the `datetime` with which open stays should be ended. Leave to `None` in order to used `datetime.now()` Returns ------- DataFrame *stay* DataFrame with `STAY_ID` as index, and the following columns: - `\"person_id\"` - `\"t_start\"`: The start date of the first visit of the stay - `\"t_end\"`: The end date of the last visit of the stay - `\"STAY_DURATION\"`: The duration (in hours) of the stay Raises ------ MissingConceptError If `STAY_ID` and `CONTIGUOUS_STAY_ID` are not in the input columns. \"\"\" if set (( \"STAY_ID\" , \"CONTIGUOUS_STAY_ID\" )) - set ( vo . columns ): raise MissingConceptError ( df_name = \"visit_occurence\" , required_concepts = [ ( \"STAY_ID\" , \"should be computed via 'merge_visits'\" ), ( \"CONTIGUOUS_STAY_ID\" , \"should be computed via 'merge_visits'\" ), ], ) if missing_end_date_handling == \"fill\" : # Cannot use fillna() with datetime in Koalas if open_stay_end_datetime is None : open_stay_end_datetime = datetime . now () vo [ \"visit_end_datetime_calc\" ] = open_stay_end_datetime vo [ \"visit_end_datetime_calc\" ] = vo [ \"visit_end_datetime\" ] . combine_first ( vo [ \"visit_end_datetime_calc\" ] ) elif missing_end_date_handling == \"coerce\" : vo [ \"visit_end_datetime_calc\" ] = vo [ \"visit_end_datetime\" ] agg_dict = dict ( person_id = ( \"person_id\" , \"first\" ), t_start = ( \"visit_start_datetime\" , \"min\" ), t_end = ( \"visit_end_datetime_calc\" , \"max\" ), ) if algo == \"sum_of_visits_duration\" : agg_dict [ \"STAY_ID\" ] = ( \"STAY_ID\" , \"first\" ) contiguous_stays = vo . groupby ( \"CONTIGUOUS_STAY_ID\" ) . agg ( ** agg_dict ) contiguous_stays [ \"CONTIGUOUS_STAY_DURATION\" ] = substract_datetime ( contiguous_stays [ \"t_end\" ], contiguous_stays [ \"t_start\" ], out = \"hours\" ) agg_dict = dict ( person_id = ( \"person_id\" , \"first\" ), t_start = ( \"t_start\" , \"min\" ), t_end = ( \"t_end\" , \"max\" ), STAY_DURATION = ( \"CONTIGUOUS_STAY_DURATION\" , \"sum\" ), ) stays = contiguous_stays . groupby ( \"STAY_ID\" ) . agg ( ** agg_dict ) elif algo == \"visits_date_difference\" : stays = vo . groupby ( \"STAY_ID\" ) . agg ( ** agg_dict ) stays [ \"STAY_DURATION\" ] = substract_datetime ( stays [ \"t_end\" ], stays [ \"t_start\" ], out = \"hours\" ) if missing_end_date_handling == \"coerce\" : stays . loc [ stays [ \"t_end\" ] . isna (), \"STAY_DURATION\" ] = NaN return stays","title":"stays"},{"location":"reference/period/stays/#eds_scikitperiodstays","text":"","title":"eds_scikit.period.stays"},{"location":"reference/period/stays/#eds_scikit.period.stays.cleaning","text":"cleaning ( vo , long_stay_threshold : timedelta , long_stay_filtering : Union [ str , None ], remove_deleted_visits : bool , open_stay_end_datetime : datetime ) -> Tuple [ DataFrame , DataFrame ] Preprocessing of visits before merging them in stays. The function will split the input vo DataFrame into 2, one that should undergo the merging procedure, and one that shouldn't. Depending on the input parameters, 3 type of visits can be prevented to undergo the merging procedure: Too long visits Too long AND unclosed visits Removed visits See the merge_visits() function for details of the parameters Source code in eds_scikit/period/stays.py 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 def cleaning ( vo , long_stay_threshold : timedelta , long_stay_filtering : Union [ str , None ], remove_deleted_visits : bool , open_stay_end_datetime : datetime , ) -> Tuple [ DataFrame , DataFrame ]: \"\"\" Preprocessing of visits before merging them in stays. The function will split the input `vo` DataFrame into 2, one that should undergo the merging procedure, and one that shouldn't. Depending on the input parameters, 3 type of visits can be prevented to undergo the merging procedure: - Too long visits - Too long AND unclosed visits - Removed visits See the [merge_visits()][eds_scikit.period.stays.merge_visits] function for details of the parameters \"\"\" LONG_STAY_FILTERING_VALUES = [ \"all\" , \"open\" , None ] DELETED_ROW_VALUE = \"supprim\u00e9\" if long_stay_filtering not in LONG_STAY_FILTERING_VALUES : raise ValueError ( f \"\"\"Unknown value for `long_stay_filtering`. Accepted values are { LONG_STAY_FILTERING_VALUES } \"\"\" ) if remove_deleted_visits : deleted_visit_mask = vo [ \"row_status_source_value\" ] == DELETED_ROW_VALUE no_starting_date_mask = vo [ \"visit_start_datetime\" ] . isna () no_ending_date_mask = vo [ \"visit_end_datetime\" ] . isna () vo [ \"visit_end_datetime_calc\" ] = open_stay_end_datetime # Cannot use fillna() with datetime in Koalas vo [ \"visit_end_datetime_calc\" ] = vo [ \"visit_end_datetime\" ] . combine_first ( vo [ \"visit_end_datetime_calc\" ] ) too_long_stays_mask = ( substract_datetime ( vo [ \"visit_end_datetime_calc\" ], vo [ \"visit_start_datetime\" ]) >= long_stay_threshold . total_seconds () ) mask = no_starting_date_mask if long_stay_filtering == \"all\" : mask = mask | too_long_stays_mask elif long_stay_filtering == \"open\" : mask = mask | ( too_long_stays_mask & no_ending_date_mask ) if remove_deleted_visits : mask = ( mask ) | deleted_visit_mask return vo [ ~ mask ], vo [ mask ]","title":"cleaning()"},{"location":"reference/period/stays/#eds_scikit.period.stays.merge_visits","text":"merge_visits ( vo : DataFrame , remove_deleted_visits : bool = True , long_stay_threshold : timedelta = timedelta ( days = 365 ), long_stay_filtering : Optional [ str ] = 'all' , open_stay_end_datetime : Optional [ datetime ] = None , max_timedelta : timedelta = timedelta ( days = 2 ), merge_different_hospitals : bool = False , merge_different_source_values : Union [ bool , List [ str ]] = [ 'hospitalis\u00e9s' , 'urgence' ]) -> DataFrame Merge \"close\" visit occurrences to consider them as a single stay by adding a STAY_ID and CONTIGUOUS_STAY_ID columns to the DataFrame. The value of these columns will be the visit_occurrence_id of the first (meaning the oldest) visit of the stay. From a temporal point of view, we consider that two visits belong to the same stay if either: They intersect The time difference between the end of the most recent and the start of the oldest is lower than max_timedelta (for STAY_ID ) or 0 (for CONTIGUOUS_STAY_ID ) Additionally, other parameters are available to further adjust the merging rules. See below. PARAMETER DESCRIPTION vo The visit_occurrence DataFrame, with at least the following columns: - visit_occurrence_id - person_id - visit_start_datetime_calc (from preprocessing) - visit_end_datetime (from preprocessing) Depending on the input parameters, additional columns may be required: - care_site_id (if merge_different_hospitals == True ) - visit_source_value (if merge_different_source_values != False ) - row_status_source_value (if remove_deleted_visits= True ) TYPE: DataFrame remove_deleted_visits Wether to remove deleted visits from the merging procedure. Deleted visits are extracted via the row_status_source_value column TYPE: bool DEFAULT: True long_stay_filtering Filtering method for long and/or non-closed visits. First of all, visits with no starting date won't be merged with any other visit, and visits with no ending date will have a temporary \"theoretical\" ending date set by datetime.now() . That being said, some visits are sometimes years long because they weren't closed at time. If other visits occurred during this timespan, they could be all merged into the same stay. To avoid this issue, filtering can be done depending on the long_stay_filtering value: all : All long stays (closed and open) are removed from the merging procedure open : Only long open stays are removed from the merging procedure None : No filtering is done on long visits Long stays are determined by the long_stay_threshold value. TYPE: Optional [ str ] DEFAULT: 'all' long_stay_threshold Minimum visit duration value to consider a visit as candidate for \"long visits filtering\" TYPE: timedelta DEFAULT: timedelta(days=365) open_stay_end_datetime Datetime to use in order to fill the visit_end_datetime of open visits. This is necessary in order to compute stay duration and to filter long stays. If not provided datetime.now() will be used. You might provide the extraction date of your data here. TYPE: Optional [ datetime ] DEFAULT: None max_timedelta Maximum time difference between the end of a visit and the start of another to consider them as belonging to the same stay. This duration is internally converted in seconds before comparing. Thus, if you want e.g. to merge visits happening in two consecutive days, you should use timedelta(days=2) and NOT timedelta(days=1) in order to take into account extreme cases such as an first visit ending on Monday at 00h01 AM and another one starting at 23h59 PM on Tuesday TYPE: timedelta DEFAULT: timedelta(days=2) merge_different_hospitals Wether to allow visits occurring in different hospitals to be merged into a same stay TYPE: bool DEFAULT: False merge_different_source_values Wether to allow visits with different visit_source_value to be merged into a same stay. Values can be: True : the visit_source_value isn't taken into account for the merging False : only visits with the same visit_source_value can be merged into a same stay List[str] : only visits which visit_source_value is in the provided list can be merged together. Warning : You should avoid merging visits where visit_source_value == \"hospitalisation incompl\u00e8te\" , because those stays are often never closed. TYPE: Union [ bool , List [ str ]] DEFAULT: ['hospitalis\u00e9s', 'urgence'] RETURNS DESCRIPTION vo Visit occurrence DataFrame with additional STAY_ID column TYPE: DataFrame Examples: >>> import pandas as pd >>> from datetime import datetime , timedelta >>> data = { 1 : ['A', 999, datetime(2021,1,1), datetime(2021,1,5), 'hospitalis\u00e9s'], 2 : ['B', 999, datetime(2021,1,4), datetime(2021,1,8), 'hospitalis\u00e9s'], 3 : ['C', 999, datetime(2021,1,12), datetime(2021,1,18), 'hospitalis\u00e9s'], 4 : ['D', 999, datetime(2021,1,13), datetime(2021,1,14), 'urgence'], 5 : ['E', 999, datetime(2021,1,19), datetime(2021,1,21), 'hospitalis\u00e9s'], 6 : ['F', 999, datetime(2021,1,25), datetime(2021,1,27), 'hospitalis\u00e9s'], 7 : ['G', 999, datetime(2017,1,1), None, \"hospitalis\u00e9s\"] } >>> vo = pd . DataFrame . from_dict ( data, orient=\"index\", columns=[ \"visit_occurrence_id\", \"person_id\", \"visit_start_datetime\", \"visit_end_datetime\", \"visit_source_value\", ], ) >>> vo visit_occurrence_id person_id visit_start_datetime visit_end_datetime visit_source_value 1 A 999 2021-01-01 2021-01-05 hospitalis\u00e9s 2 B 999 2021-01-04 2021-01-08 hospitalis\u00e9s 3 C 999 2021-01-12 2021-01-18 hospitalis\u00e9s 4 D 999 2021-01-13 2021-01-14 urgence 5 E 999 2021-01-19 2021-01-21 hospitalis\u00e9s 6 F 999 2021-01-25 2021-01-27 hospitalis\u00e9s 7 G 999 2017-01-01 NaT hospitalis\u00e9s >>> vo = merge_visits ( vo, remove_deleted_visits=True, long_stay_threshold=timedelta(days=365), long_stay_filtering=\"all\", max_timedelta=timedelta(hours=24), merge_different_hospitals=False, merge_different_source_values=[\"hospitalis\u00e9s\", \"urgence\"], ) >>> vo visit_occurrence_id person_id visit_start_datetime visit_end_datetime visit_source_value STAY_ID CONTIGUOUS_STAY_ID 1 A 999 2021-01-01 2021-01-05 hospitalis\u00e9s A A 2 B 999 2021-01-04 2021-01-08 hospitalis\u00e9s A A 3 C 999 2021-01-12 2021-01-18 hospitalis\u00e9s C C 4 D 999 2021-01-13 2021-01-14 urgence C C 5 E 999 2021-01-19 2021-01-21 hospitalis\u00e9s C E 6 F 999 2021-01-25 2021-01-27 hospitalis\u00e9s F F 7 G 999 2017-01-01 NaT hospitalis\u00e9s G G Source code in eds_scikit/period/stays.py 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 @concept_checker ( concepts = [ \"STAY_ID\" , \"CONTIGUOUS_STAY_ID\" ]) def merge_visits ( vo : DataFrame , remove_deleted_visits : bool = True , long_stay_threshold : timedelta = timedelta ( days = 365 ), long_stay_filtering : Optional [ str ] = \"all\" , open_stay_end_datetime : Optional [ datetime ] = None , max_timedelta : timedelta = timedelta ( days = 2 ), merge_different_hospitals : bool = False , merge_different_source_values : Union [ bool , List [ str ]] = [ \"hospitalis\u00e9s\" , \"urgence\" ], ) -> DataFrame : \"\"\" Merge \"close\" visit occurrences to consider them as a single stay by adding a ``STAY_ID`` and ``CONTIGUOUS_STAY_ID`` columns to the DataFrame. The value of these columns will be the `visit_occurrence_id` of the first (meaning the oldest) visit of the stay. From a temporal point of view, we consider that two visits belong to the same stay if either: - They intersect - The time difference between the end of the most recent and the start of the oldest is lower than ``max_timedelta`` (for ``STAY_ID``) or 0 (for ``CONTIGUOUS_STAY_ID``) Additionally, other parameters are available to further adjust the merging rules. See below. Parameters ---------- vo : DataFrame The ``visit_occurrence`` DataFrame, with at least the following columns: - visit_occurrence_id - person_id - visit_start_datetime_calc (from preprocessing) - visit_end_datetime (from preprocessing) Depending on the input parameters, additional columns may be required: - care_site_id (if ``merge_different_hospitals == True``) - visit_source_value (if ``merge_different_source_values != False``) - row_status_source_value (if ``remove_deleted_visits= True``) remove_deleted_visits: bool Wether to remove deleted visits from the merging procedure. Deleted visits are extracted via the `row_status_source_value` column long_stay_filtering : Optional[str] Filtering method for long and/or non-closed visits. First of all, visits with no starting date won't be merged with any other visit, and visits with no ending date will have a temporary \"theoretical\" ending date set by ``datetime.now()``. That being said, some visits are sometimes years long because they weren't closed at time. If other visits occurred during this timespan, they could be all merged into the same stay. To avoid this issue, filtering can be done depending on the ``long_stay_filtering`` value: - ``all``: All long stays (closed and open) are removed from the merging procedure - ``open``: Only long open stays are removed from the merging procedure - ``None``: No filtering is done on long visits Long stays are determined by the ``long_stay_threshold`` value. long_stay_threshold : timedelta Minimum visit duration value to consider a visit as candidate for \"long visits filtering\" open_stay_end_datetime: Optional[datetime] Datetime to use in order to fill the `visit_end_datetime` of open visits. This is necessary in order to compute stay duration and to filter long stays. If not provided `datetime.now()` will be used. You might provide the extraction date of your data here. max_timedelta : timedelta Maximum time difference between the end of a visit and the start of another to consider them as belonging to the same stay. This duration is internally converted in seconds before comparing. Thus, if you want e.g. to merge visits happening in two consecutive days, you should use `timedelta(days=2)` and NOT `timedelta(days=1)` in order to take into account extreme cases such as an first visit ending on Monday at 00h01 AM and another one starting at 23h59 PM on Tuesday merge_different_hospitals : bool Wether to allow visits occurring in different hospitals to be merged into a same stay merge_different_source_values : Union[bool, List[str]] Wether to allow visits with different `visit_source_value` to be merged into a same stay. Values can be: - `True`: the `visit_source_value` isn't taken into account for the merging - `False`: only visits with the same `visit_source_value` can be merged into a same stay - `List[str]`: only visits which `visit_source_value` is in the provided list can be merged together. **Warning**: You should avoid merging visits where `visit_source_value == \"hospitalisation incompl\u00e8te\"`, because those stays are often never closed. Returns ------- vo : DataFrame Visit occurrence DataFrame with additional `STAY_ID` column Examples -------- >>> import pandas as pd >>> from datetime import datetime, timedelta >>> data = { 1 : ['A', 999, datetime(2021,1,1), datetime(2021,1,5), 'hospitalis\u00e9s'], 2 : ['B', 999, datetime(2021,1,4), datetime(2021,1,8), 'hospitalis\u00e9s'], 3 : ['C', 999, datetime(2021,1,12), datetime(2021,1,18), 'hospitalis\u00e9s'], 4 : ['D', 999, datetime(2021,1,13), datetime(2021,1,14), 'urgence'], 5 : ['E', 999, datetime(2021,1,19), datetime(2021,1,21), 'hospitalis\u00e9s'], 6 : ['F', 999, datetime(2021,1,25), datetime(2021,1,27), 'hospitalis\u00e9s'], 7 : ['G', 999, datetime(2017,1,1), None, \"hospitalis\u00e9s\"] } >>> vo = pd.DataFrame.from_dict( data, orient=\"index\", columns=[ \"visit_occurrence_id\", \"person_id\", \"visit_start_datetime\", \"visit_end_datetime\", \"visit_source_value\", ], ) >>> vo visit_occurrence_id person_id visit_start_datetime visit_end_datetime visit_source_value 1 A 999 2021-01-01 2021-01-05 hospitalis\u00e9s 2 B 999 2021-01-04 2021-01-08 hospitalis\u00e9s 3 C 999 2021-01-12 2021-01-18 hospitalis\u00e9s 4 D 999 2021-01-13 2021-01-14 urgence 5 E 999 2021-01-19 2021-01-21 hospitalis\u00e9s 6 F 999 2021-01-25 2021-01-27 hospitalis\u00e9s 7 G 999 2017-01-01 NaT hospitalis\u00e9s >>> vo = merge_visits( vo, remove_deleted_visits=True, long_stay_threshold=timedelta(days=365), long_stay_filtering=\"all\", max_timedelta=timedelta(hours=24), merge_different_hospitals=False, merge_different_source_values=[\"hospitalis\u00e9s\", \"urgence\"], ) >>> vo visit_occurrence_id person_id visit_start_datetime visit_end_datetime visit_source_value STAY_ID CONTIGUOUS_STAY_ID 1 A 999 2021-01-01 2021-01-05 hospitalis\u00e9s A A 2 B 999 2021-01-04 2021-01-08 hospitalis\u00e9s A A 3 C 999 2021-01-12 2021-01-18 hospitalis\u00e9s C C 4 D 999 2021-01-13 2021-01-14 urgence C C 5 E 999 2021-01-19 2021-01-21 hospitalis\u00e9s C E 6 F 999 2021-01-25 2021-01-27 hospitalis\u00e9s F F 7 G 999 2017-01-01 NaT hospitalis\u00e9s G G \"\"\" # Preprocessing vo_to_merge , vo_to_not_merge = cleaning ( vo , remove_deleted_visits = remove_deleted_visits , long_stay_threshold = long_stay_threshold , long_stay_filtering = long_stay_filtering , open_stay_end_datetime = open_stay_end_datetime if open_stay_end_datetime is not None else datetime . now (), ) fw = get_framework ( vo_to_merge ) grouping_keys = [ \"person_id\" ] if not merge_different_hospitals : grouping_keys . append ( \"care_site_id\" ) if not merge_different_source_values : grouping_keys . append ( \"visit_source_value\" ) elif type ( merge_different_source_values ) == list : tmp = fw . DataFrame ( data = dict ( visit_source_value = merge_different_source_values , grouped_visit_source_value = True , ) ) vo_to_merge = vo_to_merge . merge ( tmp , on = \"visit_source_value\" , how = \"left\" ) vo_to_merge [ \"grouped_visit_source_value\" ] = vo_to_merge [ \"grouped_visit_source_value\" ] . fillna ( value = False ) grouping_keys . append ( \"grouped_visit_source_value\" ) # Cartesian product merged = vo_to_merge . merge ( vo_to_merge , on = grouping_keys , how = \"inner\" , suffixes = ( \"_1\" , \"_2\" ), ) # Keeping only visits where 1 occurs before 2 merged = merged [ merged [ \"visit_start_datetime_1\" ] <= merged [ \"visit_start_datetime_2\" ] ] # Checking correct overlap th = max_timedelta . total_seconds () merged [ \"overlap\" ] = substract_datetime ( merged [ \"visit_start_datetime_2\" ], merged [ \"visit_end_datetime_calc_1\" ] ) merged [ \"to_merge\" ] = ( merged [ \"overlap\" ] <= th ) . astype ( int ) merged [ \"contiguous\" ] = ( merged [ \"overlap\" ] <= 0 ) . astype ( int ) def get_first ( merged : DataFrame , contiguous_only : bool = False , ) -> Tuple [ DataFrame , DataFrame ]: \"\"\" Returns a boolean flag for each visit, telling if the visit if the first of a stay. The ``contiguous_only`` parameter controls if the visits have to be contiguous in the stay \"\"\" flag_col = \"contiguous\" if contiguous_only else \"to_merge\" flag_name = \"1_is_first_contiguous\" if contiguous_only else \"1_is_first\" concept_prefix = \"CONTIGUOUS_\" if contiguous_only else \"\" # If the only previous visit to be merged with is itself, we found our first visit ! first_visits = merged . groupby ( \"visit_occurrence_id_2\" )[ flag_col ] . sum () == 1 first_visits . name = flag_name # Adding this boolean flag to the merged DataFrame merged = merged . merge ( first_visits , left_on = \"visit_occurrence_id_1\" , right_index = True , how = \"inner\" , ) # Getting the corresponding first visit first_visit = ( merged . sort_values ( by = [ flag_name , \"visit_start_datetime_1\" ], ascending = [ False , False ] ) . groupby ( \"visit_occurrence_id_2\" ) . first ()[ \"visit_occurrence_id_1\" ] . reset_index () . rename ( columns = { \"visit_occurrence_id_1\" : f \" { concept_prefix } STAY_ID\" , \"visit_occurrence_id_2\" : \"visit_occurrence_id\" , } ) ) return merged , first_visit merged , first_contiguous_visit = get_first ( merged , contiguous_only = True ) merged , first_visit = get_first ( merged , contiguous_only = False ) # Concatenating merge visits with previously discarded ones results = fw . concat ( [ vo_to_merge . merge ( first_visit , on = \"visit_occurrence_id\" , how = \"inner\" , ) . merge ( first_contiguous_visit , on = \"visit_occurrence_id\" , how = \"inner\" , ), vo_to_not_merge , ] ) # Adding visit_occurrence_id as STAY_ID and CONTIGUOUS_STAY_ID to discarded visits results [ \"STAY_ID\" ] = results [ \"STAY_ID\" ] . combine_first ( results [ \"visit_occurrence_id\" ] ) results [ \"CONTIGUOUS_STAY_ID\" ] = results [ \"CONTIGUOUS_STAY_ID\" ] . combine_first ( results [ \"visit_occurrence_id\" ] ) # Removing tmp columns vo = vo . drop ( columns = [ \"visit_end_datetime_calc\" ]) return results . drop ( columns = ( set ( results . columns ) & set ([ \"visit_end_datetime_calc\" , \"grouped_visit_source_value\" ]) ) )","title":"merge_visits()"},{"location":"reference/period/stays/#eds_scikit.period.stays.get_stays_duration","text":"get_stays_duration ( vo : DataFrame , algo : str = 'sum_of_visits_duration' , missing_end_date_handling : str = 'fill' , open_stay_end_datetime : Optional [ datetime ] = None ) -> DataFrame Computes stay duration. The input DataFrame should contain the STAY_ID and CONTIGUOUS_STAY_ID columns, that can be computed via the merge_visits() function. PARAMETER DESCRIPTION vo visit occurrence DataFrame with the STAY_ID and CONTIGUOUS_STAY_ID columns TYPE: DataFrame algo Which algo to use for computing stay durations. Available values are: \"sum_of_visits_duration\" : The stay duration will correspond to the sum of each visit duration in the stay. \"visits_date_difference\" : The stay duration will correspond to the difference between the end date of the last visit and the start date of the first visit. TYPE: str DEFAULT: 'sum_of_visits_duration' missing_end_date_handling How to handle visits with no end date. Available values are: \"fill\" : Missing values are filled with datetime.now() \"coerce\" : Missing values are handled as such, so duration of stays with open visits will be NaN. TYPE: str DEFAULT: 'fill' open_stay_end_datetime Used if missing_end_date_handling == \"fill\" . Provide the datetime with which open stays should be ended. Leave to None in order to used datetime.now() TYPE: Optional [ datetime ] DEFAULT: None RETURNS DESCRIPTION DataFrame stay DataFrame with STAY_ID as index, and the following columns: \"person_id\" \"t_start\" : The start date of the first visit of the stay \"t_end\" : The end date of the last visit of the stay \"STAY_DURATION\" : The duration (in hours) of the stay RAISES DESCRIPTION MissingConceptError If STAY_ID and CONTIGUOUS_STAY_ID are not in the input columns. Source code in eds_scikit/period/stays.py 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 @algo_checker ( algos = [ \"sum_of_visits_duration\" , \"visits_date_difference\" ]) @concept_checker ( concepts = [ \"STAY_DURATION\" ], only_adds_concepts = False ) def get_stays_duration ( vo : DataFrame , algo : str = \"sum_of_visits_duration\" , missing_end_date_handling : str = \"fill\" , open_stay_end_datetime : Optional [ datetime ] = None , ) -> DataFrame : \"\"\" Computes stay duration. The input DataFrame should contain the `STAY_ID` and `CONTIGUOUS_STAY_ID` columns, that can be computed via the `merge_visits()` function. Parameters ---------- vo : DataFrame visit occurrence DataFrame with the `STAY_ID` and `CONTIGUOUS_STAY_ID` columns algo : str Which algo to use for computing stay durations. Available values are: - `\"sum_of_visits_duration\"`: The stay duration will correspond to the sum of each visit duration in the stay. - `\"visits_date_difference\"`: The stay duration will correspond to the difference between the end date of the last visit and the start date of the first visit. missing_end_date_handling : str How to handle visits with no end date. Available values are: - `\"fill\"`: Missing values are filled with `datetime.now()` - `\"coerce\"`: Missing values are handled as such, so duration of stays with open visits will be NaN. open_stay_end_datetime: Optional[datetime] Used if `missing_end_date_handling == \"fill\"`. Provide the `datetime` with which open stays should be ended. Leave to `None` in order to used `datetime.now()` Returns ------- DataFrame *stay* DataFrame with `STAY_ID` as index, and the following columns: - `\"person_id\"` - `\"t_start\"`: The start date of the first visit of the stay - `\"t_end\"`: The end date of the last visit of the stay - `\"STAY_DURATION\"`: The duration (in hours) of the stay Raises ------ MissingConceptError If `STAY_ID` and `CONTIGUOUS_STAY_ID` are not in the input columns. \"\"\" if set (( \"STAY_ID\" , \"CONTIGUOUS_STAY_ID\" )) - set ( vo . columns ): raise MissingConceptError ( df_name = \"visit_occurence\" , required_concepts = [ ( \"STAY_ID\" , \"should be computed via 'merge_visits'\" ), ( \"CONTIGUOUS_STAY_ID\" , \"should be computed via 'merge_visits'\" ), ], ) if missing_end_date_handling == \"fill\" : # Cannot use fillna() with datetime in Koalas if open_stay_end_datetime is None : open_stay_end_datetime = datetime . now () vo [ \"visit_end_datetime_calc\" ] = open_stay_end_datetime vo [ \"visit_end_datetime_calc\" ] = vo [ \"visit_end_datetime\" ] . combine_first ( vo [ \"visit_end_datetime_calc\" ] ) elif missing_end_date_handling == \"coerce\" : vo [ \"visit_end_datetime_calc\" ] = vo [ \"visit_end_datetime\" ] agg_dict = dict ( person_id = ( \"person_id\" , \"first\" ), t_start = ( \"visit_start_datetime\" , \"min\" ), t_end = ( \"visit_end_datetime_calc\" , \"max\" ), ) if algo == \"sum_of_visits_duration\" : agg_dict [ \"STAY_ID\" ] = ( \"STAY_ID\" , \"first\" ) contiguous_stays = vo . groupby ( \"CONTIGUOUS_STAY_ID\" ) . agg ( ** agg_dict ) contiguous_stays [ \"CONTIGUOUS_STAY_DURATION\" ] = substract_datetime ( contiguous_stays [ \"t_end\" ], contiguous_stays [ \"t_start\" ], out = \"hours\" ) agg_dict = dict ( person_id = ( \"person_id\" , \"first\" ), t_start = ( \"t_start\" , \"min\" ), t_end = ( \"t_end\" , \"max\" ), STAY_DURATION = ( \"CONTIGUOUS_STAY_DURATION\" , \"sum\" ), ) stays = contiguous_stays . groupby ( \"STAY_ID\" ) . agg ( ** agg_dict ) elif algo == \"visits_date_difference\" : stays = vo . groupby ( \"STAY_ID\" ) . agg ( ** agg_dict ) stays [ \"STAY_DURATION\" ] = substract_datetime ( stays [ \"t_end\" ], stays [ \"t_start\" ], out = \"hours\" ) if missing_end_date_handling == \"coerce\" : stays . loc [ stays [ \"t_end\" ] . isna (), \"STAY_DURATION\" ] = NaN return stays","title":"get_stays_duration()"},{"location":"reference/period/tagging_functions/","text":"eds_scikit.period.tagging_functions tagging tagging ( tag_to_df : DataFrame , tag_from_df : DataFrame , concept_to_tag : str , tag_to_date_cols : List [ str ] = [ 't_start' , 't_end' ], tag_from_date_cols : List [ str ] = [ 't_start' , 't_end' ], algo : str = 'intersection' ) -> DataFrame PARAMETER DESCRIPTION tag_to_df TYPE: DataFrame tag_from_df TYPE: DataFrame concept_to_tag TYPE: str tag_to_date_cols TYPE: List [ str ], optional DEFAULT: ['t_start', 't_end'] tag_from_date_cols TYPE: List [ str ], optional DEFAULT: ['t_start', 't_end'] algo TYPE: str , optional DEFAULT: 'intersection' RETURNS DESCRIPTION DataFrame Source code in eds_scikit/period/tagging_functions.py 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 def tagging ( tag_to_df : DataFrame , tag_from_df : DataFrame , concept_to_tag : str , tag_to_date_cols : List [ str ] = [ \"t_start\" , \"t_end\" ], tag_from_date_cols : List [ str ] = [ \"t_start\" , \"t_end\" ], algo : str = \"intersection\" , ) -> DataFrame : \"\"\" Parameters ---------- tag_to_df : DataFrame tag_from_df : DataFrame concept_to_tag : str tag_to_date_cols : List[str], optional tag_from_date_cols : List[str], optional algo : str, optional Returns ------- DataFrame \"\"\" framework = get_framework ( tag_to_df ) tag_to_df = tag_to_df . assign ( event_id = tag_to_df . index ) tag_from = tag_from_df . loc [ tag_from_df . concept == concept_to_tag , [ \"person_id\" , \"value\" ] + [ \"t_start\" , \"t_end\" ], ] tmp = ( tag_to_df . rename ( columns = { tag_to_date_cols [ 0 ]: \"t_start_x\" , tag_to_date_cols [ 1 ]: \"t_end_x\" } ) . merge ( tag_from . rename ( columns = { tag_from_date_cols [ 0 ]: \"t_start_y\" , tag_from_date_cols [ 1 ]: \"t_end_y\" , } ), on = \"person_id\" , how = \"left\" , ) . dropna ( subset = [ \"t_start_x\" , \"t_end_x\" , \"t_start_y\" , \"t_end_y\" ]) ) if len ( tmp ) == 0 : # TODO: is this necessary ? logger . warning ( \"No matching were found between the 2 DataFrames\" ) return framework . DataFrame ( columns = [ \"person_id\" , \"t_start\" , \"t_end\" , \"concept\" , \"value\" ] ) tmp [ \"tag\" ] = compare_intervals ( tmp [ \"t_start_x\" ], tmp [ \"t_end_x\" ], tmp [ \"t_start_y\" ], tmp [ \"t_end_y\" ], algo = algo , ) value_col = ( \"value_y\" if (( \"value\" in tag_to_df . columns ) and ( \"value\" in tag_from_df . columns )) else \"value\" ) tags = ( tmp . groupby ([ \"event_id\" , value_col ]) . tag . any () . unstack () . fillna ( False ) . reset_index () ) tags = tag_to_df [[ \"event_id\" ]] . merge ( tags , on = \"event_id\" , how = \"left\" ) . fillna ( False ) tags = tag_to_df . merge ( tags , on = \"event_id\" , how = \"left\" ) . drop ( columns = \"event_id\" ) return tags","title":"tagging_functions"},{"location":"reference/period/tagging_functions/#eds_scikitperiodtagging_functions","text":"","title":"eds_scikit.period.tagging_functions"},{"location":"reference/period/tagging_functions/#eds_scikit.period.tagging_functions.tagging","text":"tagging ( tag_to_df : DataFrame , tag_from_df : DataFrame , concept_to_tag : str , tag_to_date_cols : List [ str ] = [ 't_start' , 't_end' ], tag_from_date_cols : List [ str ] = [ 't_start' , 't_end' ], algo : str = 'intersection' ) -> DataFrame PARAMETER DESCRIPTION tag_to_df TYPE: DataFrame tag_from_df TYPE: DataFrame concept_to_tag TYPE: str tag_to_date_cols TYPE: List [ str ], optional DEFAULT: ['t_start', 't_end'] tag_from_date_cols TYPE: List [ str ], optional DEFAULT: ['t_start', 't_end'] algo TYPE: str , optional DEFAULT: 'intersection' RETURNS DESCRIPTION DataFrame Source code in eds_scikit/period/tagging_functions.py 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 def tagging ( tag_to_df : DataFrame , tag_from_df : DataFrame , concept_to_tag : str , tag_to_date_cols : List [ str ] = [ \"t_start\" , \"t_end\" ], tag_from_date_cols : List [ str ] = [ \"t_start\" , \"t_end\" ], algo : str = \"intersection\" , ) -> DataFrame : \"\"\" Parameters ---------- tag_to_df : DataFrame tag_from_df : DataFrame concept_to_tag : str tag_to_date_cols : List[str], optional tag_from_date_cols : List[str], optional algo : str, optional Returns ------- DataFrame \"\"\" framework = get_framework ( tag_to_df ) tag_to_df = tag_to_df . assign ( event_id = tag_to_df . index ) tag_from = tag_from_df . loc [ tag_from_df . concept == concept_to_tag , [ \"person_id\" , \"value\" ] + [ \"t_start\" , \"t_end\" ], ] tmp = ( tag_to_df . rename ( columns = { tag_to_date_cols [ 0 ]: \"t_start_x\" , tag_to_date_cols [ 1 ]: \"t_end_x\" } ) . merge ( tag_from . rename ( columns = { tag_from_date_cols [ 0 ]: \"t_start_y\" , tag_from_date_cols [ 1 ]: \"t_end_y\" , } ), on = \"person_id\" , how = \"left\" , ) . dropna ( subset = [ \"t_start_x\" , \"t_end_x\" , \"t_start_y\" , \"t_end_y\" ]) ) if len ( tmp ) == 0 : # TODO: is this necessary ? logger . warning ( \"No matching were found between the 2 DataFrames\" ) return framework . DataFrame ( columns = [ \"person_id\" , \"t_start\" , \"t_end\" , \"concept\" , \"value\" ] ) tmp [ \"tag\" ] = compare_intervals ( tmp [ \"t_start_x\" ], tmp [ \"t_end_x\" ], tmp [ \"t_start_y\" ], tmp [ \"t_end_y\" ], algo = algo , ) value_col = ( \"value_y\" if (( \"value\" in tag_to_df . columns ) and ( \"value\" in tag_from_df . columns )) else \"value\" ) tags = ( tmp . groupby ([ \"event_id\" , value_col ]) . tag . any () . unstack () . fillna ( False ) . reset_index () ) tags = tag_to_df [[ \"event_id\" ]] . merge ( tags , on = \"event_id\" , how = \"left\" ) . fillna ( False ) tags = tag_to_df . merge ( tags , on = \"event_id\" , how = \"left\" ) . drop ( columns = \"event_id\" ) return tags","title":"tagging()"},{"location":"reference/phenotype/","text":"eds_scikit.phenotype","title":"`eds_scikit.phenotype`"},{"location":"reference/phenotype/#eds_scikitphenotype","text":"","title":"eds_scikit.phenotype"},{"location":"reference/phenotype/base/","text":"eds_scikit.phenotype.base Features Features () Class used to store features (i.e. DataFrames). Features are stored in the self._features dictionary. Source code in eds_scikit/phenotype/base.py 22 23 24 def __init__ ( self ): self . _features = {} self . last_feature = None Phenotype Phenotype ( data : BaseData , name : Optional [ str ] = None , ** kwargs ) Base class for phenotyping PARAMETER DESCRIPTION data A BaseData object TYPE: BaseData name Name of the phenotype. If left to None, the name of the class will be used instead TYPE: Optional [ str ] DEFAULT: None Source code in eds_scikit/phenotype/base.py 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 def __init__ ( self , data : BaseData , name : Optional [ str ] = None , ** kwargs , ): \"\"\" Parameters ---------- data : BaseData A BaseData object name : Optional[str] Name of the phenotype. If left to None, the name of the class will be used instead \"\"\" self . data = data self . features = Features () self . name = ( to_valid_variable_name ( name ) if name is not None else self . __class__ . __name__ ) self . logger = logger . bind ( classname = self . name , sep = \".\" ) add_code_feature add_code_feature ( output_feature : str , codes : dict , source : str = 'icd10' , additional_filtering : Optional [ dict ] = None ) Adds a feature from either ICD10 or CCAM codes PARAMETER DESCRIPTION output_feature Name of the feature TYPE: str codes Dictionary of codes to provide to the from_codes function TYPE: dict source Either 'icd10' or 'ccam', by default 'icd10' TYPE: str DEFAULT: 'icd10' additional_filtering Dictionary passed to the from_codes functions for filtering TYPE: Optional [ dict ] DEFAULT: None RETURNS DESCRIPTION Phenotype The current Phenotype object with an additional feature stored in self.features[output_feature] Source code in eds_scikit/phenotype/base.py 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 def add_code_feature ( self , output_feature : str , codes : dict , source : str = \"icd10\" , additional_filtering : Optional [ dict ] = None , ): \"\"\" Adds a feature from either ICD10 or CCAM codes Parameters ---------- output_feature : str Name of the feature codes : dict Dictionary of codes to provide to the `from_codes` function source : str, Either 'icd10' or 'ccam', by default 'icd10' additional_filtering : Optional[dict] Dictionary passed to the `from_codes` functions for filtering Returns ------- Phenotype The current Phenotype object with an additional feature stored in self.features[output_feature] \"\"\" additional_filtering = additional_filtering or dict () if source not in [ \"icd10\" , \"ccam\" ]: raise ValueError ( f \"source should be either 'icd10' or 'ccam', got { source } \" ) self . logger . info ( f \"Getting { source . upper () } features...\" ) from_code_func = ( conditions_from_icd10 if ( source == \"icd10\" ) else procedures_from_ccam ) codes_df = ( self . data . condition_occurrence if ( source == \"icd10\" ) else self . data . procedure_occurrence ) df = from_code_func ( codes_df , codes = codes , additional_filtering = additional_filtering , date_from_visit = False , ) df [ \"phenotype\" ] = self . name df = df . rename ( columns = { \"concept\" : \"subphenotype\" }) bd . cache ( df ) self . features [ output_feature ] = df self . logger . info ( f \" { source . upper () } features stored in self.features[' { output_feature } '] (N = { len ( df ) } )\" ) return self agg_single_feature agg_single_feature ( input_feature : str , output_feature : Optional [ str ] = None , level : str = 'patient' , subphenotype : bool = True , threshold : int = 1 ) -> Phenotype Simple aggregation rule on a feature: If level=\"patient\", keeps patients with at least threshold visits showing the (sub)phenotype If level=\"visit\", keeps visits with at least threshold events (could be ICD10 codes, NLP features, biology, etc) showing the (sub)phenotype PARAMETER DESCRIPTION input_feature Name of the input feature TYPE: str output_feature Name of the input feature. If None, will be set to input_feature + \"_agg\" TYPE: Optional [ str ] DEFAULT: None level On which level to do the aggregation, either \"patient\" or \"visit\" TYPE: str DEFAULT: 'patient' subphenotype Whether the threshold should apply to the phenotype (\"phenotype\" column) of the subphenotype (\"subphenotype\" column) TYPE: bool DEFAULT: True threshold Minimal number of events (which definition depends on the level value) TYPE: int , optional DEFAULT: 1 RETURNS DESCRIPTION Phenotype The current Phenotype object with an additional feature stored in self.features[output_feature] Source code in eds_scikit/phenotype/base.py 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 def agg_single_feature ( self , input_feature : str , output_feature : Optional [ str ] = None , level : str = \"patient\" , subphenotype : bool = True , threshold : int = 1 , ) -> \"Phenotype\" : \"\"\" Simple aggregation rule on a feature: - If level=\"patient\", keeps patients with at least `threshold` visits showing the (sub)phenotype - If level=\"visit\", keeps visits with at least `threshold` events (could be ICD10 codes, NLP features, biology, etc) showing the (sub)phenotype Parameters ---------- input_feature : str Name of the input feature output_feature : Optional[str] Name of the input feature. If None, will be set to input_feature + \"_agg\" level : str On which level to do the aggregation, either \"patient\" or \"visit\" subphenotype : bool Whether the threshold should apply to the phenotype (\"phenotype\" column) of the subphenotype (\"subphenotype\" column) threshold : int, optional Minimal number of *events* (which definition depends on the `level` value) Returns ------- Phenotype The current Phenotype object with an additional feature stored in self.features[output_feature] \"\"\" assert level in { \"patient\" , \"visit\" } output_feature = output_feature or f \" { input_feature } _agg\" if input_feature not in self . features : raise ValueError ( f \"Input feature { input_feature } not found in self.features. \" \"Maybe you forgot to call self.get_features() ?\" ) # We use `size` below for two reasons # 1) to use it with the `threshold` parameter directly if level == 'visit' # 2) to drop duplicates on the group_cols + [\"visit_occurrence_id\"] subset phenotype_type = \"subphenotype\" if subphenotype else \"phenotype\" group_cols = [ \"person_id\" , phenotype_type ] group_visit = ( self . features [ input_feature ] . groupby ( group_cols + [ \"visit_occurrence_id\" ]) . size () . rename ( \"N\" ) # number of events per visit_occurrence . reset_index () ) if level == \"patient\" : group_visit = ( group_visit . groupby ( group_cols ) . size () . rename ( \"N\" ) # number of visits per person . reset_index () ) group_visit = group_visit [ group_visit [ \"N\" ] >= threshold ] . drop ( columns = \"N\" ) group_visit [ \"phenotype\" ] = self . name bd . cache ( group_visit ) self . features [ output_feature ] = group_visit self . logger . info ( f \"Aggregation from { input_feature } stored in self.features[' { output_feature } '] \" f \"(N = { len ( group_visit ) } )\" ) return self agg_two_features agg_two_features ( input_feature_1 : str , input_feature_2 : str , output_feature : str = None , how : str = 'AND' , level : str = 'patient' , subphenotype : bool = True , thresholds : Tuple [ int , int ] = ( 1 , 1 )) -> Phenotype If level='patient', keeps a specific patient if At least thresholds[0] visits are found in feature_1 AND/OR At least thresholds[1] visits are found in feature_2 If level='visit', keeps a specific visit if At least thresholds[0] events are found in feature_1 AND/OR At least thresholds[1] events are found in feature_2 PARAMETER DESCRIPTION input_feature_1 Name of the first input feature TYPE: str input_feature_2 Name of the second input feature TYPE: str output_feature Name of the input feature. If None, will be set to input_feature + \"_agg\" TYPE: str DEFAULT: None how Whether to perform a boolean \"AND\" or \"OR\" aggregation TYPE: str , optional DEFAULT: 'AND' level On which level to do the aggregation, either \"patient\" or \"visit\" TYPE: str DEFAULT: 'patient' subphenotype Whether the threshold should apply to the phenotype (\"phenotype\" column) of the subphenotype (\"subphenotype\" column) TYPE: bool DEFAULT: True thresholds Repsective threshold for the first and second feature TYPE: Tuple [ int , int ], optional DEFAULT: (1, 1) RETURNS DESCRIPTION Phenotype The current Phenotype object with an additional feature stored in self.features[output_feature] Source code in eds_scikit/phenotype/base.py 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 def agg_two_features ( self , input_feature_1 : str , input_feature_2 : str , output_feature : str = None , how : str = \"AND\" , level : str = \"patient\" , subphenotype : bool = True , thresholds : Tuple [ int , int ] = ( 1 , 1 ), ) -> \"Phenotype\" : \"\"\" - If level='patient', keeps a specific patient if - At least `thresholds[0]` visits are found in feature_1 AND/OR - At least `thresholds[1]` visits are found in feature_2 - If level='visit', keeps a specific visit if - At least `thresholds[0]` events are found in feature_1 AND/OR - At least `thresholds[1]` events are found in feature_2 Parameters ---------- input_feature_1 : str Name of the first input feature input_feature_2 : str Name of the second input feature output_feature : str Name of the input feature. If None, will be set to input_feature + \"_agg\" how : str, optional Whether to perform a boolean \"AND\" or \"OR\" aggregation level : str On which level to do the aggregation, either \"patient\" or \"visit\" subphenotype : bool Whether the threshold should apply to the phenotype (\"phenotype\" column) of the subphenotype (\"subphenotype\" column) thresholds : Tuple[int, int], optional Repsective threshold for the first and second feature Returns ------- Phenotype The current Phenotype object with an additional feature stored in self.features[output_feature] \"\"\" self . agg_single_feature ( input_feature = input_feature_1 , level = level , subphenotype = subphenotype , threshold = thresholds [ 0 ], ) self . agg_single_feature ( input_feature = input_feature_2 , level = level , subphenotype = subphenotype , threshold = thresholds [ 1 ], ) results_1 = self . features [ f \" { input_feature_1 } _agg\" ] results_2 = self . features [ f \" { input_feature_2 } _agg\" ] assert set ( results_1 . columns ) == set ( results_2 . columns ) if how == \"AND\" : result = results_1 . merge ( results_2 , on = list ( results_1 . columns ), how = \"inner\" ) elif how == \"OR\" : result = bd . concat ( [ results_1 , results_2 , ] ) . drop_duplicates () else : raise ValueError ( f \"'how' options are ('AND', 'OR'), got { how } .\" ) bd . cache ( result ) output_feature = output_feature or f \" { input_feature_1 } _ { how } _ { input_feature_2 } \" self . features [ output_feature ] = result self . logger . info ( f \"Aggregation from { input_feature_1 } { how } { input_feature_1 } stored in self.features[' { output_feature } '] \" f \"(N = { len ( result ) } )\" ) return self compute compute ( ** kwargs ) Fetch all necessary features and perform aggregation Source code in eds_scikit/phenotype/base.py 325 326 327 328 329 def compute ( self , ** kwargs ): \"\"\" Fetch all necessary features and perform aggregation \"\"\" raise NotImplementedError () to_data to_data ( key : Optional [ str ] = None ) -> BaseData Appends the feature found in self.features[key] to the data object. If no key is provided, uses the last added feature PARAMETER DESCRIPTION key Key of the self.feature dictionary TYPE: Optional [ str ] DEFAULT: None RETURNS DESCRIPTION BaseData The data object with phenotype added to data.computed Source code in eds_scikit/phenotype/base.py 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 def to_data ( self , key : Optional [ str ] = None ) -> BaseData : \"\"\" Appends the feature found in self.features[key] to the data object. If no key is provided, uses the last added feature Parameters ---------- key : Optional[str] Key of the self.feature dictionary Returns ------- BaseData The data object with phenotype added to `data.computed` \"\"\" if not self . features : self . compute () if key is None : self . logger . info ( \"No key provided: Using last added feature.\" ) return self . _set ( self . features . last ()) else : assert ( key in self . features ), f \"Key { key } not found in features. Available { self . features } \" self . logger . info ( \"Using feature {key} \" ) return self . _set ( self . features [ key ]) to_valid_variable_name to_valid_variable_name ( s : str ) Converts a string to a valid variable name Source code in eds_scikit/phenotype/base.py 415 416 417 418 419 420 421 422 423 424 425 426 def to_valid_variable_name ( s : str ): \"\"\" Converts a string to a valid variable name \"\"\" # Replace non-alphanumeric characters with underscores s = re . sub ( r \"\\W+\" , \"_\" , s ) # Remove leading underscores s = re . sub ( r \"^_+\" , \"\" , s ) # If the string is empty or starts with a number, prepend an underscore if not s or s [ 0 ] . isdigit (): s = \"_\" + s return s","title":"base"},{"location":"reference/phenotype/base/#eds_scikitphenotypebase","text":"","title":"eds_scikit.phenotype.base"},{"location":"reference/phenotype/base/#eds_scikit.phenotype.base.Features","text":"Features () Class used to store features (i.e. DataFrames). Features are stored in the self._features dictionary. Source code in eds_scikit/phenotype/base.py 22 23 24 def __init__ ( self ): self . _features = {} self . last_feature = None","title":"Features"},{"location":"reference/phenotype/base/#eds_scikit.phenotype.base.Phenotype","text":"Phenotype ( data : BaseData , name : Optional [ str ] = None , ** kwargs ) Base class for phenotyping PARAMETER DESCRIPTION data A BaseData object TYPE: BaseData name Name of the phenotype. If left to None, the name of the class will be used instead TYPE: Optional [ str ] DEFAULT: None Source code in eds_scikit/phenotype/base.py 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 def __init__ ( self , data : BaseData , name : Optional [ str ] = None , ** kwargs , ): \"\"\" Parameters ---------- data : BaseData A BaseData object name : Optional[str] Name of the phenotype. If left to None, the name of the class will be used instead \"\"\" self . data = data self . features = Features () self . name = ( to_valid_variable_name ( name ) if name is not None else self . __class__ . __name__ ) self . logger = logger . bind ( classname = self . name , sep = \".\" )","title":"Phenotype"},{"location":"reference/phenotype/base/#eds_scikit.phenotype.base.Phenotype.add_code_feature","text":"add_code_feature ( output_feature : str , codes : dict , source : str = 'icd10' , additional_filtering : Optional [ dict ] = None ) Adds a feature from either ICD10 or CCAM codes PARAMETER DESCRIPTION output_feature Name of the feature TYPE: str codes Dictionary of codes to provide to the from_codes function TYPE: dict source Either 'icd10' or 'ccam', by default 'icd10' TYPE: str DEFAULT: 'icd10' additional_filtering Dictionary passed to the from_codes functions for filtering TYPE: Optional [ dict ] DEFAULT: None RETURNS DESCRIPTION Phenotype The current Phenotype object with an additional feature stored in self.features[output_feature] Source code in eds_scikit/phenotype/base.py 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 def add_code_feature ( self , output_feature : str , codes : dict , source : str = \"icd10\" , additional_filtering : Optional [ dict ] = None , ): \"\"\" Adds a feature from either ICD10 or CCAM codes Parameters ---------- output_feature : str Name of the feature codes : dict Dictionary of codes to provide to the `from_codes` function source : str, Either 'icd10' or 'ccam', by default 'icd10' additional_filtering : Optional[dict] Dictionary passed to the `from_codes` functions for filtering Returns ------- Phenotype The current Phenotype object with an additional feature stored in self.features[output_feature] \"\"\" additional_filtering = additional_filtering or dict () if source not in [ \"icd10\" , \"ccam\" ]: raise ValueError ( f \"source should be either 'icd10' or 'ccam', got { source } \" ) self . logger . info ( f \"Getting { source . upper () } features...\" ) from_code_func = ( conditions_from_icd10 if ( source == \"icd10\" ) else procedures_from_ccam ) codes_df = ( self . data . condition_occurrence if ( source == \"icd10\" ) else self . data . procedure_occurrence ) df = from_code_func ( codes_df , codes = codes , additional_filtering = additional_filtering , date_from_visit = False , ) df [ \"phenotype\" ] = self . name df = df . rename ( columns = { \"concept\" : \"subphenotype\" }) bd . cache ( df ) self . features [ output_feature ] = df self . logger . info ( f \" { source . upper () } features stored in self.features[' { output_feature } '] (N = { len ( df ) } )\" ) return self","title":"add_code_feature()"},{"location":"reference/phenotype/base/#eds_scikit.phenotype.base.Phenotype.agg_single_feature","text":"agg_single_feature ( input_feature : str , output_feature : Optional [ str ] = None , level : str = 'patient' , subphenotype : bool = True , threshold : int = 1 ) -> Phenotype Simple aggregation rule on a feature: If level=\"patient\", keeps patients with at least threshold visits showing the (sub)phenotype If level=\"visit\", keeps visits with at least threshold events (could be ICD10 codes, NLP features, biology, etc) showing the (sub)phenotype PARAMETER DESCRIPTION input_feature Name of the input feature TYPE: str output_feature Name of the input feature. If None, will be set to input_feature + \"_agg\" TYPE: Optional [ str ] DEFAULT: None level On which level to do the aggregation, either \"patient\" or \"visit\" TYPE: str DEFAULT: 'patient' subphenotype Whether the threshold should apply to the phenotype (\"phenotype\" column) of the subphenotype (\"subphenotype\" column) TYPE: bool DEFAULT: True threshold Minimal number of events (which definition depends on the level value) TYPE: int , optional DEFAULT: 1 RETURNS DESCRIPTION Phenotype The current Phenotype object with an additional feature stored in self.features[output_feature] Source code in eds_scikit/phenotype/base.py 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 def agg_single_feature ( self , input_feature : str , output_feature : Optional [ str ] = None , level : str = \"patient\" , subphenotype : bool = True , threshold : int = 1 , ) -> \"Phenotype\" : \"\"\" Simple aggregation rule on a feature: - If level=\"patient\", keeps patients with at least `threshold` visits showing the (sub)phenotype - If level=\"visit\", keeps visits with at least `threshold` events (could be ICD10 codes, NLP features, biology, etc) showing the (sub)phenotype Parameters ---------- input_feature : str Name of the input feature output_feature : Optional[str] Name of the input feature. If None, will be set to input_feature + \"_agg\" level : str On which level to do the aggregation, either \"patient\" or \"visit\" subphenotype : bool Whether the threshold should apply to the phenotype (\"phenotype\" column) of the subphenotype (\"subphenotype\" column) threshold : int, optional Minimal number of *events* (which definition depends on the `level` value) Returns ------- Phenotype The current Phenotype object with an additional feature stored in self.features[output_feature] \"\"\" assert level in { \"patient\" , \"visit\" } output_feature = output_feature or f \" { input_feature } _agg\" if input_feature not in self . features : raise ValueError ( f \"Input feature { input_feature } not found in self.features. \" \"Maybe you forgot to call self.get_features() ?\" ) # We use `size` below for two reasons # 1) to use it with the `threshold` parameter directly if level == 'visit' # 2) to drop duplicates on the group_cols + [\"visit_occurrence_id\"] subset phenotype_type = \"subphenotype\" if subphenotype else \"phenotype\" group_cols = [ \"person_id\" , phenotype_type ] group_visit = ( self . features [ input_feature ] . groupby ( group_cols + [ \"visit_occurrence_id\" ]) . size () . rename ( \"N\" ) # number of events per visit_occurrence . reset_index () ) if level == \"patient\" : group_visit = ( group_visit . groupby ( group_cols ) . size () . rename ( \"N\" ) # number of visits per person . reset_index () ) group_visit = group_visit [ group_visit [ \"N\" ] >= threshold ] . drop ( columns = \"N\" ) group_visit [ \"phenotype\" ] = self . name bd . cache ( group_visit ) self . features [ output_feature ] = group_visit self . logger . info ( f \"Aggregation from { input_feature } stored in self.features[' { output_feature } '] \" f \"(N = { len ( group_visit ) } )\" ) return self","title":"agg_single_feature()"},{"location":"reference/phenotype/base/#eds_scikit.phenotype.base.Phenotype.agg_two_features","text":"agg_two_features ( input_feature_1 : str , input_feature_2 : str , output_feature : str = None , how : str = 'AND' , level : str = 'patient' , subphenotype : bool = True , thresholds : Tuple [ int , int ] = ( 1 , 1 )) -> Phenotype If level='patient', keeps a specific patient if At least thresholds[0] visits are found in feature_1 AND/OR At least thresholds[1] visits are found in feature_2 If level='visit', keeps a specific visit if At least thresholds[0] events are found in feature_1 AND/OR At least thresholds[1] events are found in feature_2 PARAMETER DESCRIPTION input_feature_1 Name of the first input feature TYPE: str input_feature_2 Name of the second input feature TYPE: str output_feature Name of the input feature. If None, will be set to input_feature + \"_agg\" TYPE: str DEFAULT: None how Whether to perform a boolean \"AND\" or \"OR\" aggregation TYPE: str , optional DEFAULT: 'AND' level On which level to do the aggregation, either \"patient\" or \"visit\" TYPE: str DEFAULT: 'patient' subphenotype Whether the threshold should apply to the phenotype (\"phenotype\" column) of the subphenotype (\"subphenotype\" column) TYPE: bool DEFAULT: True thresholds Repsective threshold for the first and second feature TYPE: Tuple [ int , int ], optional DEFAULT: (1, 1) RETURNS DESCRIPTION Phenotype The current Phenotype object with an additional feature stored in self.features[output_feature] Source code in eds_scikit/phenotype/base.py 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 def agg_two_features ( self , input_feature_1 : str , input_feature_2 : str , output_feature : str = None , how : str = \"AND\" , level : str = \"patient\" , subphenotype : bool = True , thresholds : Tuple [ int , int ] = ( 1 , 1 ), ) -> \"Phenotype\" : \"\"\" - If level='patient', keeps a specific patient if - At least `thresholds[0]` visits are found in feature_1 AND/OR - At least `thresholds[1]` visits are found in feature_2 - If level='visit', keeps a specific visit if - At least `thresholds[0]` events are found in feature_1 AND/OR - At least `thresholds[1]` events are found in feature_2 Parameters ---------- input_feature_1 : str Name of the first input feature input_feature_2 : str Name of the second input feature output_feature : str Name of the input feature. If None, will be set to input_feature + \"_agg\" how : str, optional Whether to perform a boolean \"AND\" or \"OR\" aggregation level : str On which level to do the aggregation, either \"patient\" or \"visit\" subphenotype : bool Whether the threshold should apply to the phenotype (\"phenotype\" column) of the subphenotype (\"subphenotype\" column) thresholds : Tuple[int, int], optional Repsective threshold for the first and second feature Returns ------- Phenotype The current Phenotype object with an additional feature stored in self.features[output_feature] \"\"\" self . agg_single_feature ( input_feature = input_feature_1 , level = level , subphenotype = subphenotype , threshold = thresholds [ 0 ], ) self . agg_single_feature ( input_feature = input_feature_2 , level = level , subphenotype = subphenotype , threshold = thresholds [ 1 ], ) results_1 = self . features [ f \" { input_feature_1 } _agg\" ] results_2 = self . features [ f \" { input_feature_2 } _agg\" ] assert set ( results_1 . columns ) == set ( results_2 . columns ) if how == \"AND\" : result = results_1 . merge ( results_2 , on = list ( results_1 . columns ), how = \"inner\" ) elif how == \"OR\" : result = bd . concat ( [ results_1 , results_2 , ] ) . drop_duplicates () else : raise ValueError ( f \"'how' options are ('AND', 'OR'), got { how } .\" ) bd . cache ( result ) output_feature = output_feature or f \" { input_feature_1 } _ { how } _ { input_feature_2 } \" self . features [ output_feature ] = result self . logger . info ( f \"Aggregation from { input_feature_1 } { how } { input_feature_1 } stored in self.features[' { output_feature } '] \" f \"(N = { len ( result ) } )\" ) return self","title":"agg_two_features()"},{"location":"reference/phenotype/base/#eds_scikit.phenotype.base.Phenotype.compute","text":"compute ( ** kwargs ) Fetch all necessary features and perform aggregation Source code in eds_scikit/phenotype/base.py 325 326 327 328 329 def compute ( self , ** kwargs ): \"\"\" Fetch all necessary features and perform aggregation \"\"\" raise NotImplementedError ()","title":"compute()"},{"location":"reference/phenotype/base/#eds_scikit.phenotype.base.Phenotype.to_data","text":"to_data ( key : Optional [ str ] = None ) -> BaseData Appends the feature found in self.features[key] to the data object. If no key is provided, uses the last added feature PARAMETER DESCRIPTION key Key of the self.feature dictionary TYPE: Optional [ str ] DEFAULT: None RETURNS DESCRIPTION BaseData The data object with phenotype added to data.computed Source code in eds_scikit/phenotype/base.py 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 def to_data ( self , key : Optional [ str ] = None ) -> BaseData : \"\"\" Appends the feature found in self.features[key] to the data object. If no key is provided, uses the last added feature Parameters ---------- key : Optional[str] Key of the self.feature dictionary Returns ------- BaseData The data object with phenotype added to `data.computed` \"\"\" if not self . features : self . compute () if key is None : self . logger . info ( \"No key provided: Using last added feature.\" ) return self . _set ( self . features . last ()) else : assert ( key in self . features ), f \"Key { key } not found in features. Available { self . features } \" self . logger . info ( \"Using feature {key} \" ) return self . _set ( self . features [ key ])","title":"to_data()"},{"location":"reference/phenotype/base/#eds_scikit.phenotype.base.to_valid_variable_name","text":"to_valid_variable_name ( s : str ) Converts a string to a valid variable name Source code in eds_scikit/phenotype/base.py 415 416 417 418 419 420 421 422 423 424 425 426 def to_valid_variable_name ( s : str ): \"\"\" Converts a string to a valid variable name \"\"\" # Replace non-alphanumeric characters with underscores s = re . sub ( r \"\\W+\" , \"_\" , s ) # Remove leading underscores s = re . sub ( r \"^_+\" , \"\" , s ) # If the string is empty or starts with a number, prepend an underscore if not s or s [ 0 ] . isdigit (): s = \"_\" + s return s","title":"to_valid_variable_name()"},{"location":"reference/phenotype/cancer/","text":"eds_scikit.phenotype.cancer","title":"`eds_scikit.phenotype.cancer`"},{"location":"reference/phenotype/cancer/#eds_scikitphenotypecancer","text":"","title":"eds_scikit.phenotype.cancer"},{"location":"reference/phenotype/cancer/cancer/","text":"eds_scikit.phenotype.cancer.cancer CancerFromICD10 CancerFromICD10 ( data : BaseData , cancer_types : Optional [ List [ str ]] = None , level : str = 'patient' , subphenotype : bool = True , threshold : int = 1 ) Bases: Phenotype Phenotyping visits or patients using ICD10 cancer codes PARAMETER DESCRIPTION data A BaseData object TYPE: BaseData cancer_types Optional list of cancer types to use for phenotyping TYPE: Optional[List[str]] DEFAULT: None level On which level to do the aggregation, either \"patient\" or \"visit\" TYPE: str DEFAULT: 'patient' subphenotype Whether the threshold should apply to the phenotype (\"phenotype\" column) of the subphenotype (\"subphenotype\" column) TYPE: bool DEFAULT: True threshold Minimal number of events (which definition depends on the level value) TYPE: int DEFAULT: 1 Source code in eds_scikit/phenotype/cancer/cancer.py 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 def __init__ ( self , data : BaseData , cancer_types : Optional [ List [ str ]] = None , level : str = \"patient\" , subphenotype : bool = True , threshold : int = 1 , ): \"\"\" Parameters ---------- data : BaseData A BaseData object cancer_types : Optional[List[str]] Optional list of cancer types to use for phenotyping level : str On which level to do the aggregation, either \"patient\" or \"visit\" subphenotype : bool Whether the threshold should apply to the phenotype (\"phenotype\" column) of the subphenotype (\"subphenotype\" column) threshold : int Minimal number of *events* (which definition depends on the `level` value) \"\"\" super () . __init__ ( data ) if cancer_types is None : cancer_types = self . ALL_CANCER_TYPES incorrect_cancer_types = set ( cancer_types ) - set ( self . ALL_CANCER_TYPES ) if incorrect_cancer_types : raise ValueError ( f \"Incorrect cancer types ( { incorrect_cancer_types } ). \" f \"Available cancer types are { self . ALL_CANCER_TYPES } \" ) self . icd10_codes = { k : v for k , v in self . ICD10_CODES . items () if k in cancer_types } self . level = level self . subphenotype = subphenotype self . threshold = threshold ICD10_CODES class-attribute ICD10_CODES = { cancer_type : { 'prefix' : df . code . to_list ()} for ( cancer_type , df ) in ICD10_CODES_DF . groupby ( 'Cancer type' )} For each cancer type, contains a set of corresponding ICD10 codes. ALL_CANCER_TYPES class-attribute ALL_CANCER_TYPES = list ( ICD10_CODES . keys ()) Available cancer types. compute compute () Fetch all necessary features and perform aggregation Source code in eds_scikit/phenotype/cancer/cancer.py 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 def compute ( self ): \"\"\" Fetch all necessary features and perform aggregation \"\"\" self . add_code_feature ( output_feature = \"icd10\" , source = \"icd10\" , codes = self . icd10_codes , additional_filtering = dict ( condition_status_source_value = { \"DP\" , \"DR\" }), ) self . agg_single_feature ( input_feature = \"icd10\" , level = self . level , subphenotype = self . subphenotype , threshold = self . threshold , )","title":"cancer"},{"location":"reference/phenotype/cancer/cancer/#eds_scikitphenotypecancercancer","text":"","title":"eds_scikit.phenotype.cancer.cancer"},{"location":"reference/phenotype/cancer/cancer/#eds_scikit.phenotype.cancer.cancer.CancerFromICD10","text":"CancerFromICD10 ( data : BaseData , cancer_types : Optional [ List [ str ]] = None , level : str = 'patient' , subphenotype : bool = True , threshold : int = 1 ) Bases: Phenotype Phenotyping visits or patients using ICD10 cancer codes PARAMETER DESCRIPTION data A BaseData object TYPE: BaseData cancer_types Optional list of cancer types to use for phenotyping TYPE: Optional[List[str]] DEFAULT: None level On which level to do the aggregation, either \"patient\" or \"visit\" TYPE: str DEFAULT: 'patient' subphenotype Whether the threshold should apply to the phenotype (\"phenotype\" column) of the subphenotype (\"subphenotype\" column) TYPE: bool DEFAULT: True threshold Minimal number of events (which definition depends on the level value) TYPE: int DEFAULT: 1 Source code in eds_scikit/phenotype/cancer/cancer.py 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 def __init__ ( self , data : BaseData , cancer_types : Optional [ List [ str ]] = None , level : str = \"patient\" , subphenotype : bool = True , threshold : int = 1 , ): \"\"\" Parameters ---------- data : BaseData A BaseData object cancer_types : Optional[List[str]] Optional list of cancer types to use for phenotyping level : str On which level to do the aggregation, either \"patient\" or \"visit\" subphenotype : bool Whether the threshold should apply to the phenotype (\"phenotype\" column) of the subphenotype (\"subphenotype\" column) threshold : int Minimal number of *events* (which definition depends on the `level` value) \"\"\" super () . __init__ ( data ) if cancer_types is None : cancer_types = self . ALL_CANCER_TYPES incorrect_cancer_types = set ( cancer_types ) - set ( self . ALL_CANCER_TYPES ) if incorrect_cancer_types : raise ValueError ( f \"Incorrect cancer types ( { incorrect_cancer_types } ). \" f \"Available cancer types are { self . ALL_CANCER_TYPES } \" ) self . icd10_codes = { k : v for k , v in self . ICD10_CODES . items () if k in cancer_types } self . level = level self . subphenotype = subphenotype self . threshold = threshold","title":"CancerFromICD10"},{"location":"reference/phenotype/cancer/cancer/#eds_scikit.phenotype.cancer.cancer.CancerFromICD10.ICD10_CODES","text":"ICD10_CODES = { cancer_type : { 'prefix' : df . code . to_list ()} for ( cancer_type , df ) in ICD10_CODES_DF . groupby ( 'Cancer type' )} For each cancer type, contains a set of corresponding ICD10 codes.","title":"ICD10_CODES"},{"location":"reference/phenotype/cancer/cancer/#eds_scikit.phenotype.cancer.cancer.CancerFromICD10.ALL_CANCER_TYPES","text":"ALL_CANCER_TYPES = list ( ICD10_CODES . keys ()) Available cancer types.","title":"ALL_CANCER_TYPES"},{"location":"reference/phenotype/cancer/cancer/#eds_scikit.phenotype.cancer.cancer.CancerFromICD10.compute","text":"compute () Fetch all necessary features and perform aggregation Source code in eds_scikit/phenotype/cancer/cancer.py 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 def compute ( self ): \"\"\" Fetch all necessary features and perform aggregation \"\"\" self . add_code_feature ( output_feature = \"icd10\" , source = \"icd10\" , codes = self . icd10_codes , additional_filtering = dict ( condition_status_source_value = { \"DP\" , \"DR\" }), ) self . agg_single_feature ( input_feature = \"icd10\" , level = self . level , subphenotype = self . subphenotype , threshold = self . threshold , )","title":"compute()"},{"location":"reference/phenotype/diabetes/","text":"eds_scikit.phenotype.diabetes","title":"`eds_scikit.phenotype.diabetes`"},{"location":"reference/phenotype/diabetes/#eds_scikitphenotypediabetes","text":"","title":"eds_scikit.phenotype.diabetes"},{"location":"reference/phenotype/diabetes/diabetes/","text":"eds_scikit.phenotype.diabetes.diabetes DiabetesFromICD10 DiabetesFromICD10 ( data , diabetes_types : Optional [ List [ str ]] = None , level : str = 'visit' , subphenotype : bool = True , threshold : int = 1 ) Bases: Phenotype Phenotyping visits or patients using ICD10 diabetes codes PARAMETER DESCRIPTION data A BaseData object TYPE: BaseData diabetes_types Optional list of diabetes types to use for phenotyping TYPE: Optional[List[str]] DEFAULT: None level On which level to do the aggregation, either \"patient\" or \"visit\" TYPE: str DEFAULT: 'visit' subphenotype Whether the threshold should apply to the phenotype (\"phenotype\" column) of the subphenotype (\"subphenotype\" column) TYPE: bool DEFAULT: True threshold Minimal number of events (which definition depends on the level value) TYPE: int DEFAULT: 1 Source code in eds_scikit/phenotype/diabetes/diabetes.py 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 def __init__ ( self , data , diabetes_types : Optional [ List [ str ]] = None , level : str = \"visit\" , subphenotype : bool = True , threshold : int = 1 , ): \"\"\" Parameters ---------- data : BaseData A BaseData object diabetes_types : Optional[List[str]] Optional list of diabetes types to use for phenotyping level : str On which level to do the aggregation, either \"patient\" or \"visit\" subphenotype : bool Whether the threshold should apply to the phenotype (\"phenotype\" column) of the subphenotype (\"subphenotype\" column) threshold : int Minimal number of *events* (which definition depends on the `level` value) \"\"\" super () . __init__ ( data ) if diabetes_types is None : diabetes_types = self . ALL_DIABETES_TYPES incorrect_diabetes_types = set ( diabetes_types ) - set ( self . ALL_DIABETES_TYPES ) if incorrect_diabetes_types : raise ValueError ( f \"Incorrect diabetes types ( { incorrect_diabetes_types } ). \" f \"Available diabetes types are { self . ALL_DIABETES_TYPES } \" ) self . icd10_codes = { k : v for k , v in self . ICD10_CODES . items () if k in diabetes_types } self . level = level self . subphenotype = subphenotype self . threshold = threshold ALL_DIABETES_TYPES class-attribute ALL_DIABETES_TYPES = list ( ICD10_CODES . keys ()) Available diabetes types. compute compute () Fetch all necessary features and perform aggregation Source code in eds_scikit/phenotype/diabetes/diabetes.py 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 def compute ( self ): \"\"\" Fetch all necessary features and perform aggregation \"\"\" self . add_code_feature ( output_feature = \"icd10\" , source = \"icd10\" , codes = self . ICD10_CODES , additional_filtering = dict ( condition_status_source_value = { \"DP\" , \"DAS\" }), ) self . agg_single_feature ( input_feature = \"icd10\" , level = self . level , subphenotype = self . subphenotype , threshold = self . threshold , )","title":"diabetes"},{"location":"reference/phenotype/diabetes/diabetes/#eds_scikitphenotypediabetesdiabetes","text":"","title":"eds_scikit.phenotype.diabetes.diabetes"},{"location":"reference/phenotype/diabetes/diabetes/#eds_scikit.phenotype.diabetes.diabetes.DiabetesFromICD10","text":"DiabetesFromICD10 ( data , diabetes_types : Optional [ List [ str ]] = None , level : str = 'visit' , subphenotype : bool = True , threshold : int = 1 ) Bases: Phenotype Phenotyping visits or patients using ICD10 diabetes codes PARAMETER DESCRIPTION data A BaseData object TYPE: BaseData diabetes_types Optional list of diabetes types to use for phenotyping TYPE: Optional[List[str]] DEFAULT: None level On which level to do the aggregation, either \"patient\" or \"visit\" TYPE: str DEFAULT: 'visit' subphenotype Whether the threshold should apply to the phenotype (\"phenotype\" column) of the subphenotype (\"subphenotype\" column) TYPE: bool DEFAULT: True threshold Minimal number of events (which definition depends on the level value) TYPE: int DEFAULT: 1 Source code in eds_scikit/phenotype/diabetes/diabetes.py 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 def __init__ ( self , data , diabetes_types : Optional [ List [ str ]] = None , level : str = \"visit\" , subphenotype : bool = True , threshold : int = 1 , ): \"\"\" Parameters ---------- data : BaseData A BaseData object diabetes_types : Optional[List[str]] Optional list of diabetes types to use for phenotyping level : str On which level to do the aggregation, either \"patient\" or \"visit\" subphenotype : bool Whether the threshold should apply to the phenotype (\"phenotype\" column) of the subphenotype (\"subphenotype\" column) threshold : int Minimal number of *events* (which definition depends on the `level` value) \"\"\" super () . __init__ ( data ) if diabetes_types is None : diabetes_types = self . ALL_DIABETES_TYPES incorrect_diabetes_types = set ( diabetes_types ) - set ( self . ALL_DIABETES_TYPES ) if incorrect_diabetes_types : raise ValueError ( f \"Incorrect diabetes types ( { incorrect_diabetes_types } ). \" f \"Available diabetes types are { self . ALL_DIABETES_TYPES } \" ) self . icd10_codes = { k : v for k , v in self . ICD10_CODES . items () if k in diabetes_types } self . level = level self . subphenotype = subphenotype self . threshold = threshold","title":"DiabetesFromICD10"},{"location":"reference/phenotype/diabetes/diabetes/#eds_scikit.phenotype.diabetes.diabetes.DiabetesFromICD10.ALL_DIABETES_TYPES","text":"ALL_DIABETES_TYPES = list ( ICD10_CODES . keys ()) Available diabetes types.","title":"ALL_DIABETES_TYPES"},{"location":"reference/phenotype/diabetes/diabetes/#eds_scikit.phenotype.diabetes.diabetes.DiabetesFromICD10.compute","text":"compute () Fetch all necessary features and perform aggregation Source code in eds_scikit/phenotype/diabetes/diabetes.py 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 def compute ( self ): \"\"\" Fetch all necessary features and perform aggregation \"\"\" self . add_code_feature ( output_feature = \"icd10\" , source = \"icd10\" , codes = self . ICD10_CODES , additional_filtering = dict ( condition_status_source_value = { \"DP\" , \"DAS\" }), ) self . agg_single_feature ( input_feature = \"icd10\" , level = self . level , subphenotype = self . subphenotype , threshold = self . threshold , )","title":"compute()"},{"location":"reference/phenotype/psychiatric_disorder/","text":"eds_scikit.phenotype.psychiatric_disorder","title":"`eds_scikit.phenotype.psychiatric_disorder`"},{"location":"reference/phenotype/psychiatric_disorder/#eds_scikitphenotypepsychiatric_disorder","text":"","title":"eds_scikit.phenotype.psychiatric_disorder"},{"location":"reference/phenotype/psychiatric_disorder/psychiatric_disorder/","text":"eds_scikit.phenotype.psychiatric_disorder.psychiatric_disorder PsychiatricDisorderFromICD10 PsychiatricDisorderFromICD10 ( data , disorder_types : Optional [ List [ str ]] = None , level : str = 'patient' , subphenotype : bool = True , threshold : int = 1 ) Bases: Phenotype Phenotyping visits or patients with psychiatric disorders using ICD10 codes PARAMETER DESCRIPTION data A BaseData object TYPE: BaseData disorder_types Optional list of disorder types to use for phenotyping TYPE: Optional[List[str]] DEFAULT: None level On which level to do the aggregation, either \"patient\" or \"visit\" TYPE: str DEFAULT: 'patient' subphenotype Whether the threshold should apply to the phenotype (\"phenotype\" column) of the subphenotype (\"subphenotype\" column) TYPE: bool DEFAULT: True threshold Minimal number of events (which definition depends on the level value) TYPE: int DEFAULT: 1 Source code in eds_scikit/phenotype/psychiatric_disorder/psychiatric_disorder.py 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 def __init__ ( self , data , disorder_types : Optional [ List [ str ]] = None , level : str = \"patient\" , subphenotype : bool = True , threshold : int = 1 , ): \"\"\" Parameters ---------- data : BaseData A BaseData object disorder_types : Optional[List[str]] Optional list of disorder types to use for phenotyping level : str On which level to do the aggregation, either \"patient\" or \"visit\" subphenotype : bool Whether the threshold should apply to the phenotype (\"phenotype\" column) of the subphenotype (\"subphenotype\" column) threshold : int Minimal number of *events* (which definition depends on the `level` value) \"\"\" super () . __init__ ( data ) if disorder_types is None : disorder_types = self . ALL_DISORDER_TYPES incorrect_disorder_types = set ( disorder_types ) - set ( self . ALL_DISORDER_TYPES ) if incorrect_disorder_types : raise ValueError ( f \"Incorrect cancer types ( { incorrect_disorder_types } ). \" f \"Available cancer types are { self . ALL_DISORDER_TYPES } \" ) self . icd10_codes = { k : v for k , v in self . ICD10_CODES . items () if k in disorder_types } self . level = level self . subphenotype = subphenotype self . threshold = threshold ICD10_CODES class-attribute ICD10_CODES = { disorder_group : { 'exact' : df . ICD10_Code . to_list ()} for ( disorder_group , df ) in ICD10_CODES_DF . groupby ( 'disorder_group' )} ICD10 codes used for phenotyping ALL_DISORDER_TYPES class-attribute ALL_DISORDER_TYPES = list ( ICD10_CODES . keys ()) Available disorder types. compute compute () Fetch all necessary features and perform aggregation Source code in eds_scikit/phenotype/psychiatric_disorder/psychiatric_disorder.py 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 def compute ( self ): \"\"\" Fetch all necessary features and perform aggregation \"\"\" self . add_code_feature ( output_feature = \"icd10\" , source = \"icd10\" , codes = self . ICD10_CODES , ) self . agg_single_feature ( input_feature = \"icd10\" , level = self . level , subphenotype = self . subphenotype , threshold = self . threshold , )","title":"psychiatric_disorder"},{"location":"reference/phenotype/psychiatric_disorder/psychiatric_disorder/#eds_scikitphenotypepsychiatric_disorderpsychiatric_disorder","text":"","title":"eds_scikit.phenotype.psychiatric_disorder.psychiatric_disorder"},{"location":"reference/phenotype/psychiatric_disorder/psychiatric_disorder/#eds_scikit.phenotype.psychiatric_disorder.psychiatric_disorder.PsychiatricDisorderFromICD10","text":"PsychiatricDisorderFromICD10 ( data , disorder_types : Optional [ List [ str ]] = None , level : str = 'patient' , subphenotype : bool = True , threshold : int = 1 ) Bases: Phenotype Phenotyping visits or patients with psychiatric disorders using ICD10 codes PARAMETER DESCRIPTION data A BaseData object TYPE: BaseData disorder_types Optional list of disorder types to use for phenotyping TYPE: Optional[List[str]] DEFAULT: None level On which level to do the aggregation, either \"patient\" or \"visit\" TYPE: str DEFAULT: 'patient' subphenotype Whether the threshold should apply to the phenotype (\"phenotype\" column) of the subphenotype (\"subphenotype\" column) TYPE: bool DEFAULT: True threshold Minimal number of events (which definition depends on the level value) TYPE: int DEFAULT: 1 Source code in eds_scikit/phenotype/psychiatric_disorder/psychiatric_disorder.py 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 def __init__ ( self , data , disorder_types : Optional [ List [ str ]] = None , level : str = \"patient\" , subphenotype : bool = True , threshold : int = 1 , ): \"\"\" Parameters ---------- data : BaseData A BaseData object disorder_types : Optional[List[str]] Optional list of disorder types to use for phenotyping level : str On which level to do the aggregation, either \"patient\" or \"visit\" subphenotype : bool Whether the threshold should apply to the phenotype (\"phenotype\" column) of the subphenotype (\"subphenotype\" column) threshold : int Minimal number of *events* (which definition depends on the `level` value) \"\"\" super () . __init__ ( data ) if disorder_types is None : disorder_types = self . ALL_DISORDER_TYPES incorrect_disorder_types = set ( disorder_types ) - set ( self . ALL_DISORDER_TYPES ) if incorrect_disorder_types : raise ValueError ( f \"Incorrect cancer types ( { incorrect_disorder_types } ). \" f \"Available cancer types are { self . ALL_DISORDER_TYPES } \" ) self . icd10_codes = { k : v for k , v in self . ICD10_CODES . items () if k in disorder_types } self . level = level self . subphenotype = subphenotype self . threshold = threshold","title":"PsychiatricDisorderFromICD10"},{"location":"reference/phenotype/psychiatric_disorder/psychiatric_disorder/#eds_scikit.phenotype.psychiatric_disorder.psychiatric_disorder.PsychiatricDisorderFromICD10.ICD10_CODES","text":"ICD10_CODES = { disorder_group : { 'exact' : df . ICD10_Code . to_list ()} for ( disorder_group , df ) in ICD10_CODES_DF . groupby ( 'disorder_group' )} ICD10 codes used for phenotyping","title":"ICD10_CODES"},{"location":"reference/phenotype/psychiatric_disorder/psychiatric_disorder/#eds_scikit.phenotype.psychiatric_disorder.psychiatric_disorder.PsychiatricDisorderFromICD10.ALL_DISORDER_TYPES","text":"ALL_DISORDER_TYPES = list ( ICD10_CODES . keys ()) Available disorder types.","title":"ALL_DISORDER_TYPES"},{"location":"reference/phenotype/psychiatric_disorder/psychiatric_disorder/#eds_scikit.phenotype.psychiatric_disorder.psychiatric_disorder.PsychiatricDisorderFromICD10.compute","text":"compute () Fetch all necessary features and perform aggregation Source code in eds_scikit/phenotype/psychiatric_disorder/psychiatric_disorder.py 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 def compute ( self ): \"\"\" Fetch all necessary features and perform aggregation \"\"\" self . add_code_feature ( output_feature = \"icd10\" , source = \"icd10\" , codes = self . ICD10_CODES , ) self . agg_single_feature ( input_feature = \"icd10\" , level = self . level , subphenotype = self . subphenotype , threshold = self . threshold , )","title":"compute()"},{"location":"reference/phenotype/suicide_attempt/","text":"eds_scikit.phenotype.suicide_attempt","title":"`eds_scikit.phenotype.suicide_attempt`"},{"location":"reference/phenotype/suicide_attempt/#eds_scikitphenotypesuicide_attempt","text":"","title":"eds_scikit.phenotype.suicide_attempt"},{"location":"reference/phenotype/suicide_attempt/suicide_attempt/","text":"eds_scikit.phenotype.suicide_attempt.suicide_attempt SuicideAttemptFromICD10 SuicideAttemptFromICD10 ( data : BaseData , algo : str = 'Haguenoer2008' ) Bases: Phenotype Phenotyping visits related to a suicide attempt. Two algorithms are available: \"X60-X84\": The visit needs to have at least one ICD10 code in the range X60 to X84 \"Haguenoer2008\": The visit needs to have at least one ICD10 DAS code in the range X60 to X84, and a ICD10 DP code in the range S to T PARAMETER DESCRIPTION data A BaseData object TYPE: BaseData algo The name of the algorithm. Should be either \"Haguenoer2008\" or \"X60-X84\" TYPE: str , optional DEFAULT: 'Haguenoer2008' Source code in eds_scikit/phenotype/suicide_attempt/suicide_attempt.py 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 def __init__ ( self , data : BaseData , algo : str = \"Haguenoer2008\" , ): \"\"\" Parameters ---------- data : BaseData A BaseData object algo : str, optional The name of the algorithm. Should be either \"Haguenoer2008\" or \"X60-X84\" \"\"\" super () . __init__ ( data , name = f \"SuicideAttemptFromICD10_ { algo } \" , ) self . algo = algo ICD10_CODES class-attribute ICD10_CODES = { 'X60-X84' : dict ( codes = { 'X60-X84' : dict ( regex = [ 'X[67]' , 'X8[0-4]' ])}), 'Haguenoer2008' : dict ( codes = { 'Haguenoer2008' : dict ( regex = [ 'S' , 'T[0-9]' ])}, additional_filtering = dict ( condition_status_source_value = 'DP' ))} ICD10 codes used by both algorithms compute compute () Fetch and aggregate features Source code in eds_scikit/phenotype/suicide_attempt/suicide_attempt.py 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 def compute ( self ): \"\"\" Fetch and aggregate features \"\"\" if self . algo == \"X60-X84\" : self . add_code_feature ( output_feature = \"X60-X84\" , source = \"icd10\" , codes = self . ICD10_CODES [ \"X60-X84\" ][ \"codes\" ], ) self . agg_single_feature ( \"X60-X84\" , level = \"visit\" , subphenotype = False , threshold = 1 , ) elif self . algo == \"Haguenoer2008\" : self . add_code_feature ( output_feature = \"X60-X84\" , source = \"icd10\" , codes = self . ICD10_CODES [ \"X60-X84\" ][ \"codes\" ], additional_filtering = dict ( condition_status_source_value = \"DAS\" ), ) self . add_code_feature ( output_feature = \"DP\" , source = \"icd10\" , codes = self . ICD10_CODES [ \"Haguenoer2008\" ][ \"codes\" ], additional_filtering = self . ICD10_CODES [ \"Haguenoer2008\" ][ \"additional_filtering\" ], ) self . agg_two_features ( \"X60-X84\" , \"DP\" , output_feature = \"Haguenoer2008\" , how = \"AND\" , level = \"visit\" , subphenotype = False , thresholds = ( 1 , 1 ), )","title":"suicide_attempt"},{"location":"reference/phenotype/suicide_attempt/suicide_attempt/#eds_scikitphenotypesuicide_attemptsuicide_attempt","text":"","title":"eds_scikit.phenotype.suicide_attempt.suicide_attempt"},{"location":"reference/phenotype/suicide_attempt/suicide_attempt/#eds_scikit.phenotype.suicide_attempt.suicide_attempt.SuicideAttemptFromICD10","text":"SuicideAttemptFromICD10 ( data : BaseData , algo : str = 'Haguenoer2008' ) Bases: Phenotype Phenotyping visits related to a suicide attempt. Two algorithms are available: \"X60-X84\": The visit needs to have at least one ICD10 code in the range X60 to X84 \"Haguenoer2008\": The visit needs to have at least one ICD10 DAS code in the range X60 to X84, and a ICD10 DP code in the range S to T PARAMETER DESCRIPTION data A BaseData object TYPE: BaseData algo The name of the algorithm. Should be either \"Haguenoer2008\" or \"X60-X84\" TYPE: str , optional DEFAULT: 'Haguenoer2008' Source code in eds_scikit/phenotype/suicide_attempt/suicide_attempt.py 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 def __init__ ( self , data : BaseData , algo : str = \"Haguenoer2008\" , ): \"\"\" Parameters ---------- data : BaseData A BaseData object algo : str, optional The name of the algorithm. Should be either \"Haguenoer2008\" or \"X60-X84\" \"\"\" super () . __init__ ( data , name = f \"SuicideAttemptFromICD10_ { algo } \" , ) self . algo = algo","title":"SuicideAttemptFromICD10"},{"location":"reference/phenotype/suicide_attempt/suicide_attempt/#eds_scikit.phenotype.suicide_attempt.suicide_attempt.SuicideAttemptFromICD10.ICD10_CODES","text":"ICD10_CODES = { 'X60-X84' : dict ( codes = { 'X60-X84' : dict ( regex = [ 'X[67]' , 'X8[0-4]' ])}), 'Haguenoer2008' : dict ( codes = { 'Haguenoer2008' : dict ( regex = [ 'S' , 'T[0-9]' ])}, additional_filtering = dict ( condition_status_source_value = 'DP' ))} ICD10 codes used by both algorithms","title":"ICD10_CODES"},{"location":"reference/phenotype/suicide_attempt/suicide_attempt/#eds_scikit.phenotype.suicide_attempt.suicide_attempt.SuicideAttemptFromICD10.compute","text":"compute () Fetch and aggregate features Source code in eds_scikit/phenotype/suicide_attempt/suicide_attempt.py 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 def compute ( self ): \"\"\" Fetch and aggregate features \"\"\" if self . algo == \"X60-X84\" : self . add_code_feature ( output_feature = \"X60-X84\" , source = \"icd10\" , codes = self . ICD10_CODES [ \"X60-X84\" ][ \"codes\" ], ) self . agg_single_feature ( \"X60-X84\" , level = \"visit\" , subphenotype = False , threshold = 1 , ) elif self . algo == \"Haguenoer2008\" : self . add_code_feature ( output_feature = \"X60-X84\" , source = \"icd10\" , codes = self . ICD10_CODES [ \"X60-X84\" ][ \"codes\" ], additional_filtering = dict ( condition_status_source_value = \"DAS\" ), ) self . add_code_feature ( output_feature = \"DP\" , source = \"icd10\" , codes = self . ICD10_CODES [ \"Haguenoer2008\" ][ \"codes\" ], additional_filtering = self . ICD10_CODES [ \"Haguenoer2008\" ][ \"additional_filtering\" ], ) self . agg_two_features ( \"X60-X84\" , \"DP\" , output_feature = \"Haguenoer2008\" , how = \"AND\" , level = \"visit\" , subphenotype = False , thresholds = ( 1 , 1 ), )","title":"compute()"},{"location":"reference/plot/","text":"eds_scikit.plot","title":"`eds_scikit.plot`"},{"location":"reference/plot/#eds_scikitplot","text":"","title":"eds_scikit.plot"},{"location":"reference/plot/age_pyramid/","text":"eds_scikit.plot.age_pyramid plot_age_pyramid plot_age_pyramid ( person : DataFrame , datetime_ref : datetime = None , return_array : bool = False ) -> Tuple [ alt . ConcatChart , Series ] Plot an age pyramid from a 'person' pandas DataFrame. PARAMETER DESCRIPTION person The person table. Must have the following columns: - birth_datetime , dtype : datetime or str - person_id , dtype : any - gender_source_value , dtype : str, {'m', 'f'} TYPE: pd.DataFrame (ks.DataFrame not supported), datetime_ref : Union[datetime, str], default None The reference date to compute population age from. If a string, it searches for a column with the same name in the person table: each patient has his own datetime reference. If a datetime, the reference datetime is the same for all patients. If set to None, datetime.today() will be used instead. filename : str, default None The path to save figure at. savefig : bool, default False If set to True, filename must be set. The plot will be saved at the specified filename. return_array : bool, default False If set to True, return chart and its pd.Dataframe representation. RETURNS DESCRIPTION chart If savefig set to True, returns None. TYPE: alt . ConcatChart group_gender_age : Series, The total number of patients grouped by gender and binned age. Source code in eds_scikit/plot/age_pyramid.py 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 def plot_age_pyramid ( person : DataFrame , datetime_ref : datetime = None , return_array : bool = False , ) -> Tuple [ alt . ConcatChart , Series ]: \"\"\"Plot an age pyramid from a 'person' pandas DataFrame. Parameters ---------- person : pd.DataFrame (ks.DataFrame not supported), The person table. Must have the following columns: - `birth_datetime`, dtype : datetime or str - `person_id`, dtype : any - `gender_source_value`, dtype : str, {'m', 'f'} datetime_ref : Union[datetime, str], default None The reference date to compute population age from. If a string, it searches for a column with the same name in the person table: each patient has his own datetime reference. If a datetime, the reference datetime is the same for all patients. If set to None, datetime.today() will be used instead. filename : str, default None The path to save figure at. savefig : bool, default False If set to True, filename must be set. The plot will be saved at the specified filename. return_array : bool, default False If set to True, return chart and its pd.Dataframe representation. Returns ------- chart : alt.ConcatChart, If savefig set to True, returns None. group_gender_age : Series, The total number of patients grouped by gender and binned age. \"\"\" check_columns ( person , [ \"person_id\" , \"birth_datetime\" , \"gender_source_value\" ]) datetime_ref_raw = copy ( datetime_ref ) if datetime_ref is None : datetime_ref = datetime . today () elif isinstance ( datetime_ref , datetime ): datetime_ref = pd . to_datetime ( datetime_ref ) elif isinstance ( datetime_ref , str ): # A string type for datetime_ref could be either # a column name or a datetime in string format. if datetime_ref in person . columns : datetime_ref = person [ datetime_ref ] else : datetime_ref = pd . to_datetime ( datetime_ref , errors = \"coerce\" ) # In case of error, will return NaT if pd . isnull ( datetime_ref ): raise ValueError ( f \"`datetime_ref` must either be a column name or parseable date, \" f \"got string ' { datetime_ref_raw } '\" ) else : raise TypeError ( f \"`datetime_ref` must be either None, a parseable string date\" f \", a column name or a datetime. Got type: { type ( datetime_ref ) } , { datetime_ref } \" ) cols_to_keep = [ \"person_id\" , \"birth_datetime\" , \"gender_source_value\" ] person_ = bd . to_pandas ( person [ cols_to_keep ]) person_ [ \"age\" ] = ( datetime_ref - person_ [ \"birth_datetime\" ]) . dt . total_seconds () person_ [ \"age\" ] /= 365 * 24 * 3600 # Remove outliers mask_age_inliners = ( person_ [ \"age\" ] > 0 ) & ( person_ [ \"age\" ] < 125 ) n_outliers = ( ~ mask_age_inliners ) . sum () if n_outliers > 0 : perc_outliers = 100 * n_outliers / person_ . shape [ 0 ] logger . warning ( f \" { n_outliers } ( { perc_outliers : .4f } %) individuals' \" \"age is out of the (0, 125) interval, we skip them.\" ) person_ = person_ . loc [ mask_age_inliners ] # Aggregate rare age categories mask_rare_age_agg = person_ [ \"age\" ] > 90 person_ . loc [ mask_rare_age_agg , \"age\" ] = 99 bins = np . arange ( 0 , 100 , 10 ) labels = [ f \" { left } - { right } \" for left , right in zip ( bins [: - 1 ], bins [ 1 :])] person_ [ \"age_bins\" ] = pd . cut ( person_ [ \"age\" ], bins = bins , labels = labels ) person_ = person_ . loc [ person_ [ \"gender_source_value\" ] . isin ([ \"m\" , \"f\" ])] group_gender_age = person_ . groupby ([ \"gender_source_value\" , \"age_bins\" ])[ \"person_id\" ] . count () male = group_gender_age [ \"m\" ] . reset_index () female = group_gender_age [ \"f\" ] . reset_index () left = ( alt . Chart ( male ) . mark_bar () . encode ( y = alt . Y ( \"age_bins\" , axis = None , sort = alt . SortOrder ( \"descending\" )), x = alt . X ( \"person_id\" , sort = alt . SortOrder ( \"descending\" )), ) . properties ( title = \"Male\" ) ) right = ( alt . Chart ( female ) . mark_bar ( color = \"coral\" ) . encode ( y = alt . Y ( \"age_bins\" , axis = None , sort = alt . SortOrder ( \"descending\" )), x = alt . X ( \"person_id\" , title = \"N\" ), ) . properties ( title = \"Female\" ) ) middle = ( alt . Chart ( male ) . mark_text () . encode ( y = alt . Y ( \"age_bins\" , axis = None , sort = alt . SortOrder ( \"descending\" )), text = alt . Text ( \"age_bins\" ), ) ) chart = alt . concat ( left , middle , right , spacing = 5 ) if return_array : return group_gender_age return chart","title":"age_pyramid"},{"location":"reference/plot/age_pyramid/#eds_scikitplotage_pyramid","text":"","title":"eds_scikit.plot.age_pyramid"},{"location":"reference/plot/age_pyramid/#eds_scikit.plot.age_pyramid.plot_age_pyramid","text":"plot_age_pyramid ( person : DataFrame , datetime_ref : datetime = None , return_array : bool = False ) -> Tuple [ alt . ConcatChart , Series ] Plot an age pyramid from a 'person' pandas DataFrame. PARAMETER DESCRIPTION person The person table. Must have the following columns: - birth_datetime , dtype : datetime or str - person_id , dtype : any - gender_source_value , dtype : str, {'m', 'f'} TYPE: pd.DataFrame (ks.DataFrame not supported), datetime_ref : Union[datetime, str], default None The reference date to compute population age from. If a string, it searches for a column with the same name in the person table: each patient has his own datetime reference. If a datetime, the reference datetime is the same for all patients. If set to None, datetime.today() will be used instead. filename : str, default None The path to save figure at. savefig : bool, default False If set to True, filename must be set. The plot will be saved at the specified filename. return_array : bool, default False If set to True, return chart and its pd.Dataframe representation. RETURNS DESCRIPTION chart If savefig set to True, returns None. TYPE: alt . ConcatChart group_gender_age : Series, The total number of patients grouped by gender and binned age. Source code in eds_scikit/plot/age_pyramid.py 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 def plot_age_pyramid ( person : DataFrame , datetime_ref : datetime = None , return_array : bool = False , ) -> Tuple [ alt . ConcatChart , Series ]: \"\"\"Plot an age pyramid from a 'person' pandas DataFrame. Parameters ---------- person : pd.DataFrame (ks.DataFrame not supported), The person table. Must have the following columns: - `birth_datetime`, dtype : datetime or str - `person_id`, dtype : any - `gender_source_value`, dtype : str, {'m', 'f'} datetime_ref : Union[datetime, str], default None The reference date to compute population age from. If a string, it searches for a column with the same name in the person table: each patient has his own datetime reference. If a datetime, the reference datetime is the same for all patients. If set to None, datetime.today() will be used instead. filename : str, default None The path to save figure at. savefig : bool, default False If set to True, filename must be set. The plot will be saved at the specified filename. return_array : bool, default False If set to True, return chart and its pd.Dataframe representation. Returns ------- chart : alt.ConcatChart, If savefig set to True, returns None. group_gender_age : Series, The total number of patients grouped by gender and binned age. \"\"\" check_columns ( person , [ \"person_id\" , \"birth_datetime\" , \"gender_source_value\" ]) datetime_ref_raw = copy ( datetime_ref ) if datetime_ref is None : datetime_ref = datetime . today () elif isinstance ( datetime_ref , datetime ): datetime_ref = pd . to_datetime ( datetime_ref ) elif isinstance ( datetime_ref , str ): # A string type for datetime_ref could be either # a column name or a datetime in string format. if datetime_ref in person . columns : datetime_ref = person [ datetime_ref ] else : datetime_ref = pd . to_datetime ( datetime_ref , errors = \"coerce\" ) # In case of error, will return NaT if pd . isnull ( datetime_ref ): raise ValueError ( f \"`datetime_ref` must either be a column name or parseable date, \" f \"got string ' { datetime_ref_raw } '\" ) else : raise TypeError ( f \"`datetime_ref` must be either None, a parseable string date\" f \", a column name or a datetime. Got type: { type ( datetime_ref ) } , { datetime_ref } \" ) cols_to_keep = [ \"person_id\" , \"birth_datetime\" , \"gender_source_value\" ] person_ = bd . to_pandas ( person [ cols_to_keep ]) person_ [ \"age\" ] = ( datetime_ref - person_ [ \"birth_datetime\" ]) . dt . total_seconds () person_ [ \"age\" ] /= 365 * 24 * 3600 # Remove outliers mask_age_inliners = ( person_ [ \"age\" ] > 0 ) & ( person_ [ \"age\" ] < 125 ) n_outliers = ( ~ mask_age_inliners ) . sum () if n_outliers > 0 : perc_outliers = 100 * n_outliers / person_ . shape [ 0 ] logger . warning ( f \" { n_outliers } ( { perc_outliers : .4f } %) individuals' \" \"age is out of the (0, 125) interval, we skip them.\" ) person_ = person_ . loc [ mask_age_inliners ] # Aggregate rare age categories mask_rare_age_agg = person_ [ \"age\" ] > 90 person_ . loc [ mask_rare_age_agg , \"age\" ] = 99 bins = np . arange ( 0 , 100 , 10 ) labels = [ f \" { left } - { right } \" for left , right in zip ( bins [: - 1 ], bins [ 1 :])] person_ [ \"age_bins\" ] = pd . cut ( person_ [ \"age\" ], bins = bins , labels = labels ) person_ = person_ . loc [ person_ [ \"gender_source_value\" ] . isin ([ \"m\" , \"f\" ])] group_gender_age = person_ . groupby ([ \"gender_source_value\" , \"age_bins\" ])[ \"person_id\" ] . count () male = group_gender_age [ \"m\" ] . reset_index () female = group_gender_age [ \"f\" ] . reset_index () left = ( alt . Chart ( male ) . mark_bar () . encode ( y = alt . Y ( \"age_bins\" , axis = None , sort = alt . SortOrder ( \"descending\" )), x = alt . X ( \"person_id\" , sort = alt . SortOrder ( \"descending\" )), ) . properties ( title = \"Male\" ) ) right = ( alt . Chart ( female ) . mark_bar ( color = \"coral\" ) . encode ( y = alt . Y ( \"age_bins\" , axis = None , sort = alt . SortOrder ( \"descending\" )), x = alt . X ( \"person_id\" , title = \"N\" ), ) . properties ( title = \"Female\" ) ) middle = ( alt . Chart ( male ) . mark_text () . encode ( y = alt . Y ( \"age_bins\" , axis = None , sort = alt . SortOrder ( \"descending\" )), text = alt . Text ( \"age_bins\" ), ) ) chart = alt . concat ( left , middle , right , spacing = 5 ) if return_array : return group_gender_age return chart","title":"plot_age_pyramid()"},{"location":"reference/plot/default_table_viz/","text":"eds_scikit.plot.default_table_viz","title":"default_table_viz"},{"location":"reference/plot/default_table_viz/#eds_scikitplotdefault_table_viz","text":"","title":"eds_scikit.plot.default_table_viz"},{"location":"reference/plot/event_sequences/","text":"eds_scikit.plot.event_sequences plot_event_sequences plot_event_sequences ( df_events : pd . DataFrame , event_col : Optional [ str ] = 'event' , event_start_datetime_col : Optional [ str ] = 'event_start_datetime' , event_end_datetime_col : Optional [ str ] = 'event_end_datetime' , dim_mapping : Optional [ Dict [ str , Dict [ str , Union [ Tuple [ int ], str ]]]] = None , index_date_col : Optional [ str ] = None , family_col : Optional [ str ] = None , family_to_index : Optional [ Dict [ str , int ]] = None , list_person_ids : Optional [ List [ str ]] = None , same_x_axis_scale : Optional [ bool ] = False , subplot_height : Optional [ int ] = 200 , subplot_width : Optional [ int ] = 500 , point_size : Optional [ int ] = 400 , bar_height : Optional [ int ] = 20 , title : Optional [ str ] = None , seed : Optional [ int ] = 0 ) -> alt . VConcatChart Plots individual sequences from an events DataFrame. Each event must be recorded with a start date, a name and a person_id . Events can be both one-time (only start date given) or longitudinal (both start and end dates). Events can also be aggregated in families using the family_col argument. Finally, events labelling and colors can be manually set by providing a dim_mapping dictionary. PARAMETER DESCRIPTION df_events DataFrame gathering the events information. Must contain at least person_id , event, t_start and t_end columns. TYPE: pd . DataFrame event_col Column name of the events. TYPE: Optional [ str ] DEFAULT: 'event' event_start_datetime_col Column name of the event start datetime. TYPE: Optional [ str ] DEFAULT: 'event_start_datetime' event_end_datetime_col Column name of the event end datetime. TYPE: Optional [ str ] DEFAULT: 'event_end_datetime' dim_mapping Mapping dictionary to provide plotting details on events. Must be of type : dim_labelling = { \"event_1\" : { \"color\" : ( 255 , 200 , 150 ), \"label\" : \"Event 1\" }, \"event_2\" : { \"color\" : ( 200 , 255 , 150 ), \"label\" : \"Event 2\" }, } TYPE: Optional [ Dict [ str , Dict [ str , Union [ Tuple [ int ], str ]]]] DEFAULT: None index_date_col Column name of the index date to compute relative datetimes for events. For example, it could be the date of inclusion for each patient. TYPE: Optional [ str ] DEFAULT: None family_col Column name of family events. Events of a given family will be plot on the same row. TYPE: Optional [ str ] DEFAULT: None family_to_index Dictionary mapping event family names to ordering indices. TYPE: Optional [ Dict [ str , int ]] DEFAULT: None list_person_ids List of person_ids to plot. If None given, only the first three individual sequences will be plot. TYPE: Optional [ List [ str ]] DEFAULT: None same_x_axis_scale Whether to use the same axis scale for all sequences. TYPE: Optional [ bool ] DEFAULT: False subplot_height Height of each plot. TYPE: Optional [ int ] DEFAULT: 200 subplot_width Width of each plot. TYPE: Optional [ int ] DEFAULT: 500 point_size Size of points for one-time events. TYPE: Optional [ int ] DEFAULT: 400 bar_height Height of bars for continuous events. TYPE: Optional [ int ] DEFAULT: 20 title Chart title. TYPE: Optional [ str ] DEFAULT: None seed Seed to randomly draw colors when not provided. TYPE: Optional [ int ] DEFAULT: 0 RETURNS DESCRIPTION chart Chart with the plotted individual event sequences. TYPE: alt . VConcatChart Source code in eds_scikit/plot/event_sequences.py 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 def plot_event_sequences ( df_events : pd . DataFrame , event_col : Optional [ str ] = \"event\" , event_start_datetime_col : Optional [ str ] = \"event_start_datetime\" , event_end_datetime_col : Optional [ str ] = \"event_end_datetime\" , dim_mapping : Optional [ Dict [ str , Dict [ str , Union [ Tuple [ int ], str ]]]] = None , index_date_col : Optional [ str ] = None , family_col : Optional [ str ] = None , family_to_index : Optional [ Dict [ str , int ]] = None , list_person_ids : Optional [ List [ str ]] = None , same_x_axis_scale : Optional [ bool ] = False , subplot_height : Optional [ int ] = 200 , subplot_width : Optional [ int ] = 500 , point_size : Optional [ int ] = 400 , bar_height : Optional [ int ] = 20 , title : Optional [ str ] = None , seed : Optional [ int ] = 0 , ) -> alt . VConcatChart : \"\"\" Plots individual sequences from an events DataFrame. Each event must be recorded with a start date, a name and a `person_id`. Events can be both one-time (only start date given) or longitudinal (both start and end dates). Events can also be aggregated in families using the `family_col` argument. Finally, events labelling and colors can be manually set by providing a `dim_mapping` dictionary. Parameters ---------- df_events: pd.DataFrame DataFrame gathering the events information. Must contain at least `person_id`, event, t_start and t_end columns. event_col: Optional[str] = \"event\" Column name of the events. event_start_datetime_col: Optional[str] = \"event_start_datetime\" Column name of the event start datetime. event_end_datetime_col: Optional[str] = \"event_end_datetime\" Column name of the event end datetime. dim_mapping: Optional[Dict[str,Dict[str,Union[tuple(int),str]]]] = None Mapping dictionary to provide plotting details on events. Must be of type : ```python dim_labelling = { \"event_1\": {\"color\": (255, 200, 150), \"label\": \"Event 1\"}, \"event_2\": {\"color\": (200, 255, 150), \"label\": \"Event 2\"}, } ``` index_date_col: Optional[str] = None Column name of the index date to compute relative datetimes for events. For example, it could be the date of inclusion for each patient. family_col: Optional[str] = None Column name of family events. Events of a given family will be plot on the same row. family_to_index: Optional[Dict[str,int]] = None Dictionary mapping event family names to ordering indices. list_person_ids: Optional[List[str]] = None List of person_ids to plot. If None given, only the first three individual sequences will be plot. same_x_axis_scale: Optional[bool] = False Whether to use the same axis scale for all sequences. subplot_height: Optional[int] = 200 Height of each plot. subplot_width: Optional[int] = 500 Width of each plot. point_size: Optional[int] = 400 Size of points for one-time events. bar_height: Optional[int] = 20 Height of bars for continuous events. title: Optional[str] = None Chart title. seed: int = 0 Seed to randomly draw colors when not provided. Returns ------- chart: alt.VConcatChart Chart with the plotted individual event sequences. \"\"\" rng = np . random . RandomState ( seed ) # Check required columns required_columns = [ \"person_id\" , event_col , event_start_datetime_col , event_end_datetime_col , ] if index_date_col is not None : required_columns . append ( index_date_col ) if family_col is not None : required_columns . append ( family_col ) check_columns ( df_events , required_columns = required_columns ) # Pre-selection of the sequences to plot if list_person_ids is None : list_person_ids = df_events . person_id . unique ()[: 3 ] df_events = df_events . query ( \"person_id in @list_person_ids\" ) # Ordering order = { val : idx for idx , val in enumerate ( list_person_ids )} df_events = df_events . sort_values ( by = \"person_id\" , key = lambda x : x . map ( order )) # Encoding events start and end dates if index_date_col is not None : df_events [ \"relative_event_start\" ] = ( df_events [ event_start_datetime_col ] - df_events [ index_date_col ] ) . dt . days . astype ( int ) df_events [ \"event_duration\" ] = ( ( df_events [ event_end_datetime_col ] - df_events [ event_start_datetime_col ]) . dt . days . fillna ( 1 ) . astype ( int ) ) df_events [ \"relative_event_end\" ] = ( df_events . relative_event_start + df_events . event_duration ) x_encoding = \"relative_event_start:Q\" x2_encoding = \"relative_event_end:Q\" else : x_encoding = f \" { event_start_datetime_col } :T\" x2_encoding = f \" { event_end_datetime_col } :T\" # Ordering events if family_col is not None : if family_to_index is None : family_to_index = { v : k for k , v in enumerate ( df_events [ family_col ] . unique ()) } df_events [ \"dim_id\" ] = df_events [ family_col ] . map ( family_to_index ) else : _ , classes = np . unique ( df_events [ event_col ], return_inverse = True ) df_events [ \"dim_id\" ] = classes # Mapping events towards colors and labels if dim_mapping is not None : df_events [ \"dim_label\" ] = df_events [ event_col ] . apply ( lambda x : dim_mapping [ x ][ \"label\" ] ) labels = [] colors = [] for event in dim_mapping . keys (): labels . append ( dim_mapping [ event ][ \"label\" ]) colors . append ( f \"rgb { dim_mapping [ event ][ 'color' ] } \" ) else : df_events [ \"dim_label\" ] = df_events [ event_col ] labels = list ( df_events [ \"dim_label\" ] . unique ()) colors = [ f \"rgb { tuple ( rng . randint ( 0 , 255 , size = 3 )) } \" for _ in labels ] # Base chart raw = alt . Chart ( df_events ) . encode ( x = alt . X ( x_encoding ), y = alt . Y ( \"dim_id:O\" , title = \"\" ), color = alt . Color ( \"dim_label:O\" , scale = alt . Scale ( domain = labels , range = colors ), title = \"Event type\" , ), ) # One-time events point_dim = ( raw . transform_filter ( { \"not\" : alt . FieldValidPredicate ( field = event_end_datetime_col , valid = True )} ) . mark_point ( filled = True , size = point_size , cursor = \"pointer\" ) . encode ( tooltip = [ f \" { event_col } \" , f \" { event_start_datetime_col } \" ], ) ) # Continuous events continuous_dim = ( raw . transform_filter ( alt . FieldValidPredicate ( event_end_datetime_col , valid = True ) ) . mark_bar ( filled = True , cursor = \"pointer\" , cornerRadius = bar_height / 2 , height = bar_height , ) . encode ( x2 = x2_encoding , tooltip = [ f \" { event_col } \" , f \" { event_start_datetime_col } \" , f \" { event_end_datetime_col } \" , ], ) ) # Aggregation base = ( point_dim + continuous_dim ) . properties ( width = subplot_width , height = subplot_height , ) # Vertical concatenation of all patients' sequences chart = ( alt . vconcat () . configure_legend ( labelFontSize = 13 , symbolSize = 150 , titleFontSize = 15 ) . configure_axisY ( disable = True ) ) for person_id in df_events . person_id . unique (): chart &= base . transform_filter ( alt . expr . datum . person_id == person_id ) . properties ( title = f \"Sequence of patient { person_id } \" ) if same_x_axis_scale : chart = chart . resolve_scale ( x = \"shared\" ) if title is not None : chart = chart . properties ( title = title ) return chart","title":"event_sequences"},{"location":"reference/plot/event_sequences/#eds_scikitplotevent_sequences","text":"","title":"eds_scikit.plot.event_sequences"},{"location":"reference/plot/event_sequences/#eds_scikit.plot.event_sequences.plot_event_sequences","text":"plot_event_sequences ( df_events : pd . DataFrame , event_col : Optional [ str ] = 'event' , event_start_datetime_col : Optional [ str ] = 'event_start_datetime' , event_end_datetime_col : Optional [ str ] = 'event_end_datetime' , dim_mapping : Optional [ Dict [ str , Dict [ str , Union [ Tuple [ int ], str ]]]] = None , index_date_col : Optional [ str ] = None , family_col : Optional [ str ] = None , family_to_index : Optional [ Dict [ str , int ]] = None , list_person_ids : Optional [ List [ str ]] = None , same_x_axis_scale : Optional [ bool ] = False , subplot_height : Optional [ int ] = 200 , subplot_width : Optional [ int ] = 500 , point_size : Optional [ int ] = 400 , bar_height : Optional [ int ] = 20 , title : Optional [ str ] = None , seed : Optional [ int ] = 0 ) -> alt . VConcatChart Plots individual sequences from an events DataFrame. Each event must be recorded with a start date, a name and a person_id . Events can be both one-time (only start date given) or longitudinal (both start and end dates). Events can also be aggregated in families using the family_col argument. Finally, events labelling and colors can be manually set by providing a dim_mapping dictionary. PARAMETER DESCRIPTION df_events DataFrame gathering the events information. Must contain at least person_id , event, t_start and t_end columns. TYPE: pd . DataFrame event_col Column name of the events. TYPE: Optional [ str ] DEFAULT: 'event' event_start_datetime_col Column name of the event start datetime. TYPE: Optional [ str ] DEFAULT: 'event_start_datetime' event_end_datetime_col Column name of the event end datetime. TYPE: Optional [ str ] DEFAULT: 'event_end_datetime' dim_mapping Mapping dictionary to provide plotting details on events. Must be of type : dim_labelling = { \"event_1\" : { \"color\" : ( 255 , 200 , 150 ), \"label\" : \"Event 1\" }, \"event_2\" : { \"color\" : ( 200 , 255 , 150 ), \"label\" : \"Event 2\" }, } TYPE: Optional [ Dict [ str , Dict [ str , Union [ Tuple [ int ], str ]]]] DEFAULT: None index_date_col Column name of the index date to compute relative datetimes for events. For example, it could be the date of inclusion for each patient. TYPE: Optional [ str ] DEFAULT: None family_col Column name of family events. Events of a given family will be plot on the same row. TYPE: Optional [ str ] DEFAULT: None family_to_index Dictionary mapping event family names to ordering indices. TYPE: Optional [ Dict [ str , int ]] DEFAULT: None list_person_ids List of person_ids to plot. If None given, only the first three individual sequences will be plot. TYPE: Optional [ List [ str ]] DEFAULT: None same_x_axis_scale Whether to use the same axis scale for all sequences. TYPE: Optional [ bool ] DEFAULT: False subplot_height Height of each plot. TYPE: Optional [ int ] DEFAULT: 200 subplot_width Width of each plot. TYPE: Optional [ int ] DEFAULT: 500 point_size Size of points for one-time events. TYPE: Optional [ int ] DEFAULT: 400 bar_height Height of bars for continuous events. TYPE: Optional [ int ] DEFAULT: 20 title Chart title. TYPE: Optional [ str ] DEFAULT: None seed Seed to randomly draw colors when not provided. TYPE: Optional [ int ] DEFAULT: 0 RETURNS DESCRIPTION chart Chart with the plotted individual event sequences. TYPE: alt . VConcatChart Source code in eds_scikit/plot/event_sequences.py 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 def plot_event_sequences ( df_events : pd . DataFrame , event_col : Optional [ str ] = \"event\" , event_start_datetime_col : Optional [ str ] = \"event_start_datetime\" , event_end_datetime_col : Optional [ str ] = \"event_end_datetime\" , dim_mapping : Optional [ Dict [ str , Dict [ str , Union [ Tuple [ int ], str ]]]] = None , index_date_col : Optional [ str ] = None , family_col : Optional [ str ] = None , family_to_index : Optional [ Dict [ str , int ]] = None , list_person_ids : Optional [ List [ str ]] = None , same_x_axis_scale : Optional [ bool ] = False , subplot_height : Optional [ int ] = 200 , subplot_width : Optional [ int ] = 500 , point_size : Optional [ int ] = 400 , bar_height : Optional [ int ] = 20 , title : Optional [ str ] = None , seed : Optional [ int ] = 0 , ) -> alt . VConcatChart : \"\"\" Plots individual sequences from an events DataFrame. Each event must be recorded with a start date, a name and a `person_id`. Events can be both one-time (only start date given) or longitudinal (both start and end dates). Events can also be aggregated in families using the `family_col` argument. Finally, events labelling and colors can be manually set by providing a `dim_mapping` dictionary. Parameters ---------- df_events: pd.DataFrame DataFrame gathering the events information. Must contain at least `person_id`, event, t_start and t_end columns. event_col: Optional[str] = \"event\" Column name of the events. event_start_datetime_col: Optional[str] = \"event_start_datetime\" Column name of the event start datetime. event_end_datetime_col: Optional[str] = \"event_end_datetime\" Column name of the event end datetime. dim_mapping: Optional[Dict[str,Dict[str,Union[tuple(int),str]]]] = None Mapping dictionary to provide plotting details on events. Must be of type : ```python dim_labelling = { \"event_1\": {\"color\": (255, 200, 150), \"label\": \"Event 1\"}, \"event_2\": {\"color\": (200, 255, 150), \"label\": \"Event 2\"}, } ``` index_date_col: Optional[str] = None Column name of the index date to compute relative datetimes for events. For example, it could be the date of inclusion for each patient. family_col: Optional[str] = None Column name of family events. Events of a given family will be plot on the same row. family_to_index: Optional[Dict[str,int]] = None Dictionary mapping event family names to ordering indices. list_person_ids: Optional[List[str]] = None List of person_ids to plot. If None given, only the first three individual sequences will be plot. same_x_axis_scale: Optional[bool] = False Whether to use the same axis scale for all sequences. subplot_height: Optional[int] = 200 Height of each plot. subplot_width: Optional[int] = 500 Width of each plot. point_size: Optional[int] = 400 Size of points for one-time events. bar_height: Optional[int] = 20 Height of bars for continuous events. title: Optional[str] = None Chart title. seed: int = 0 Seed to randomly draw colors when not provided. Returns ------- chart: alt.VConcatChart Chart with the plotted individual event sequences. \"\"\" rng = np . random . RandomState ( seed ) # Check required columns required_columns = [ \"person_id\" , event_col , event_start_datetime_col , event_end_datetime_col , ] if index_date_col is not None : required_columns . append ( index_date_col ) if family_col is not None : required_columns . append ( family_col ) check_columns ( df_events , required_columns = required_columns ) # Pre-selection of the sequences to plot if list_person_ids is None : list_person_ids = df_events . person_id . unique ()[: 3 ] df_events = df_events . query ( \"person_id in @list_person_ids\" ) # Ordering order = { val : idx for idx , val in enumerate ( list_person_ids )} df_events = df_events . sort_values ( by = \"person_id\" , key = lambda x : x . map ( order )) # Encoding events start and end dates if index_date_col is not None : df_events [ \"relative_event_start\" ] = ( df_events [ event_start_datetime_col ] - df_events [ index_date_col ] ) . dt . days . astype ( int ) df_events [ \"event_duration\" ] = ( ( df_events [ event_end_datetime_col ] - df_events [ event_start_datetime_col ]) . dt . days . fillna ( 1 ) . astype ( int ) ) df_events [ \"relative_event_end\" ] = ( df_events . relative_event_start + df_events . event_duration ) x_encoding = \"relative_event_start:Q\" x2_encoding = \"relative_event_end:Q\" else : x_encoding = f \" { event_start_datetime_col } :T\" x2_encoding = f \" { event_end_datetime_col } :T\" # Ordering events if family_col is not None : if family_to_index is None : family_to_index = { v : k for k , v in enumerate ( df_events [ family_col ] . unique ()) } df_events [ \"dim_id\" ] = df_events [ family_col ] . map ( family_to_index ) else : _ , classes = np . unique ( df_events [ event_col ], return_inverse = True ) df_events [ \"dim_id\" ] = classes # Mapping events towards colors and labels if dim_mapping is not None : df_events [ \"dim_label\" ] = df_events [ event_col ] . apply ( lambda x : dim_mapping [ x ][ \"label\" ] ) labels = [] colors = [] for event in dim_mapping . keys (): labels . append ( dim_mapping [ event ][ \"label\" ]) colors . append ( f \"rgb { dim_mapping [ event ][ 'color' ] } \" ) else : df_events [ \"dim_label\" ] = df_events [ event_col ] labels = list ( df_events [ \"dim_label\" ] . unique ()) colors = [ f \"rgb { tuple ( rng . randint ( 0 , 255 , size = 3 )) } \" for _ in labels ] # Base chart raw = alt . Chart ( df_events ) . encode ( x = alt . X ( x_encoding ), y = alt . Y ( \"dim_id:O\" , title = \"\" ), color = alt . Color ( \"dim_label:O\" , scale = alt . Scale ( domain = labels , range = colors ), title = \"Event type\" , ), ) # One-time events point_dim = ( raw . transform_filter ( { \"not\" : alt . FieldValidPredicate ( field = event_end_datetime_col , valid = True )} ) . mark_point ( filled = True , size = point_size , cursor = \"pointer\" ) . encode ( tooltip = [ f \" { event_col } \" , f \" { event_start_datetime_col } \" ], ) ) # Continuous events continuous_dim = ( raw . transform_filter ( alt . FieldValidPredicate ( event_end_datetime_col , valid = True ) ) . mark_bar ( filled = True , cursor = \"pointer\" , cornerRadius = bar_height / 2 , height = bar_height , ) . encode ( x2 = x2_encoding , tooltip = [ f \" { event_col } \" , f \" { event_start_datetime_col } \" , f \" { event_end_datetime_col } \" , ], ) ) # Aggregation base = ( point_dim + continuous_dim ) . properties ( width = subplot_width , height = subplot_height , ) # Vertical concatenation of all patients' sequences chart = ( alt . vconcat () . configure_legend ( labelFontSize = 13 , symbolSize = 150 , titleFontSize = 15 ) . configure_axisY ( disable = True ) ) for person_id in df_events . person_id . unique (): chart &= base . transform_filter ( alt . expr . datum . person_id == person_id ) . properties ( title = f \"Sequence of patient { person_id } \" ) if same_x_axis_scale : chart = chart . resolve_scale ( x = \"shared\" ) if title is not None : chart = chart . properties ( title = title ) return chart","title":"plot_event_sequences()"},{"location":"reference/plot/table_viz/","text":"eds_scikit.plot.table_viz reduce_table reduce_table ( table : DataFrame , category_columns : List [ str ], date_column : str , start_date : str , end_date : str , mapper : dict = None ) -> DataFrame Reduce table PARAMETER DESCRIPTION table Input dataframe to reduce TYPE: DataFrame category_columns Columns to perform reduction on TYPE: List [ str ] date_column Date column TYPE: str start_date start date TYPE: str end_date end date TYPE: str RETURNS DESCRIPTION DataFrame Reducted DataFrame. RAISES DESCRIPTION ValueError description Source code in eds_scikit/plot/table_viz.py 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 def reduce_table ( table : DataFrame , category_columns : List [ str ], date_column : str , start_date : str , end_date : str , mapper : dict = None ) -> DataFrame : \"\"\"Reduce table Parameters ---------- table : DataFrame Input dataframe to reduce category_columns : List[str] Columns to perform reduction on date_column : str Date column start_date : str start date end_date : str end date Returns ------- DataFrame Reducted DataFrame. Raises ------ ValueError _description_ \"\"\" framework = get_framework ( table ) # assert qu'on a bien les bonnes colonnes table = table [( table [ date_column ] >= start_date ) & ( table [ date_column ] <= end_date )] table [ \"datetime\" ] = framework . to_datetime ( table [ date_column ] . dt . strftime ( '%Y-%m' )) category_columns = [ * category_columns , \"datetime\" ] if mapper : for col , mapping in mapper . items (): table = map_column ( table , mapping , col , col ) table = table [ category_columns ] shape = table . shape # to prevent computation errors nunique = table . nunique () oversized_columns = nunique [( nunique . index != \"datetime\" ) & ( nunique > 50 )] . tolist () if oversized_columns != []: raise ValueError ( \"Input table columns can't have more then 50 values. Consider using eds_scikit.plot.map_column.\" ) table_count = table . fillna ( \"NaN\" ) . groupby ( category_columns ) . size () . reset_index ( name = 'count' ) shape = table_count . shape # to prevent computation errors table_count = to ( \"pandas\" , table_count ) table_count [ \"datetime\" ] = pd . to_datetime ( table_count [ \"datetime\" ]) date_dataframe = pd . DataFrame ( pd . date_range ( start = start_date , end = end_date , freq = 'MS' ), columns = [ 'datetime' ]) table_count = table_count . merge ( date_dataframe , on = \"datetime\" , how = \"right\" ) table_count [ \"count\" ] = table_count [ \"count\" ] . fillna ( 0 ) table_count = table_count . fillna ( \"NaN\" ) return table_count visualize_table visualize_table ( table_count : DataFrame , title : str = 'table exploration dashboard' ) -> alt . Chart Generate reduced table dashboard. PARAMETER DESCRIPTION table_count Output from eds_scikit.plot.table_viz.reduce_table TYPE: DataFrame title Chart title TYPE: str DEFAULT: 'table exploration dashboard' RETURNS DESCRIPTION alt . Chart reduce_table dashboard RAISES DESCRIPTION ValueError description Source code in eds_scikit/plot/table_viz.py 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 def visualize_table ( table_count : DataFrame , title : str = \"table exploration dashboard\" ) -> alt . Chart : \"\"\"Generate reduced table dashboard. Parameters ---------- table_count : DataFrame Output from eds_scikit.plot.table_viz.reduce_table title : str Chart title Returns ------- alt.Chart reduce_table dashboard Raises ------ ValueError _description_ \"\"\" check_columns = [ \"count\" , \"datetime\" ] for check_column in check_columns : if not ( check_column in table_count . columns ): raise ValueError ( \"Input table must have a {check_column} column.\" ) selections = {} columns = [ col for col in table_count . columns if ( col != 'count' ) and ( col != \"datetime\" )] for col in columns : selections [ col ] = alt . selection_point ( fields = [ col ], on = 'click' , bind = 'legend' , clear = 'dblclick' ) #selections[\"time\"] = alt.selection_interval(encodings=['x'], translate=False, zoom=False) charts = [] for i , col in enumerate ( columns ): color_scale = generate_color_map ( table_count , col ) width , height = 200 , 50 chart = alt . Chart ( table_count ) . mark_bar () . encode ( x = f 'sum(count):Q' , color = alt . Color ( col + ':N' , scale = color_scale ), opacity = alt . condition ( selections [ col ], alt . value ( 1 ), alt . value ( 0.3 )), tooltip = [ col ] ) . add_params ( selections [ col ] ) . transform_filter ( reduce (( lambda x , y : x & y ), [ selections [ s ] for s in selections if ( s != col )]) ) base_t = alt . Chart ( table_count ) . mark_line () . encode ( x = alt . X ( 'yearmonth(datetime):T' ), y = alt . Y ( f 'sum(count):Q' , axis = alt . Axis ( format = \"s\" )), color = alt . Color ( col + ':N' , scale = color_scale ), opacity = alt . condition ( selections [ col ], alt . value ( 1 ), alt . value ( 0.3 )) ) . transform_filter ( reduce (( lambda x , y : x & y ), [ selections [ s ] for s in selections if ( s != col )]) ) . properties ( width = width , height = height ) #right = base_t.encode( # alt.X('datetime:T')#.scale(domain=selections[\"time\"]) #).properties(width=width, height=height) left = base_t #.add_params(selections[\"time\"]).properties(width=width, height=height) chart = ( chart | left #| right ) . properties ( title = col #, width=600, height=100 ) charts . append ( chart ) charts = alt . vconcat ( * charts ) . resolve_scale ( color = 'independent' ) . properties ( padding = { \"left\" : 50 , \"top\" : 50 , \"right\" : 50 , \"bottom\" : 50 }, title = { \"text\" : [ title ], \"subtitle\" : [ \"ALT + SHIFT to select multiple categories\" , \"Double-click on legend to unselect\" , \"Reduce table column and values size for better interactivity\" ], \"fontSize\" : 25 , \"subtitleFontSize\" : 15 , \"offset\" : 50 , \"subtitlePadding\" : 20 , }, ) . configure_legend ( columns = 4 , symbolLimit = 0 ) return charts","title":"table_viz"},{"location":"reference/plot/table_viz/#eds_scikitplottable_viz","text":"","title":"eds_scikit.plot.table_viz"},{"location":"reference/plot/table_viz/#eds_scikit.plot.table_viz.reduce_table","text":"reduce_table ( table : DataFrame , category_columns : List [ str ], date_column : str , start_date : str , end_date : str , mapper : dict = None ) -> DataFrame Reduce table PARAMETER DESCRIPTION table Input dataframe to reduce TYPE: DataFrame category_columns Columns to perform reduction on TYPE: List [ str ] date_column Date column TYPE: str start_date start date TYPE: str end_date end date TYPE: str RETURNS DESCRIPTION DataFrame Reducted DataFrame. RAISES DESCRIPTION ValueError description Source code in eds_scikit/plot/table_viz.py 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 def reduce_table ( table : DataFrame , category_columns : List [ str ], date_column : str , start_date : str , end_date : str , mapper : dict = None ) -> DataFrame : \"\"\"Reduce table Parameters ---------- table : DataFrame Input dataframe to reduce category_columns : List[str] Columns to perform reduction on date_column : str Date column start_date : str start date end_date : str end date Returns ------- DataFrame Reducted DataFrame. Raises ------ ValueError _description_ \"\"\" framework = get_framework ( table ) # assert qu'on a bien les bonnes colonnes table = table [( table [ date_column ] >= start_date ) & ( table [ date_column ] <= end_date )] table [ \"datetime\" ] = framework . to_datetime ( table [ date_column ] . dt . strftime ( '%Y-%m' )) category_columns = [ * category_columns , \"datetime\" ] if mapper : for col , mapping in mapper . items (): table = map_column ( table , mapping , col , col ) table = table [ category_columns ] shape = table . shape # to prevent computation errors nunique = table . nunique () oversized_columns = nunique [( nunique . index != \"datetime\" ) & ( nunique > 50 )] . tolist () if oversized_columns != []: raise ValueError ( \"Input table columns can't have more then 50 values. Consider using eds_scikit.plot.map_column.\" ) table_count = table . fillna ( \"NaN\" ) . groupby ( category_columns ) . size () . reset_index ( name = 'count' ) shape = table_count . shape # to prevent computation errors table_count = to ( \"pandas\" , table_count ) table_count [ \"datetime\" ] = pd . to_datetime ( table_count [ \"datetime\" ]) date_dataframe = pd . DataFrame ( pd . date_range ( start = start_date , end = end_date , freq = 'MS' ), columns = [ 'datetime' ]) table_count = table_count . merge ( date_dataframe , on = \"datetime\" , how = \"right\" ) table_count [ \"count\" ] = table_count [ \"count\" ] . fillna ( 0 ) table_count = table_count . fillna ( \"NaN\" ) return table_count","title":"reduce_table()"},{"location":"reference/plot/table_viz/#eds_scikit.plot.table_viz.visualize_table","text":"visualize_table ( table_count : DataFrame , title : str = 'table exploration dashboard' ) -> alt . Chart Generate reduced table dashboard. PARAMETER DESCRIPTION table_count Output from eds_scikit.plot.table_viz.reduce_table TYPE: DataFrame title Chart title TYPE: str DEFAULT: 'table exploration dashboard' RETURNS DESCRIPTION alt . Chart reduce_table dashboard RAISES DESCRIPTION ValueError description Source code in eds_scikit/plot/table_viz.py 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 def visualize_table ( table_count : DataFrame , title : str = \"table exploration dashboard\" ) -> alt . Chart : \"\"\"Generate reduced table dashboard. Parameters ---------- table_count : DataFrame Output from eds_scikit.plot.table_viz.reduce_table title : str Chart title Returns ------- alt.Chart reduce_table dashboard Raises ------ ValueError _description_ \"\"\" check_columns = [ \"count\" , \"datetime\" ] for check_column in check_columns : if not ( check_column in table_count . columns ): raise ValueError ( \"Input table must have a {check_column} column.\" ) selections = {} columns = [ col for col in table_count . columns if ( col != 'count' ) and ( col != \"datetime\" )] for col in columns : selections [ col ] = alt . selection_point ( fields = [ col ], on = 'click' , bind = 'legend' , clear = 'dblclick' ) #selections[\"time\"] = alt.selection_interval(encodings=['x'], translate=False, zoom=False) charts = [] for i , col in enumerate ( columns ): color_scale = generate_color_map ( table_count , col ) width , height = 200 , 50 chart = alt . Chart ( table_count ) . mark_bar () . encode ( x = f 'sum(count):Q' , color = alt . Color ( col + ':N' , scale = color_scale ), opacity = alt . condition ( selections [ col ], alt . value ( 1 ), alt . value ( 0.3 )), tooltip = [ col ] ) . add_params ( selections [ col ] ) . transform_filter ( reduce (( lambda x , y : x & y ), [ selections [ s ] for s in selections if ( s != col )]) ) base_t = alt . Chart ( table_count ) . mark_line () . encode ( x = alt . X ( 'yearmonth(datetime):T' ), y = alt . Y ( f 'sum(count):Q' , axis = alt . Axis ( format = \"s\" )), color = alt . Color ( col + ':N' , scale = color_scale ), opacity = alt . condition ( selections [ col ], alt . value ( 1 ), alt . value ( 0.3 )) ) . transform_filter ( reduce (( lambda x , y : x & y ), [ selections [ s ] for s in selections if ( s != col )]) ) . properties ( width = width , height = height ) #right = base_t.encode( # alt.X('datetime:T')#.scale(domain=selections[\"time\"]) #).properties(width=width, height=height) left = base_t #.add_params(selections[\"time\"]).properties(width=width, height=height) chart = ( chart | left #| right ) . properties ( title = col #, width=600, height=100 ) charts . append ( chart ) charts = alt . vconcat ( * charts ) . resolve_scale ( color = 'independent' ) . properties ( padding = { \"left\" : 50 , \"top\" : 50 , \"right\" : 50 , \"bottom\" : 50 }, title = { \"text\" : [ title ], \"subtitle\" : [ \"ALT + SHIFT to select multiple categories\" , \"Double-click on legend to unselect\" , \"Reduce table column and values size for better interactivity\" ], \"fontSize\" : 25 , \"subtitleFontSize\" : 15 , \"offset\" : 50 , \"subtitlePadding\" : 20 , }, ) . configure_legend ( columns = 4 , symbolLimit = 0 ) return charts","title":"visualize_table()"},{"location":"reference/resources/","text":"eds_scikit.resources","title":"`eds_scikit.resources`"},{"location":"reference/resources/#eds_scikitresources","text":"","title":"eds_scikit.resources"},{"location":"reference/resources/reg/","text":"eds_scikit.resources.reg Registry get get ( key : str , function_name : str ) Get a function from one of the registry PARAMETER DESCRIPTION key The registry's name. The function will be retrieved from self. TYPE: str function_name The function's name, The function will be retrieved via self. .get(function_name). Can be of the form \"function_name.version\" TYPE: str RETURNS DESCRIPTION Callable The registered function Source code in eds_scikit/resources/reg.py 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 def get ( self , key : str , function_name : str , ): \"\"\" Get a function from one of the registry Parameters ---------- key : str The registry's name. The function will be retrieved from self.<key> function_name : str The function's name, The function will be retrieved via self.<key>.get(function_name). Can be of the form \"function_name.version\" Returns ------- Callable The registered function \"\"\" if not hasattr ( self , key ): raise ValueError ( f \"eds-scikit's registry has no { key } key !\" ) r = getattr ( self , key ) candidates = r . get_all () . keys () if function_name in candidates : # Exact match func = r . get ( function_name ) else : # Looking for a match excluding version string candidates = [ func for func in candidates if function_name == func . split ( \".\" )[ 0 ] ] if len ( candidates ) > 1 : # Multiple versions available, a specific one should be specified raise ValueError ( ( f \"Multiple functions are available under the name { function_name } : \\n \" f \" { candidates } \\n \" \"Please choose one of the implementation listed above.\" ) ) if not candidates : # No registered function raise ValueError ( ( f \"No function registered under the name { function_name } \" f \"was found in eds-scikit's { key } registry. \\n \" \"If you work in AP-HP's ecosystem, you should install \" 'extra resources via `pip install \"eds-scikit[aphp]\"' \"You can define your own and decorate it as follow: \\n \" \"from eds_scikit.resources import registry \\n \" f \"@registry. { key } (' { function_name } ')\" f \"def your_custom_func(args, **kwargs):\" , \" ...\" , ) ) func = r . get ( candidates [ 0 ]) return func","title":"reg"},{"location":"reference/resources/reg/#eds_scikitresourcesreg","text":"","title":"eds_scikit.resources.reg"},{"location":"reference/resources/reg/#eds_scikit.resources.reg.Registry","text":"","title":"Registry"},{"location":"reference/resources/reg/#eds_scikit.resources.reg.Registry.get","text":"get ( key : str , function_name : str ) Get a function from one of the registry PARAMETER DESCRIPTION key The registry's name. The function will be retrieved from self. TYPE: str function_name The function's name, The function will be retrieved via self. .get(function_name). Can be of the form \"function_name.version\" TYPE: str RETURNS DESCRIPTION Callable The registered function Source code in eds_scikit/resources/reg.py 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 def get ( self , key : str , function_name : str , ): \"\"\" Get a function from one of the registry Parameters ---------- key : str The registry's name. The function will be retrieved from self.<key> function_name : str The function's name, The function will be retrieved via self.<key>.get(function_name). Can be of the form \"function_name.version\" Returns ------- Callable The registered function \"\"\" if not hasattr ( self , key ): raise ValueError ( f \"eds-scikit's registry has no { key } key !\" ) r = getattr ( self , key ) candidates = r . get_all () . keys () if function_name in candidates : # Exact match func = r . get ( function_name ) else : # Looking for a match excluding version string candidates = [ func for func in candidates if function_name == func . split ( \".\" )[ 0 ] ] if len ( candidates ) > 1 : # Multiple versions available, a specific one should be specified raise ValueError ( ( f \"Multiple functions are available under the name { function_name } : \\n \" f \" { candidates } \\n \" \"Please choose one of the implementation listed above.\" ) ) if not candidates : # No registered function raise ValueError ( ( f \"No function registered under the name { function_name } \" f \"was found in eds-scikit's { key } registry. \\n \" \"If you work in AP-HP's ecosystem, you should install \" 'extra resources via `pip install \"eds-scikit[aphp]\"' \"You can define your own and decorate it as follow: \\n \" \"from eds_scikit.resources import registry \\n \" f \"@registry. { key } (' { function_name } ')\" f \"def your_custom_func(args, **kwargs):\" , \" ...\" , ) ) func = r . get ( candidates [ 0 ]) return func","title":"get()"},{"location":"reference/resources/utils/","text":"eds_scikit.resources.utils versionize versionize ( algo : str ) -> Optional [ str ] Extract, if found, the version substring of an algorithm name. PARAMETER DESCRIPTION algo Of the form \" \" or \" . \" TYPE: str RETURNS DESCRIPTION Optional [ str ] The algo version suffix Source code in eds_scikit/resources/utils.py 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 def versionize ( algo : str ) -> Optional [ str ]: \"\"\" Extract, if found, the version substring of an algorithm name. Parameters ---------- algo : str Of the form \"<algo_name>\" or \"<algo_name>.<version>\" Returns ------- Optional[str] The algo version suffix \"\"\" splited = algo . split ( \".\" ) if len ( splited ) == 1 : return None return splited [ - 1 ]","title":"utils"},{"location":"reference/resources/utils/#eds_scikitresourcesutils","text":"","title":"eds_scikit.resources.utils"},{"location":"reference/resources/utils/#eds_scikit.resources.utils.versionize","text":"versionize ( algo : str ) -> Optional [ str ] Extract, if found, the version substring of an algorithm name. PARAMETER DESCRIPTION algo Of the form \" \" or \" . \" TYPE: str RETURNS DESCRIPTION Optional [ str ] The algo version suffix Source code in eds_scikit/resources/utils.py 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 def versionize ( algo : str ) -> Optional [ str ]: \"\"\" Extract, if found, the version substring of an algorithm name. Parameters ---------- algo : str Of the form \"<algo_name>\" or \"<algo_name>.<version>\" Returns ------- Optional[str] The algo version suffix \"\"\" splited = algo . split ( \".\" ) if len ( splited ) == 1 : return None return splited [ - 1 ]","title":"versionize()"},{"location":"reference/structures/","text":"eds_scikit.structures","title":"`eds_scikit.structures`"},{"location":"reference/structures/#eds_scikitstructures","text":"","title":"eds_scikit.structures"},{"location":"reference/structures/attributes/","text":"eds_scikit.structures.attributes ATTRIBUTE_REGEX_PATTERNS module-attribute ATTRIBUTE_REGEX_PATTERNS = [{ 'attribute' : 'IS_EMERGENCY' , 'pattern' : ' \\\\ bURG| \\\\ bSAU \\\\ b| \\\\ bUHCD \\\\ b| \\\\ bZHTCD \\\\ b' , 'true_examples' : [ 'URG' , 'URGENCES' , 'SAU' ], 'false_examples' : [ 'CHIRURGIE' ]}, { 'attribute' : 'IS_ICU' , 'pattern' : ' \\\\ bUSI| \\\\ bREA[N \\\\ s]| \\\\ bREA \\\\ b| \\\\ bUSC \\\\ b|SOINS.*INTENSIF|SURV.{0,15}CONT| \\\\ bSI \\\\ b| \\\\ bSC \\\\ b' , 'true_examples' : [ 'REA' , 'REA NEURO' , 'REANIMATION' ], 'false_examples' : [ 'CARREAU' ]}] Default argument of func: ~eds_scikit.structures.attributes.add_care_site_attributes . :meta private: Examples: :: ATTRIBUTE_REGEX_PATTERNS = [ { # required elements: name of attribute and pattern of regular expression \"attribute\": \"IS_EMERGENCY\", \"pattern\": r\"\bURG|\bSAU\b|\bUHCD\b|\bZHTCD\b\", # optional elements: list of test strings to validate the regular expression \"true_examples\": [\"URG\", \"URGENCES\", \"SAU\"], \"false_examples\": [\"CHIRURGIE\"], }, ... ] add_care_site_attributes add_care_site_attributes ( care_site : DataFrame , only_attributes : Optional [ List [ str ]] = None , attribute_regex_patterns : Optional [ List [ str ]] = None ) -> DataFrame Add boolean attributes as columns to care_site dataframe. This algo applies simple regular expressions to the care_site_name in order to compute boolean attributes of the care site. Implemented attributes are: IS_EMERGENCY IS_ICU In order to make the detection of attributes more robust, the column care_site_name is first transformed to a DESCRIPTION . This is done by func: ~eds_scikit.structures.description.add_care_site_description . PARAMETER DESCRIPTION care_site TYPE: DataFrame only_attributes if only a subset of all possible attributes should be computed TYPE: list of str DEFAULT: None attribute_regex_patterns If None , the default value is data: ~eds_scikit.structures.attributes.ATTRIBUTE_REGEX_PATTERNS TYPE: list (None) DEFAULT: None RETURNS DESCRIPTION care_site same as input with additional columns corresponding to boolean attributes. the column DESCRIPTION is also added : it contains of cleaner version of care_site_name . TYPE: DataFrame Examples: >>> care_site . head ( 2 ) care_site_id, care_site_name 21, HOSP ACCUEIL URG PED (UF) 22, HOSP CHIRURGIE DIGESTIVE 23, HOSP PEDIATRIE GEN ET SAU >>> care_site = add_care_site_attributes ( care_site , only_attributes = [ \"IS_EMERGENCY\" ]) >>> care_site . head ( 2 ) care_site_id, care_site_name, DESCRIPTION, IS_EMERGENCY 21, HOSP ACCUEIL URG PED (UF),ACCUEIL URG PED,True 22, HOSP CHIRURGIE DIGESTIVE,CHIRURGIE DIGESTIVE,False 23, HOSP PEDIATRIE GEN ET SAU,PEDIATRIE GEN ET SAU,True Specifying custom regular expressions. It is a good idea to provide true and false examples for each attribute. These examples will be tested against the provided regular expressions. >>> my_attributes = [ { \"attribute\": \"IS_EMERGENCY\", \"pattern\": r\"\bURG|\bSAU\b|\bUHCD\b|\bZHTCD\b\", \"true_examples\": [\"URG\", \"URGENCES\", \"SAU\"], \"false_examples\": [\"CHIRURGIE\"], }, { \"attribute\": \"IS_ICU\", \"pattern\": r\"\bREA\b|\bREANI\", \"true_examples\": [\"REA\", \"REA NEURO\", \"REANIMATION\"], \"false_examples\": [\"CARREAU\"], }, ] >>> care_site = add_care_site_attributes ( care_site , attribute_regex_patterns = my_attributes ) Source code in eds_scikit/structures/attributes.py 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 def add_care_site_attributes ( care_site : DataFrame , only_attributes : Optional [ List [ str ]] = None , attribute_regex_patterns : Optional [ List [ str ]] = None , ) -> DataFrame : \"\"\"Add boolean attributes as columns to care_site dataframe. This algo applies simple regular expressions to the ``care_site_name`` in order to compute boolean attributes of the care site. Implemented attributes are: - ``IS_EMERGENCY`` - ``IS_ICU`` In order to make the detection of attributes more robust, the column ``care_site_name`` is first transformed to a ``DESCRIPTION``. This is done by :py:func:`~eds_scikit.structures.description.add_care_site_description`. Parameters ---------- care_site : DataFrame only_attributes : list of str if only a subset of all possible attributes should be computed attribute_regex_patterns : list (None) If ``None``, the default value is :py:data:`~eds_scikit.structures.attributes.ATTRIBUTE_REGEX_PATTERNS` Returns ------- care_site: DataFrame same as input with additional columns corresponding to boolean attributes. the column ``DESCRIPTION`` is also added : it contains of cleaner version of ``care_site_name``. Examples -------- >>> care_site.head(2) care_site_id, care_site_name 21, HOSP ACCUEIL URG PED (UF) 22, HOSP CHIRURGIE DIGESTIVE 23, HOSP PEDIATRIE GEN ET SAU >>> care_site = add_care_site_attributes(care_site, only_attributes=[\"IS_EMERGENCY\"]) >>> care_site.head(2) care_site_id, care_site_name, DESCRIPTION, IS_EMERGENCY 21, HOSP ACCUEIL URG PED (UF),ACCUEIL URG PED,True 22, HOSP CHIRURGIE DIGESTIVE,CHIRURGIE DIGESTIVE,False 23, HOSP PEDIATRIE GEN ET SAU,PEDIATRIE GEN ET SAU,True Specifying custom regular expressions. It is a good idea to provide true and false examples for each attribute. These examples will be tested against the provided regular expressions. >>> my_attributes = [ { \"attribute\": \"IS_EMERGENCY\", \"pattern\": r\"\\bURG|\\bSAU\\b|\\bUHCD\\b|\\bZHTCD\\b\", \"true_examples\": [\"URG\", \"URGENCES\", \"SAU\"], \"false_examples\": [\"CHIRURGIE\"], }, { \"attribute\": \"IS_ICU\", \"pattern\": r\"\\bREA\\b|\\bREANI\", \"true_examples\": [\"REA\", \"REA NEURO\", \"REANIMATION\"], \"false_examples\": [\"CARREAU\"], }, ] >>> care_site = add_care_site_attributes(care_site, attribute_regex_patterns=my_attributes) \"\"\" # validate arguments if attribute_regex_patterns is None : attribute_regex_patterns = ATTRIBUTE_REGEX_PATTERNS if only_attributes : impossible = set ( only_attributes ) - set ( possible_concepts ) if impossible : raise ValueError ( f \"Unknown concepts: { impossible } \" ) attribute_regex_patterns = [ item for item in attribute_regex_patterns if item [ \"attribute\" ] in only_attributes ] validate_attribute_regex_patterns ( attribute_regex_patterns ) if \"DESCRIPTION\" not in care_site . columns : care_site = description . add_care_site_description ( care_site ) # apply algo for item in attribute_regex_patterns : new_column = { item [ \"attribute\" ]: care_site [ \"DESCRIPTION\" ] . str . contains ( item [ \"pattern\" ], regex = True ) } care_site = care_site . assign ( ** new_column ) if only_attributes : care_site = care_site . drop ([ \"DESCRIPTION\" ], axis = \"columns\" ) return care_site get_parent_attributes get_parent_attributes ( care_site : DataFrame , only_attributes : Optional [ List [ str ]] = None , version : Optional [ str ] = None , parent_type : str = 'Unit\u00e9 Fonctionnelle (UF)' ) -> DataFrame Get all known attributes from parent care sites and propagates them to each child care site PARAMETER DESCRIPTION care_site required columns: [\"care_site_id\", \"care_site_type_source_value\", \"care_site_name\"] TYPE: DataFrame only_attributes same as func: ~eds_scikit.structures.attributes.add_care_site_attributes TYPE: list of str DEFAULT: None version Optional version string for the care site hierarchy TYPE: Optional [ str ] DEFAULT: None parent_type Type of care site to consider as parent, by default \"Unit\u00e9 Fonctionnelle (UF)\". Corresponds to the \"care_site_type_source_value\" column TYPE: str DEFAULT: 'Unit\u00e9 Fonctionnelle (UF)' RETURNS DESCRIPTION care_site_attributes same index as input care_site. columns: care_site, is_emergency TYPE: DataFrame Warnings This algo requires that the care_site dataframe contains the parent care sites as well as the care sites that you want to tag. Examples: >>> attributes = get_parent_attributes ( care_site , only_attributes=[\"IS_EMERGENCY\"], parent_type=\"Unit\u00e9 Fonctionnelle (UF)\") >>> attributes . head () care_site_id, care_site_name, care_site_type_source_value, IS_EMERGENCY 92829 , ... , False 29820 , ... , True Source code in eds_scikit/structures/attributes.py 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 def get_parent_attributes ( care_site : DataFrame , only_attributes : Optional [ List [ str ]] = None , version : Optional [ str ] = None , parent_type : str = \"Unit\u00e9 Fonctionnelle (UF)\" , ) -> DataFrame : \"\"\"Get all known attributes from parent care sites and propagates them to each child care site Parameters ---------- care_site: DataFrame required columns: ``[\"care_site_id\", \"care_site_type_source_value\", \"care_site_name\"]`` only_attributes : list of str same as :py:func:`~eds_scikit.structures.attributes.add_care_site_attributes` version: Optional[str] Optional version string for the care site hierarchy parent_type: str Type of care site to consider as parent, by default \"Unit\u00e9 Fonctionnelle (UF)\". Corresponds to the `\"care_site_type_source_value\"` column Returns -------- care_site_attributes: DataFrame same index as input care_site. columns: care_site, is_emergency Warnings -------- This algo requires that the `care_site` dataframe contains the parent care sites as well as the care sites that you want to tag. Examples -------- >>> attributes = get_parent_attributes(care_site, only_attributes=[\"IS_EMERGENCY\"], parent_type=\"Unit\u00e9 Fonctionnelle (UF)\") >>> attributes.head() care_site_id, care_site_name, care_site_type_source_value, IS_EMERGENCY 92829 , ... , False 29820 , ... , True \"\"\" function_name = \"get_care_site_hierarchy\" if version is not None : function_name += f \". { version } \" hierarchy = registry . get ( \"data\" , function_name = function_name )() fw = framework . get_framework ( care_site ) hierarchy = framework . to ( fw , hierarchy ) # STEP 1: get attributes of parent parent_attributes = care_site . loc [ care_site [ \"care_site_type_source_value\" ] == parent_type , [ \"care_site_id\" , \"care_site_name\" ], ] parent_attributes = add_care_site_attributes ( parent_attributes , only_attributes = only_attributes ) boolean_columns = [ col for ( col , dtype ) in parent_attributes . dtypes . iteritems () if dtype == \"bool\" ] parent_attributes = parent_attributes . drop ( [ \"care_site_name\" ], axis = \"columns\" ) . rename ( columns = { \"care_site_id\" : \"parent_id\" }) # STEP 2: propagate attributes from parent to all children hierarchy = hierarchy . loc [:, [ \"care_site_id\" , parent_type ]] . rename ( columns = { parent_type : \"parent_id\" } ) children_attributes = hierarchy . merge ( parent_attributes , how = \"left\" , on = \"parent_id\" ) . drop ([ \"parent_id\" ], axis = \"columns\" ) # STEP 3 : merge to input dataframe old_columns = care_site . columns care_site = care_site . merge ( children_attributes , how = \"left\" , on = \"care_site_id\" ) for col in care_site . columns : if col in boolean_columns and col not in old_columns : care_site [ col ] = care_site [ col ] . fillna ( value = False ) return care_site # NOTE: this is how to return a single column that contains # EXACTLY the same index as the input dataframe. # For instance koalas requires the index name to be the same # for this operation to be valid: # >>> df[\"new_column\"] = compute_column(df) # attributes = ( # care_site.loc[:, [\"care_site_id\"]] # .reset_index() # .merge( # # drop_duplicates to ensure we keep same size as input # children_attributes.drop_duplicates(subset=[\"care_site_id\"]), # how=\"left\", # on=\"care_site_id\", # ) # .fillna(value=False) # # a merge \"forgets\" the index, we want to output the same as input # .set_index(\"index\") # ) # attributes.index.name = care_site.index.name","title":"attributes"},{"location":"reference/structures/attributes/#eds_scikitstructuresattributes","text":"","title":"eds_scikit.structures.attributes"},{"location":"reference/structures/attributes/#eds_scikit.structures.attributes.ATTRIBUTE_REGEX_PATTERNS","text":"ATTRIBUTE_REGEX_PATTERNS = [{ 'attribute' : 'IS_EMERGENCY' , 'pattern' : ' \\\\ bURG| \\\\ bSAU \\\\ b| \\\\ bUHCD \\\\ b| \\\\ bZHTCD \\\\ b' , 'true_examples' : [ 'URG' , 'URGENCES' , 'SAU' ], 'false_examples' : [ 'CHIRURGIE' ]}, { 'attribute' : 'IS_ICU' , 'pattern' : ' \\\\ bUSI| \\\\ bREA[N \\\\ s]| \\\\ bREA \\\\ b| \\\\ bUSC \\\\ b|SOINS.*INTENSIF|SURV.{0,15}CONT| \\\\ bSI \\\\ b| \\\\ bSC \\\\ b' , 'true_examples' : [ 'REA' , 'REA NEURO' , 'REANIMATION' ], 'false_examples' : [ 'CARREAU' ]}] Default argument of func: ~eds_scikit.structures.attributes.add_care_site_attributes . :meta private: Examples: :: ATTRIBUTE_REGEX_PATTERNS = [ { # required elements: name of attribute and pattern of regular expression \"attribute\": \"IS_EMERGENCY\", \"pattern\": r\"\bURG|\bSAU\b|\bUHCD\b|\bZHTCD\b\", # optional elements: list of test strings to validate the regular expression \"true_examples\": [\"URG\", \"URGENCES\", \"SAU\"], \"false_examples\": [\"CHIRURGIE\"], }, ... ]","title":"ATTRIBUTE_REGEX_PATTERNS"},{"location":"reference/structures/attributes/#eds_scikit.structures.attributes.add_care_site_attributes","text":"add_care_site_attributes ( care_site : DataFrame , only_attributes : Optional [ List [ str ]] = None , attribute_regex_patterns : Optional [ List [ str ]] = None ) -> DataFrame Add boolean attributes as columns to care_site dataframe. This algo applies simple regular expressions to the care_site_name in order to compute boolean attributes of the care site. Implemented attributes are: IS_EMERGENCY IS_ICU In order to make the detection of attributes more robust, the column care_site_name is first transformed to a DESCRIPTION . This is done by func: ~eds_scikit.structures.description.add_care_site_description . PARAMETER DESCRIPTION care_site TYPE: DataFrame only_attributes if only a subset of all possible attributes should be computed TYPE: list of str DEFAULT: None attribute_regex_patterns If None , the default value is data: ~eds_scikit.structures.attributes.ATTRIBUTE_REGEX_PATTERNS TYPE: list (None) DEFAULT: None RETURNS DESCRIPTION care_site same as input with additional columns corresponding to boolean attributes. the column DESCRIPTION is also added : it contains of cleaner version of care_site_name . TYPE: DataFrame Examples: >>> care_site . head ( 2 ) care_site_id, care_site_name 21, HOSP ACCUEIL URG PED (UF) 22, HOSP CHIRURGIE DIGESTIVE 23, HOSP PEDIATRIE GEN ET SAU >>> care_site = add_care_site_attributes ( care_site , only_attributes = [ \"IS_EMERGENCY\" ]) >>> care_site . head ( 2 ) care_site_id, care_site_name, DESCRIPTION, IS_EMERGENCY 21, HOSP ACCUEIL URG PED (UF),ACCUEIL URG PED,True 22, HOSP CHIRURGIE DIGESTIVE,CHIRURGIE DIGESTIVE,False 23, HOSP PEDIATRIE GEN ET SAU,PEDIATRIE GEN ET SAU,True Specifying custom regular expressions. It is a good idea to provide true and false examples for each attribute. These examples will be tested against the provided regular expressions. >>> my_attributes = [ { \"attribute\": \"IS_EMERGENCY\", \"pattern\": r\"\bURG|\bSAU\b|\bUHCD\b|\bZHTCD\b\", \"true_examples\": [\"URG\", \"URGENCES\", \"SAU\"], \"false_examples\": [\"CHIRURGIE\"], }, { \"attribute\": \"IS_ICU\", \"pattern\": r\"\bREA\b|\bREANI\", \"true_examples\": [\"REA\", \"REA NEURO\", \"REANIMATION\"], \"false_examples\": [\"CARREAU\"], }, ] >>> care_site = add_care_site_attributes ( care_site , attribute_regex_patterns = my_attributes ) Source code in eds_scikit/structures/attributes.py 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 def add_care_site_attributes ( care_site : DataFrame , only_attributes : Optional [ List [ str ]] = None , attribute_regex_patterns : Optional [ List [ str ]] = None , ) -> DataFrame : \"\"\"Add boolean attributes as columns to care_site dataframe. This algo applies simple regular expressions to the ``care_site_name`` in order to compute boolean attributes of the care site. Implemented attributes are: - ``IS_EMERGENCY`` - ``IS_ICU`` In order to make the detection of attributes more robust, the column ``care_site_name`` is first transformed to a ``DESCRIPTION``. This is done by :py:func:`~eds_scikit.structures.description.add_care_site_description`. Parameters ---------- care_site : DataFrame only_attributes : list of str if only a subset of all possible attributes should be computed attribute_regex_patterns : list (None) If ``None``, the default value is :py:data:`~eds_scikit.structures.attributes.ATTRIBUTE_REGEX_PATTERNS` Returns ------- care_site: DataFrame same as input with additional columns corresponding to boolean attributes. the column ``DESCRIPTION`` is also added : it contains of cleaner version of ``care_site_name``. Examples -------- >>> care_site.head(2) care_site_id, care_site_name 21, HOSP ACCUEIL URG PED (UF) 22, HOSP CHIRURGIE DIGESTIVE 23, HOSP PEDIATRIE GEN ET SAU >>> care_site = add_care_site_attributes(care_site, only_attributes=[\"IS_EMERGENCY\"]) >>> care_site.head(2) care_site_id, care_site_name, DESCRIPTION, IS_EMERGENCY 21, HOSP ACCUEIL URG PED (UF),ACCUEIL URG PED,True 22, HOSP CHIRURGIE DIGESTIVE,CHIRURGIE DIGESTIVE,False 23, HOSP PEDIATRIE GEN ET SAU,PEDIATRIE GEN ET SAU,True Specifying custom regular expressions. It is a good idea to provide true and false examples for each attribute. These examples will be tested against the provided regular expressions. >>> my_attributes = [ { \"attribute\": \"IS_EMERGENCY\", \"pattern\": r\"\\bURG|\\bSAU\\b|\\bUHCD\\b|\\bZHTCD\\b\", \"true_examples\": [\"URG\", \"URGENCES\", \"SAU\"], \"false_examples\": [\"CHIRURGIE\"], }, { \"attribute\": \"IS_ICU\", \"pattern\": r\"\\bREA\\b|\\bREANI\", \"true_examples\": [\"REA\", \"REA NEURO\", \"REANIMATION\"], \"false_examples\": [\"CARREAU\"], }, ] >>> care_site = add_care_site_attributes(care_site, attribute_regex_patterns=my_attributes) \"\"\" # validate arguments if attribute_regex_patterns is None : attribute_regex_patterns = ATTRIBUTE_REGEX_PATTERNS if only_attributes : impossible = set ( only_attributes ) - set ( possible_concepts ) if impossible : raise ValueError ( f \"Unknown concepts: { impossible } \" ) attribute_regex_patterns = [ item for item in attribute_regex_patterns if item [ \"attribute\" ] in only_attributes ] validate_attribute_regex_patterns ( attribute_regex_patterns ) if \"DESCRIPTION\" not in care_site . columns : care_site = description . add_care_site_description ( care_site ) # apply algo for item in attribute_regex_patterns : new_column = { item [ \"attribute\" ]: care_site [ \"DESCRIPTION\" ] . str . contains ( item [ \"pattern\" ], regex = True ) } care_site = care_site . assign ( ** new_column ) if only_attributes : care_site = care_site . drop ([ \"DESCRIPTION\" ], axis = \"columns\" ) return care_site","title":"add_care_site_attributes()"},{"location":"reference/structures/attributes/#eds_scikit.structures.attributes.get_parent_attributes","text":"get_parent_attributes ( care_site : DataFrame , only_attributes : Optional [ List [ str ]] = None , version : Optional [ str ] = None , parent_type : str = 'Unit\u00e9 Fonctionnelle (UF)' ) -> DataFrame Get all known attributes from parent care sites and propagates them to each child care site PARAMETER DESCRIPTION care_site required columns: [\"care_site_id\", \"care_site_type_source_value\", \"care_site_name\"] TYPE: DataFrame only_attributes same as func: ~eds_scikit.structures.attributes.add_care_site_attributes TYPE: list of str DEFAULT: None version Optional version string for the care site hierarchy TYPE: Optional [ str ] DEFAULT: None parent_type Type of care site to consider as parent, by default \"Unit\u00e9 Fonctionnelle (UF)\". Corresponds to the \"care_site_type_source_value\" column TYPE: str DEFAULT: 'Unit\u00e9 Fonctionnelle (UF)' RETURNS DESCRIPTION care_site_attributes same index as input care_site. columns: care_site, is_emergency TYPE: DataFrame","title":"get_parent_attributes()"},{"location":"reference/structures/attributes/#eds_scikit.structures.attributes.get_parent_attributes--warnings","text":"This algo requires that the care_site dataframe contains the parent care sites as well as the care sites that you want to tag. Examples: >>> attributes = get_parent_attributes ( care_site , only_attributes=[\"IS_EMERGENCY\"], parent_type=\"Unit\u00e9 Fonctionnelle (UF)\") >>> attributes . head () care_site_id, care_site_name, care_site_type_source_value, IS_EMERGENCY 92829 , ... , False 29820 , ... , True Source code in eds_scikit/structures/attributes.py 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 def get_parent_attributes ( care_site : DataFrame , only_attributes : Optional [ List [ str ]] = None , version : Optional [ str ] = None , parent_type : str = \"Unit\u00e9 Fonctionnelle (UF)\" , ) -> DataFrame : \"\"\"Get all known attributes from parent care sites and propagates them to each child care site Parameters ---------- care_site: DataFrame required columns: ``[\"care_site_id\", \"care_site_type_source_value\", \"care_site_name\"]`` only_attributes : list of str same as :py:func:`~eds_scikit.structures.attributes.add_care_site_attributes` version: Optional[str] Optional version string for the care site hierarchy parent_type: str Type of care site to consider as parent, by default \"Unit\u00e9 Fonctionnelle (UF)\". Corresponds to the `\"care_site_type_source_value\"` column Returns -------- care_site_attributes: DataFrame same index as input care_site. columns: care_site, is_emergency Warnings -------- This algo requires that the `care_site` dataframe contains the parent care sites as well as the care sites that you want to tag. Examples -------- >>> attributes = get_parent_attributes(care_site, only_attributes=[\"IS_EMERGENCY\"], parent_type=\"Unit\u00e9 Fonctionnelle (UF)\") >>> attributes.head() care_site_id, care_site_name, care_site_type_source_value, IS_EMERGENCY 92829 , ... , False 29820 , ... , True \"\"\" function_name = \"get_care_site_hierarchy\" if version is not None : function_name += f \". { version } \" hierarchy = registry . get ( \"data\" , function_name = function_name )() fw = framework . get_framework ( care_site ) hierarchy = framework . to ( fw , hierarchy ) # STEP 1: get attributes of parent parent_attributes = care_site . loc [ care_site [ \"care_site_type_source_value\" ] == parent_type , [ \"care_site_id\" , \"care_site_name\" ], ] parent_attributes = add_care_site_attributes ( parent_attributes , only_attributes = only_attributes ) boolean_columns = [ col for ( col , dtype ) in parent_attributes . dtypes . iteritems () if dtype == \"bool\" ] parent_attributes = parent_attributes . drop ( [ \"care_site_name\" ], axis = \"columns\" ) . rename ( columns = { \"care_site_id\" : \"parent_id\" }) # STEP 2: propagate attributes from parent to all children hierarchy = hierarchy . loc [:, [ \"care_site_id\" , parent_type ]] . rename ( columns = { parent_type : \"parent_id\" } ) children_attributes = hierarchy . merge ( parent_attributes , how = \"left\" , on = \"parent_id\" ) . drop ([ \"parent_id\" ], axis = \"columns\" ) # STEP 3 : merge to input dataframe old_columns = care_site . columns care_site = care_site . merge ( children_attributes , how = \"left\" , on = \"care_site_id\" ) for col in care_site . columns : if col in boolean_columns and col not in old_columns : care_site [ col ] = care_site [ col ] . fillna ( value = False ) return care_site # NOTE: this is how to return a single column that contains # EXACTLY the same index as the input dataframe. # For instance koalas requires the index name to be the same # for this operation to be valid: # >>> df[\"new_column\"] = compute_column(df) # attributes = ( # care_site.loc[:, [\"care_site_id\"]] # .reset_index() # .merge( # # drop_duplicates to ensure we keep same size as input # children_attributes.drop_duplicates(subset=[\"care_site_id\"]), # how=\"left\", # on=\"care_site_id\", # ) # .fillna(value=False) # # a merge \"forgets\" the index, we want to output the same as input # .set_index(\"index\") # ) # attributes.index.name = care_site.index.name","title":"Warnings"},{"location":"reference/structures/description/","text":"eds_scikit.structures.description add_care_site_description add_care_site_description ( care_site : DataFrame ) -> DataFrame Add column DESCRIPTION to care_site dataframe. This algo applies simple regular expression to simplify the care site name. This can be useful for post-processing the description, such as detecting the care_site characteristic from the description (is it an emergency care site ?) PARAMETER DESCRIPTION care_site with column care_site_name TYPE: DataFrame RETURNS DESCRIPTION care_site contains additional column DESCRIPTION TYPE: DataFrame Source code in eds_scikit/structures/description.py 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 @concept_checker ( concepts = [ \"DESCRIPTION\" ]) def add_care_site_description ( care_site : DataFrame ) -> DataFrame : \"\"\"Add column ``DESCRIPTION`` to care_site dataframe. This algo applies simple regular expression to simplify the care site name. This can be useful for post-processing the description, such as detecting the care_site characteristic from the description (is it an emergency care site ?) Parameters ---------- care_site : DataFrame with column ``care_site_name`` Returns ------- care_site : DataFrame contains additional column ``DESCRIPTION`` \"\"\" care_site = care_site . assign ( DESCRIPTION = description_from_care_site_name ( care_site [ \"care_site_name\" ]) ) return care_site","title":"description"},{"location":"reference/structures/description/#eds_scikitstructuresdescription","text":"","title":"eds_scikit.structures.description"},{"location":"reference/structures/description/#eds_scikit.structures.description.add_care_site_description","text":"add_care_site_description ( care_site : DataFrame ) -> DataFrame Add column DESCRIPTION to care_site dataframe. This algo applies simple regular expression to simplify the care site name. This can be useful for post-processing the description, such as detecting the care_site characteristic from the description (is it an emergency care site ?) PARAMETER DESCRIPTION care_site with column care_site_name TYPE: DataFrame RETURNS DESCRIPTION care_site contains additional column DESCRIPTION TYPE: DataFrame Source code in eds_scikit/structures/description.py 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 @concept_checker ( concepts = [ \"DESCRIPTION\" ]) def add_care_site_description ( care_site : DataFrame ) -> DataFrame : \"\"\"Add column ``DESCRIPTION`` to care_site dataframe. This algo applies simple regular expression to simplify the care site name. This can be useful for post-processing the description, such as detecting the care_site characteristic from the description (is it an emergency care site ?) Parameters ---------- care_site : DataFrame with column ``care_site_name`` Returns ------- care_site : DataFrame contains additional column ``DESCRIPTION`` \"\"\" care_site = care_site . assign ( DESCRIPTION = description_from_care_site_name ( care_site [ \"care_site_name\" ]) ) return care_site","title":"add_care_site_description()"},{"location":"reference/utils/","text":"eds_scikit.utils","title":"`eds_scikit.utils`"},{"location":"reference/utils/#eds_scikitutils","text":"","title":"eds_scikit.utils"},{"location":"reference/utils/bunch/","text":"eds_scikit.utils.bunch Vendored Bunch class from scikit-learn. Bunch Bunch ( ** kwargs ) Bases: dict Container object exposing keys as attributes. Bunch objects are sometimes used as an output for functions and methods. They extend dictionaries by enabling values to be accessed by key, bunch[\"value_key\"] , or by an attribute, bunch.value_key . Examples: >>> from sklearn.utils import Bunch >>> b = Bunch ( a = 1 , b = 2 ) >>> b [ 'b' ] 2 >>> b . b 2 >>> b . a = 3 >>> b [ 'a' ] 3 >>> b . c = 6 >>> b [ 'c' ] 6 Source code in eds_scikit/utils/bunch.py 29 30 def __init__ ( self , ** kwargs ): super () . __init__ ( kwargs )","title":"bunch"},{"location":"reference/utils/bunch/#eds_scikitutilsbunch","text":"Vendored Bunch class from scikit-learn.","title":"eds_scikit.utils.bunch"},{"location":"reference/utils/bunch/#eds_scikit.utils.bunch.Bunch","text":"Bunch ( ** kwargs ) Bases: dict Container object exposing keys as attributes. Bunch objects are sometimes used as an output for functions and methods. They extend dictionaries by enabling values to be accessed by key, bunch[\"value_key\"] , or by an attribute, bunch.value_key . Examples: >>> from sklearn.utils import Bunch >>> b = Bunch ( a = 1 , b = 2 ) >>> b [ 'b' ] 2 >>> b . b 2 >>> b . a = 3 >>> b [ 'a' ] 3 >>> b . c = 6 >>> b [ 'c' ] 6 Source code in eds_scikit/utils/bunch.py 29 30 def __init__ ( self , ** kwargs ): super () . __init__ ( kwargs )","title":"Bunch"},{"location":"reference/utils/checks/","text":"eds_scikit.utils.checks MissingConceptError MissingConceptError ( required_concepts : Union [ List [ str ], List [ Tuple [ str , str ]]], df_name : str = '' ) Bases: Exception Exception raised when a concept is missing Source code in eds_scikit/utils/checks.py 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 def __init__ ( self , required_concepts : Union [ List [ str ], List [ Tuple [ str , str ]]], df_name : str = \"\" , ): if all ( isinstance ( concept , tuple ) for concept in required_concepts ): to_display_per_concept = [ f \"- { concept } ( { msg } )\" for concept , msg in required_concepts ] else : to_display_per_concept = [ f \"- { concept } \" for concept in required_concepts ] str_to_display = \" \\n \" . join ( to_display_per_concept ) if df_name : df_name = f \" { df_name } \" message = ( f \"The { df_name } DataFrame is missing some columns, \" \"namely: \\n \" f \" { str_to_display } \" ) super () . __init__ ( message ) MissingTableError MissingTableError ( required_tables : Union [ List [ str ], List [ Tuple [ str , str ]]], data_name : str = '' ) Bases: Exception Exception raised when a table is missing in the Data Source code in eds_scikit/utils/checks.py 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 def __init__ ( self , required_tables : Union [ List [ str ], List [ Tuple [ str , str ]]], data_name : str = \"\" , ): if all ( isinstance ( table , tuple ) for table in required_tables ): to_display_per_table = [ f \"- { table } ( { msg } )\" for table , msg in required_tables ] else : to_display_per_table = [ f \"- { table } \" for table in required_tables ] str_to_display = \" \\n \" . join ( to_display_per_table ) if data_name : data_name = f \" { data_name } \" message = ( f \"The { data_name } Data is missing some tables, \" \"namely: \\n \" f \" { str_to_display } \" ) super () . __init__ ( message ) concept_checker concept_checker ( function : Callable , concepts : List [ str ] = None , only_adds_concepts : bool = True , * args , ** kwargs ) -> Any Decorator to use on functions that - Takes a DataFrame as first argument - Adds a concept to it The decorator checks: - If the first argument is a DataFrame - If the concepts to be added aren't already in the DataFrame - If the function correctly adds the concepts - If no additionnal columns are added (if only_adds_concepts is True) If one of this checks fails, raises an error Source code in eds_scikit/utils/checks.py 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 @decorator def concept_checker ( function : Callable , concepts : List [ str ] = None , only_adds_concepts : bool = True , * args , ** kwargs , ) -> Any : \"\"\" Decorator to use on functions that - Takes a DataFrame as first argument - Adds a concept to it The decorator checks: - If the first argument is a DataFrame - If the concepts to be added aren't already in the DataFrame - If the function correctly adds the concepts - If no additionnal columns are added (if only_adds_concepts is True) If one of this checks fails, raises an error \"\"\" # Is the first argument a DataFrame df = args [ 0 ] if ( type ( df ) != ks . DataFrame ) & ( type ( df ) != pd . DataFrame ): raise TypeError ( f \"The first argument of ' { function . __module__ } . { function . __name__ } ' \" \"should be a Pandas or Koalas DataFrame\" ) # Initial columns initial_cols = set ( df . columns ) # Is the concept already present if type ( concepts ) == str : concepts = [ concepts ] present_concepts = set ( concepts ) & set ( df . columns ) if present_concepts : raise ValueError ( f \"The concepts { present_concepts } are already present in the input dataframe \" f \"of ' { function . __module__ } . { function . __name__ } '. \\n \" \"You can either rename the column(s) or delete them before running \" \"the function again.\" ) result = function ( * args , ** kwargs ) # Was the concept correctly added missing_concepts = set ( concepts ) - set ( result . columns ) if len ( missing_concepts ) > 0 : raise ValueError ( f \"The concept(s) ' { missing_concepts } ' were not added to the DataFrame.\" ) # Check that no other columns were added if only_adds_concepts : result_cols = set ( result . columns ) additionnal_cols = result_cols - ( initial_cols | set ( concepts )) if additionnal_cols : logger . warning ( \"The columns\" + \"\" . join ([ f \" \\n - { s } \" for s in additionnal_cols ]) + f \" \\n were added/renamed by ' { function . __module__ } . { function . __name__ } ',\" + f \"although it should normally only add the columns { concepts } \" ) return result algo_checker algo_checker ( function : Callable , algos : Optional [ str ] = None , * args , ** kwargs ) -> Any Decorator to use on wrapper that calls specific functions based on the 'algo' argument The decorator checks if the provided algo is an implemented one. If this checks fails, raises an error Source code in eds_scikit/utils/checks.py 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 @decorator def algo_checker ( function : Callable , algos : Optional [ str ] = None , * args , ** kwargs , ) -> Any : \"\"\" Decorator to use on wrapper that calls specific functions based on the 'algo' argument The decorator checks if the provided algo is an implemented one. If this checks fails, raises an error \"\"\" algo = _get_arg_value ( function , \"algo\" , args , kwargs ) # Stripping eventual version suffix algo = algo . split ( \".\" )[ 0 ] if algo not in algos : raise ValueError ( f \"Method { algo } unknown for ' { function . __module__ } . { function . __name__ } '. \\n \" f \"Available algos are { algos } \" ) result = function ( * args , ** kwargs ) return result","title":"checks"},{"location":"reference/utils/checks/#eds_scikitutilschecks","text":"","title":"eds_scikit.utils.checks"},{"location":"reference/utils/checks/#eds_scikit.utils.checks.MissingConceptError","text":"MissingConceptError ( required_concepts : Union [ List [ str ], List [ Tuple [ str , str ]]], df_name : str = '' ) Bases: Exception Exception raised when a concept is missing Source code in eds_scikit/utils/checks.py 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 def __init__ ( self , required_concepts : Union [ List [ str ], List [ Tuple [ str , str ]]], df_name : str = \"\" , ): if all ( isinstance ( concept , tuple ) for concept in required_concepts ): to_display_per_concept = [ f \"- { concept } ( { msg } )\" for concept , msg in required_concepts ] else : to_display_per_concept = [ f \"- { concept } \" for concept in required_concepts ] str_to_display = \" \\n \" . join ( to_display_per_concept ) if df_name : df_name = f \" { df_name } \" message = ( f \"The { df_name } DataFrame is missing some columns, \" \"namely: \\n \" f \" { str_to_display } \" ) super () . __init__ ( message )","title":"MissingConceptError"},{"location":"reference/utils/checks/#eds_scikit.utils.checks.MissingTableError","text":"MissingTableError ( required_tables : Union [ List [ str ], List [ Tuple [ str , str ]]], data_name : str = '' ) Bases: Exception Exception raised when a table is missing in the Data Source code in eds_scikit/utils/checks.py 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 def __init__ ( self , required_tables : Union [ List [ str ], List [ Tuple [ str , str ]]], data_name : str = \"\" , ): if all ( isinstance ( table , tuple ) for table in required_tables ): to_display_per_table = [ f \"- { table } ( { msg } )\" for table , msg in required_tables ] else : to_display_per_table = [ f \"- { table } \" for table in required_tables ] str_to_display = \" \\n \" . join ( to_display_per_table ) if data_name : data_name = f \" { data_name } \" message = ( f \"The { data_name } Data is missing some tables, \" \"namely: \\n \" f \" { str_to_display } \" ) super () . __init__ ( message )","title":"MissingTableError"},{"location":"reference/utils/checks/#eds_scikit.utils.checks.concept_checker","text":"concept_checker ( function : Callable , concepts : List [ str ] = None , only_adds_concepts : bool = True , * args , ** kwargs ) -> Any Decorator to use on functions that - Takes a DataFrame as first argument - Adds a concept to it The decorator checks: - If the first argument is a DataFrame - If the concepts to be added aren't already in the DataFrame - If the function correctly adds the concepts - If no additionnal columns are added (if only_adds_concepts is True) If one of this checks fails, raises an error Source code in eds_scikit/utils/checks.py 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 @decorator def concept_checker ( function : Callable , concepts : List [ str ] = None , only_adds_concepts : bool = True , * args , ** kwargs , ) -> Any : \"\"\" Decorator to use on functions that - Takes a DataFrame as first argument - Adds a concept to it The decorator checks: - If the first argument is a DataFrame - If the concepts to be added aren't already in the DataFrame - If the function correctly adds the concepts - If no additionnal columns are added (if only_adds_concepts is True) If one of this checks fails, raises an error \"\"\" # Is the first argument a DataFrame df = args [ 0 ] if ( type ( df ) != ks . DataFrame ) & ( type ( df ) != pd . DataFrame ): raise TypeError ( f \"The first argument of ' { function . __module__ } . { function . __name__ } ' \" \"should be a Pandas or Koalas DataFrame\" ) # Initial columns initial_cols = set ( df . columns ) # Is the concept already present if type ( concepts ) == str : concepts = [ concepts ] present_concepts = set ( concepts ) & set ( df . columns ) if present_concepts : raise ValueError ( f \"The concepts { present_concepts } are already present in the input dataframe \" f \"of ' { function . __module__ } . { function . __name__ } '. \\n \" \"You can either rename the column(s) or delete them before running \" \"the function again.\" ) result = function ( * args , ** kwargs ) # Was the concept correctly added missing_concepts = set ( concepts ) - set ( result . columns ) if len ( missing_concepts ) > 0 : raise ValueError ( f \"The concept(s) ' { missing_concepts } ' were not added to the DataFrame.\" ) # Check that no other columns were added if only_adds_concepts : result_cols = set ( result . columns ) additionnal_cols = result_cols - ( initial_cols | set ( concepts )) if additionnal_cols : logger . warning ( \"The columns\" + \"\" . join ([ f \" \\n - { s } \" for s in additionnal_cols ]) + f \" \\n were added/renamed by ' { function . __module__ } . { function . __name__ } ',\" + f \"although it should normally only add the columns { concepts } \" ) return result","title":"concept_checker()"},{"location":"reference/utils/checks/#eds_scikit.utils.checks.algo_checker","text":"algo_checker ( function : Callable , algos : Optional [ str ] = None , * args , ** kwargs ) -> Any Decorator to use on wrapper that calls specific functions based on the 'algo' argument The decorator checks if the provided algo is an implemented one. If this checks fails, raises an error Source code in eds_scikit/utils/checks.py 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 @decorator def algo_checker ( function : Callable , algos : Optional [ str ] = None , * args , ** kwargs , ) -> Any : \"\"\" Decorator to use on wrapper that calls specific functions based on the 'algo' argument The decorator checks if the provided algo is an implemented one. If this checks fails, raises an error \"\"\" algo = _get_arg_value ( function , \"algo\" , args , kwargs ) # Stripping eventual version suffix algo = algo . split ( \".\" )[ 0 ] if algo not in algos : raise ValueError ( f \"Method { algo } unknown for ' { function . __module__ } . { function . __name__ } '. \\n \" f \"Available algos are { algos } \" ) result = function ( * args , ** kwargs ) return result","title":"algo_checker()"},{"location":"reference/utils/datetime_helpers/","text":"eds_scikit.utils.datetime_helpers add_timedelta add_timedelta ( series : Series , ** kwargs ) -> Series Adds a unique timedelta to a Pandas or Koalas Series Source code in eds_scikit/utils/datetime_helpers.py 9 10 11 12 13 def add_timedelta ( series : Series , ** kwargs ) -> Series : \"\"\" Adds a unique timedelta to a Pandas or Koalas Series \"\"\" return series . map ( lambda d : d + timedelta ( ** kwargs )) substract_datetime substract_datetime ( series_1 : Series , series_2 : Series , out : str = 'seconds' ) -> Series Substract 2 datetime series and return the number of seconds or hours between them. Source code in eds_scikit/utils/datetime_helpers.py 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 def substract_datetime ( series_1 : Series , series_2 : Series , out : str = \"seconds\" , ) -> Series : \"\"\" Substract 2 datetime series and return the number of seconds or hours between them. \"\"\" if out not in [ \"seconds\" , \"hours\" ]: raise ValueError ( \"the 'out' parameter should be in ['hours','seconds']\" ) if not ( np . issubdtype ( series_1 . dtype , np . datetime64 ) and np . issubdtype ( series_2 . dtype , np . datetime64 ) ): raise TypeError ( \"One of the provided Serie isn't a datetime Serie\" ) if is_pandas ( series_1 ) and is_pandas ( series_2 ): diff = ( series_1 - series_2 ) . dt . total_seconds () elif is_koalas ( series_1 ) and is_koalas ( series_2 ): diff = series_1 - series_2 else : raise TypeError ( \"Both series should either be a Koalas or Pandas Serie\" ) if out == \"hours\" : return diff / 3600 return diff","title":"datetime_helpers"},{"location":"reference/utils/datetime_helpers/#eds_scikitutilsdatetime_helpers","text":"","title":"eds_scikit.utils.datetime_helpers"},{"location":"reference/utils/datetime_helpers/#eds_scikit.utils.datetime_helpers.add_timedelta","text":"add_timedelta ( series : Series , ** kwargs ) -> Series Adds a unique timedelta to a Pandas or Koalas Series Source code in eds_scikit/utils/datetime_helpers.py 9 10 11 12 13 def add_timedelta ( series : Series , ** kwargs ) -> Series : \"\"\" Adds a unique timedelta to a Pandas or Koalas Series \"\"\" return series . map ( lambda d : d + timedelta ( ** kwargs ))","title":"add_timedelta()"},{"location":"reference/utils/datetime_helpers/#eds_scikit.utils.datetime_helpers.substract_datetime","text":"substract_datetime ( series_1 : Series , series_2 : Series , out : str = 'seconds' ) -> Series Substract 2 datetime series and return the number of seconds or hours between them. Source code in eds_scikit/utils/datetime_helpers.py 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 def substract_datetime ( series_1 : Series , series_2 : Series , out : str = \"seconds\" , ) -> Series : \"\"\" Substract 2 datetime series and return the number of seconds or hours between them. \"\"\" if out not in [ \"seconds\" , \"hours\" ]: raise ValueError ( \"the 'out' parameter should be in ['hours','seconds']\" ) if not ( np . issubdtype ( series_1 . dtype , np . datetime64 ) and np . issubdtype ( series_2 . dtype , np . datetime64 ) ): raise TypeError ( \"One of the provided Serie isn't a datetime Serie\" ) if is_pandas ( series_1 ) and is_pandas ( series_2 ): diff = ( series_1 - series_2 ) . dt . total_seconds () elif is_koalas ( series_1 ) and is_koalas ( series_2 ): diff = series_1 - series_2 else : raise TypeError ( \"Both series should either be a Koalas or Pandas Serie\" ) if out == \"hours\" : return diff / 3600 return diff","title":"substract_datetime()"},{"location":"reference/utils/framework/","text":"eds_scikit.utils.framework BackendDispatcher Dispatcher between pandas, koalas and custom methods. In addition to the methods below, use the BackendDispatcher class to access the custom functions defined in CustomImplem . Examples: Use a dispatcher function >>> from eds_scikit.utils.framework import bd >>> bd . is_pandas ( pd . DataFrame ()) True Use a custom implemented function >>> df = pd . DataFrame ({ \"categ\" : [ \"a\" , \"b\" , \"c\" ]}) >>> bd . add_unique_id ( df , col_name = \"id\" ) categ id 0 a 0 1 b 1 2 c 2 get_backend get_backend ( obj ) -> Optional [ ModuleType ] Return the backend of a given object. PARAMETER DESCRIPTION obj RETURNS DESCRIPTION backend TYPE: a backend among Examples: Get the backend from a DataFrame and create another DataFrame from it. This is especially useful at runtime, when you need to infer the backend of the input. >>> backend = bd . get_backend ( pd . DataFrame ()) >>> backend <module 'pandas'> >>> df = backend . DataFrame () >>> bd . get_backend ( ks . DataFrame ()) <module 'koalas'> For demo purposes, return the backend when provided directly >>> bd . get_backend ( ks ) <module 'koalas'> >>> bd . get_backend ( spark ) None Source code in eds_scikit/utils/framework.py 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 def get_backend ( self , obj ) -> Optional [ ModuleType ]: \"\"\"Return the backend of a given object. Parameters ---------- obj: DataFrame or backend module among pandas or koalas. Returns ------- backend: a backend among {pd, ks} or None Examples -------- Get the backend from a DataFrame and create another DataFrame from it. This is especially useful at runtime, when you need to infer the backend of the input. >>> backend = bd.get_backend(pd.DataFrame()) >>> backend <module 'pandas'> >>> df = backend.DataFrame() >>> bd.get_backend(ks.DataFrame()) <module 'koalas'> For demo purposes, return the backend when provided directly >>> bd.get_backend(ks) <module 'koalas'> >>> bd.get_backend(spark) None \"\"\" if isinstance ( obj , str ): return { \"pd\" : pd , \"pandas\" : pd , \"ks\" : ks , \"koalas\" : ks , } . get ( obj ) for backend in VALID_FRAMEWORKS : if ( obj . __class__ . __module__ . startswith ( backend . __name__ ) # DataFrame() or getattr ( obj , \"__name__\" , None ) == backend . __name__ # pd or ks ): return backend return None is_pandas is_pandas ( obj ) -> bool Return True when the obj is either a pd.DataFrame or the pandas module. Source code in eds_scikit/utils/framework.py 158 159 160 def is_pandas ( self , obj ) -> bool : \"\"\"Return True when the obj is either a pd.DataFrame or the pandas module.\"\"\" return self . get_backend ( obj ) is pd is_koalas is_koalas ( obj : Any ) -> bool Return True when the obj is either a ks.DataFrame or the koalas module. Source code in eds_scikit/utils/framework.py 162 163 164 def is_koalas ( self , obj : Any ) -> bool : \"\"\"Return True when the obj is either a ks.DataFrame or the koalas module.\"\"\" return self . get_backend ( obj ) is ks to to ( obj , backend ) Convert a dataframe to the provided backend. PARAMETER DESCRIPTION obj The object(s) to convert to the provided backend backend: str, DataFrame or pandas, koalas module The desired output backend. RETURNS DESCRIPTION out The converted object, in the same format as provided in input. TYPE: DataFrame or iterabel of DataFrame (list, tuple, dict) Examples: Convert a single DataFrame >>> df = pd . DataFrame ({ \"a\" : [ 1 , 2 ]}) >>> kdf = bd . to ( df , backend = \"koalas\" ) >>> type ( kdf ) databricks.koalas.frame.DataFrame Convert a list of DataFrame >>> extra_kdf = ks . DataFrame ({ \"b\" : [ 0 , 1 ]}) >>> another_kdf = ks . DataFrame ({ \"c\" : [ 2 , 3 ]}) >>> kdf_list = [ kdf , extra_kdf , another_kdf ] >>> df_list = bd . to ( kdf_list , backend = \"pandas\" ) >>> type ( df_list ) list >>> len ( df_list ) 3 >>> type ( df_list [ 0 ]) pandas.core.frame.DataFrame Convert a dictionnary of DataFrame >>> df_dict = { \"df_1\" : pd . DataFrame ({ \"a\" : [ 1 , 2 ]}), \"df_2\" : pd . DataFrame ({ \"a\" : [ 2 , 3 ]})} >>> kdf_dict = bd . to ( df_dict , backend = \"koalas\" ) >>> type ( kdf_dict ) dict >>> kdf_dict . keys () dict_keys([\"df_1\", \"df_2\"]) >>> type ( kdf_dict [ \"df_1\" ]) databricks.koalas.frame.DataFrame Source code in eds_scikit/utils/framework.py 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 def to ( self , obj , backend ): \"\"\"Convert a dataframe to the provided backend. Parameters ---------- obj: DataFrame or iterable of DataFrame (list, tuple, dict) The object(s) to convert to the provided backend backend: str, DataFrame or pandas, koalas module The desired output backend. Returns ------- out: DataFrame or iterabel of DataFrame (list, tuple, dict) The converted object, in the same format as provided in input. Examples -------- Convert a single DataFrame >>> df = pd.DataFrame({\"a\": [1, 2]}) >>> kdf = bd.to(df, backend=\"koalas\") >>> type(kdf) databricks.koalas.frame.DataFrame Convert a list of DataFrame >>> extra_kdf = ks.DataFrame({\"b\": [0, 1]}) >>> another_kdf = ks.DataFrame({\"c\": [2, 3]}) >>> kdf_list = [kdf, extra_kdf, another_kdf] >>> df_list = bd.to(kdf_list, backend=\"pandas\") >>> type(df_list) list >>> len(df_list) 3 >>> type(df_list[0]) pandas.core.frame.DataFrame Convert a dictionnary of DataFrame >>> df_dict = {\"df_1\": pd.DataFrame({\"a\": [1, 2]}), \"df_2\": pd.DataFrame({\"a\": [2, 3]})} >>> kdf_dict = bd.to(df_dict, backend=\"koalas\") >>> type(kdf_dict) dict >>> kdf_dict.keys() dict_keys([\"df_1\", \"df_2\"]) >>> type(kdf_dict[\"df_1\"]) databricks.koalas.frame.DataFrame \"\"\" if isinstance ( obj , ( list , tuple )): results = [] for _obj in obj : results . append ( self . to ( _obj , backend )) return results if isinstance ( obj , dict ): results = {} for k , _obj in obj . items (): results [ k ] = self . to ( _obj , backend ) return results backend = self . get_backend ( backend ) if self . is_pandas ( backend ): return self . to_pandas ( obj ) elif self . is_koalas ( backend ): return self . to_koalas ( obj ) else : raise ValueError ( \"Unknown backend\" )","title":"framework"},{"location":"reference/utils/framework/#eds_scikitutilsframework","text":"","title":"eds_scikit.utils.framework"},{"location":"reference/utils/framework/#eds_scikit.utils.framework.BackendDispatcher","text":"Dispatcher between pandas, koalas and custom methods. In addition to the methods below, use the BackendDispatcher class to access the custom functions defined in CustomImplem . Examples: Use a dispatcher function >>> from eds_scikit.utils.framework import bd >>> bd . is_pandas ( pd . DataFrame ()) True Use a custom implemented function >>> df = pd . DataFrame ({ \"categ\" : [ \"a\" , \"b\" , \"c\" ]}) >>> bd . add_unique_id ( df , col_name = \"id\" ) categ id 0 a 0 1 b 1 2 c 2","title":"BackendDispatcher"},{"location":"reference/utils/framework/#eds_scikit.utils.framework.BackendDispatcher.get_backend","text":"get_backend ( obj ) -> Optional [ ModuleType ] Return the backend of a given object. PARAMETER DESCRIPTION obj RETURNS DESCRIPTION backend TYPE: a backend among Examples: Get the backend from a DataFrame and create another DataFrame from it. This is especially useful at runtime, when you need to infer the backend of the input. >>> backend = bd . get_backend ( pd . DataFrame ()) >>> backend <module 'pandas'> >>> df = backend . DataFrame () >>> bd . get_backend ( ks . DataFrame ()) <module 'koalas'> For demo purposes, return the backend when provided directly >>> bd . get_backend ( ks ) <module 'koalas'> >>> bd . get_backend ( spark ) None Source code in eds_scikit/utils/framework.py 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 def get_backend ( self , obj ) -> Optional [ ModuleType ]: \"\"\"Return the backend of a given object. Parameters ---------- obj: DataFrame or backend module among pandas or koalas. Returns ------- backend: a backend among {pd, ks} or None Examples -------- Get the backend from a DataFrame and create another DataFrame from it. This is especially useful at runtime, when you need to infer the backend of the input. >>> backend = bd.get_backend(pd.DataFrame()) >>> backend <module 'pandas'> >>> df = backend.DataFrame() >>> bd.get_backend(ks.DataFrame()) <module 'koalas'> For demo purposes, return the backend when provided directly >>> bd.get_backend(ks) <module 'koalas'> >>> bd.get_backend(spark) None \"\"\" if isinstance ( obj , str ): return { \"pd\" : pd , \"pandas\" : pd , \"ks\" : ks , \"koalas\" : ks , } . get ( obj ) for backend in VALID_FRAMEWORKS : if ( obj . __class__ . __module__ . startswith ( backend . __name__ ) # DataFrame() or getattr ( obj , \"__name__\" , None ) == backend . __name__ # pd or ks ): return backend return None","title":"get_backend()"},{"location":"reference/utils/framework/#eds_scikit.utils.framework.BackendDispatcher.is_pandas","text":"is_pandas ( obj ) -> bool Return True when the obj is either a pd.DataFrame or the pandas module. Source code in eds_scikit/utils/framework.py 158 159 160 def is_pandas ( self , obj ) -> bool : \"\"\"Return True when the obj is either a pd.DataFrame or the pandas module.\"\"\" return self . get_backend ( obj ) is pd","title":"is_pandas()"},{"location":"reference/utils/framework/#eds_scikit.utils.framework.BackendDispatcher.is_koalas","text":"is_koalas ( obj : Any ) -> bool Return True when the obj is either a ks.DataFrame or the koalas module. Source code in eds_scikit/utils/framework.py 162 163 164 def is_koalas ( self , obj : Any ) -> bool : \"\"\"Return True when the obj is either a ks.DataFrame or the koalas module.\"\"\" return self . get_backend ( obj ) is ks","title":"is_koalas()"},{"location":"reference/utils/framework/#eds_scikit.utils.framework.BackendDispatcher.to","text":"to ( obj , backend ) Convert a dataframe to the provided backend. PARAMETER DESCRIPTION obj The object(s) to convert to the provided backend backend: str, DataFrame or pandas, koalas module The desired output backend. RETURNS DESCRIPTION out The converted object, in the same format as provided in input. TYPE: DataFrame or iterabel of DataFrame (list, tuple, dict) Examples: Convert a single DataFrame >>> df = pd . DataFrame ({ \"a\" : [ 1 , 2 ]}) >>> kdf = bd . to ( df , backend = \"koalas\" ) >>> type ( kdf ) databricks.koalas.frame.DataFrame Convert a list of DataFrame >>> extra_kdf = ks . DataFrame ({ \"b\" : [ 0 , 1 ]}) >>> another_kdf = ks . DataFrame ({ \"c\" : [ 2 , 3 ]}) >>> kdf_list = [ kdf , extra_kdf , another_kdf ] >>> df_list = bd . to ( kdf_list , backend = \"pandas\" ) >>> type ( df_list ) list >>> len ( df_list ) 3 >>> type ( df_list [ 0 ]) pandas.core.frame.DataFrame Convert a dictionnary of DataFrame >>> df_dict = { \"df_1\" : pd . DataFrame ({ \"a\" : [ 1 , 2 ]}), \"df_2\" : pd . DataFrame ({ \"a\" : [ 2 , 3 ]})} >>> kdf_dict = bd . to ( df_dict , backend = \"koalas\" ) >>> type ( kdf_dict ) dict >>> kdf_dict . keys () dict_keys([\"df_1\", \"df_2\"]) >>> type ( kdf_dict [ \"df_1\" ]) databricks.koalas.frame.DataFrame Source code in eds_scikit/utils/framework.py 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 def to ( self , obj , backend ): \"\"\"Convert a dataframe to the provided backend. Parameters ---------- obj: DataFrame or iterable of DataFrame (list, tuple, dict) The object(s) to convert to the provided backend backend: str, DataFrame or pandas, koalas module The desired output backend. Returns ------- out: DataFrame or iterabel of DataFrame (list, tuple, dict) The converted object, in the same format as provided in input. Examples -------- Convert a single DataFrame >>> df = pd.DataFrame({\"a\": [1, 2]}) >>> kdf = bd.to(df, backend=\"koalas\") >>> type(kdf) databricks.koalas.frame.DataFrame Convert a list of DataFrame >>> extra_kdf = ks.DataFrame({\"b\": [0, 1]}) >>> another_kdf = ks.DataFrame({\"c\": [2, 3]}) >>> kdf_list = [kdf, extra_kdf, another_kdf] >>> df_list = bd.to(kdf_list, backend=\"pandas\") >>> type(df_list) list >>> len(df_list) 3 >>> type(df_list[0]) pandas.core.frame.DataFrame Convert a dictionnary of DataFrame >>> df_dict = {\"df_1\": pd.DataFrame({\"a\": [1, 2]}), \"df_2\": pd.DataFrame({\"a\": [2, 3]})} >>> kdf_dict = bd.to(df_dict, backend=\"koalas\") >>> type(kdf_dict) dict >>> kdf_dict.keys() dict_keys([\"df_1\", \"df_2\"]) >>> type(kdf_dict[\"df_1\"]) databricks.koalas.frame.DataFrame \"\"\" if isinstance ( obj , ( list , tuple )): results = [] for _obj in obj : results . append ( self . to ( _obj , backend )) return results if isinstance ( obj , dict ): results = {} for k , _obj in obj . items (): results [ k ] = self . to ( _obj , backend ) return results backend = self . get_backend ( backend ) if self . is_pandas ( backend ): return self . to_pandas ( obj ) elif self . is_koalas ( backend ): return self . to_koalas ( obj ) else : raise ValueError ( \"Unknown backend\" )","title":"to()"},{"location":"reference/utils/hierarchy/","text":"eds_scikit.utils.hierarchy build_hierarchy build_hierarchy ( categories : pd . DataFrame , relationships : pd . DataFrame ) -> pd . DataFrame Build a dataframe with parent categories as columns Source code in eds_scikit/utils/hierarchy.py 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 def build_hierarchy ( categories : pd . DataFrame , relationships : pd . DataFrame , ) -> pd . DataFrame : \"\"\"Build a dataframe with parent categories as columns\"\"\" assert set ( categories . columns ) == { \"id\" , \"category\" } assert set ( relationships . columns ) == { \"child\" , \"parent\" } assert not categories [ \"id\" ] . duplicated () . any () assert not relationships . duplicated () . any () expanded_relationships = _follow_relationships ( relationships ) expanded_relationships = expanded_relationships . loc [ expanded_relationships [ \"child\" ] . isin ( categories [ \"id\" ]) ] relationships_with_category = _deduplicate_parent_category ( expanded_relationships , categories ) categories = _finalize_parent_categories ( categories , relationships_with_category ) return categories","title":"hierarchy"},{"location":"reference/utils/hierarchy/#eds_scikitutilshierarchy","text":"","title":"eds_scikit.utils.hierarchy"},{"location":"reference/utils/hierarchy/#eds_scikit.utils.hierarchy.build_hierarchy","text":"build_hierarchy ( categories : pd . DataFrame , relationships : pd . DataFrame ) -> pd . DataFrame Build a dataframe with parent categories as columns Source code in eds_scikit/utils/hierarchy.py 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 def build_hierarchy ( categories : pd . DataFrame , relationships : pd . DataFrame , ) -> pd . DataFrame : \"\"\"Build a dataframe with parent categories as columns\"\"\" assert set ( categories . columns ) == { \"id\" , \"category\" } assert set ( relationships . columns ) == { \"child\" , \"parent\" } assert not categories [ \"id\" ] . duplicated () . any () assert not relationships . duplicated () . any () expanded_relationships = _follow_relationships ( relationships ) expanded_relationships = expanded_relationships . loc [ expanded_relationships [ \"child\" ] . isin ( categories [ \"id\" ]) ] relationships_with_category = _deduplicate_parent_category ( expanded_relationships , categories ) categories = _finalize_parent_categories ( categories , relationships_with_category ) return categories","title":"build_hierarchy()"},{"location":"reference/utils/logging/","text":"eds_scikit.utils.logging formatter formatter ( record : dict ) Formats the logging message by: Adding color and bold Indenting the message Source code in eds_scikit/utils/logging.py 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 def formatter ( record : dict ): \"\"\" Formats the logging message by: - Adding color and bold - Indenting the message \"\"\" base_format = ( \"<b>\" # bold \"<light-blue>[eds-scikit]</light-blue>\" \"- \" \" {name} :\" # corresponds to __name__ \" {extra[classname]}{extra[sep]} \" # class name, if relevant \"</b>\" \" {function} \" # function name ) colored_format = Colorizer . ansify ( base_format ) colored_message = Colorizer . ansify ( str ( record [ \"message\" ])) escaped_record = escape ( record ) base = colored_format . format ( ** escaped_record ) lines = colored_message . splitlines () new_message = \"\" . join ( \" \\n \" + line for line in lines ) + \" \\n \" return base + new_message escape escape ( record : dict ) Escape the \"<\" character before markup parsing Source code in eds_scikit/utils/logging.py 44 45 46 47 48 49 50 51 def escape ( record : dict ): \"\"\" Escape the \"<\" character before markup parsing \"\"\" return { k : v if not isinstance ( v , str ) else v . replace ( \"<\" , r \"\\<\" ) for k , v in record . items () }","title":"logging"},{"location":"reference/utils/logging/#eds_scikitutilslogging","text":"","title":"eds_scikit.utils.logging"},{"location":"reference/utils/logging/#eds_scikit.utils.logging.formatter","text":"formatter ( record : dict ) Formats the logging message by: Adding color and bold Indenting the message Source code in eds_scikit/utils/logging.py 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 def formatter ( record : dict ): \"\"\" Formats the logging message by: - Adding color and bold - Indenting the message \"\"\" base_format = ( \"<b>\" # bold \"<light-blue>[eds-scikit]</light-blue>\" \"- \" \" {name} :\" # corresponds to __name__ \" {extra[classname]}{extra[sep]} \" # class name, if relevant \"</b>\" \" {function} \" # function name ) colored_format = Colorizer . ansify ( base_format ) colored_message = Colorizer . ansify ( str ( record [ \"message\" ])) escaped_record = escape ( record ) base = colored_format . format ( ** escaped_record ) lines = colored_message . splitlines () new_message = \"\" . join ( \" \\n \" + line for line in lines ) + \" \\n \" return base + new_message","title":"formatter()"},{"location":"reference/utils/logging/#eds_scikit.utils.logging.escape","text":"escape ( record : dict ) Escape the \"<\" character before markup parsing Source code in eds_scikit/utils/logging.py 44 45 46 47 48 49 50 51 def escape ( record : dict ): \"\"\" Escape the \"<\" character before markup parsing \"\"\" return { k : v if not isinstance ( v , str ) else v . replace ( \"<\" , r \"\\<\" ) for k , v in record . items () }","title":"escape()"},{"location":"reference/utils/test_utils/","text":"eds_scikit.utils.test_utils","title":"test_utils"},{"location":"reference/utils/test_utils/#eds_scikitutilstest_utils","text":"","title":"eds_scikit.utils.test_utils"},{"location":"reference/utils/typing/","text":"eds_scikit.utils.typing","title":"typing"},{"location":"reference/utils/typing/#eds_scikitutilstyping","text":"","title":"eds_scikit.utils.typing"},{"location":"reference/utils/custom_implem/","text":"eds_scikit.utils.custom_implem","title":"`eds_scikit.utils.custom_implem`"},{"location":"reference/utils/custom_implem/#eds_scikitutilscustom_implem","text":"","title":"eds_scikit.utils.custom_implem"},{"location":"reference/utils/custom_implem/custom_implem/","text":"eds_scikit.utils.custom_implem.custom_implem CustomImplem A collection of custom pandas and koalas methods. All public facing methods must be stateless and defined as classmethods. add_unique_id classmethod add_unique_id ( obj : Any , col_name : str = 'id' , backend = None ) -> Any Add an ID column for koalas or pandas. Source code in eds_scikit/utils/custom_implem/custom_implem.py 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 @classmethod def add_unique_id ( cls , obj : Any , col_name : str = \"id\" , backend = None , ) -> Any : \"\"\"Add an ID column for koalas or pandas.\"\"\" if backend is pd : obj [ col_name ] = range ( obj . shape [ 0 ]) return obj elif backend is ks : return obj . koalas . attach_id_column ( id_type = \"distributed\" , column = col_name ) else : raise NotImplementedError ( f \"No method 'add_unique_id' is available for backend ' { backend } '.\" ) cut classmethod cut ( x , bins , right : bool = True , labels = None , retbins : bool = False , precision : int = 3 , include_lowest : bool = False , duplicates : str = 'raise' , ordered : bool = True , backend = None ) koalas version of pd.cut Notes Simplified vendoring from: https://github.com/pandas-dev/pandas/blob/v1.5.2/pandas/core/reshape/tile.py#L50-L305 Source code in eds_scikit/utils/custom_implem/custom_implem.py 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 @classmethod def cut ( cls , x , bins , right : bool = True , labels = None , retbins : bool = False , precision : int = 3 , include_lowest : bool = False , duplicates : str = \"raise\" , ordered : bool = True , backend = None , # unused because koalas only ): \"\"\"koalas version of pd.cut Notes ----- Simplified vendoring from: https://github.com/pandas-dev/pandas/blob/v1.5.2/pandas/core/reshape/tile.py#L50-L305 \"\"\" return cut ( x , bins , right , labels , retbins , precision , include_lowest , duplicates , ordered , )","title":"custom_implem"},{"location":"reference/utils/custom_implem/custom_implem/#eds_scikitutilscustom_implemcustom_implem","text":"","title":"eds_scikit.utils.custom_implem.custom_implem"},{"location":"reference/utils/custom_implem/custom_implem/#eds_scikit.utils.custom_implem.custom_implem.CustomImplem","text":"A collection of custom pandas and koalas methods. All public facing methods must be stateless and defined as classmethods.","title":"CustomImplem"},{"location":"reference/utils/custom_implem/custom_implem/#eds_scikit.utils.custom_implem.custom_implem.CustomImplem.add_unique_id","text":"add_unique_id ( obj : Any , col_name : str = 'id' , backend = None ) -> Any Add an ID column for koalas or pandas. Source code in eds_scikit/utils/custom_implem/custom_implem.py 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 @classmethod def add_unique_id ( cls , obj : Any , col_name : str = \"id\" , backend = None , ) -> Any : \"\"\"Add an ID column for koalas or pandas.\"\"\" if backend is pd : obj [ col_name ] = range ( obj . shape [ 0 ]) return obj elif backend is ks : return obj . koalas . attach_id_column ( id_type = \"distributed\" , column = col_name ) else : raise NotImplementedError ( f \"No method 'add_unique_id' is available for backend ' { backend } '.\" )","title":"add_unique_id()"},{"location":"reference/utils/custom_implem/custom_implem/#eds_scikit.utils.custom_implem.custom_implem.CustomImplem.cut","text":"cut ( x , bins , right : bool = True , labels = None , retbins : bool = False , precision : int = 3 , include_lowest : bool = False , duplicates : str = 'raise' , ordered : bool = True , backend = None ) koalas version of pd.cut","title":"cut()"},{"location":"reference/utils/custom_implem/custom_implem/#eds_scikit.utils.custom_implem.custom_implem.CustomImplem.cut--notes","text":"Simplified vendoring from: https://github.com/pandas-dev/pandas/blob/v1.5.2/pandas/core/reshape/tile.py#L50-L305 Source code in eds_scikit/utils/custom_implem/custom_implem.py 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 @classmethod def cut ( cls , x , bins , right : bool = True , labels = None , retbins : bool = False , precision : int = 3 , include_lowest : bool = False , duplicates : str = \"raise\" , ordered : bool = True , backend = None , # unused because koalas only ): \"\"\"koalas version of pd.cut Notes ----- Simplified vendoring from: https://github.com/pandas-dev/pandas/blob/v1.5.2/pandas/core/reshape/tile.py#L50-L305 \"\"\" return cut ( x , bins , right , labels , retbins , precision , include_lowest , duplicates , ordered , )","title":"Notes"},{"location":"reference/utils/custom_implem/cut/","text":"eds_scikit.utils.custom_implem.cut cut cut ( x , bins , right : bool = True , labels = None , retbins : bool = False , precision : int = 3 , include_lowest : bool = False , duplicates : str = 'raise' , ordered : bool = True ) Bin values into discrete intervals. Use cut when you need to segment and sort data values into bins. This function is also useful for going from a continuous variable to a categorical variable. For example, cut could convert ages to groups of age ranges. Supports binning into an equal number of bins, or a pre-specified array of bins. See original function at: https://github.com/pandas-dev/pandas/blob/v1.5.2/pandas/core/reshape/tile.py#L50-L305 # noqa PARAMETER DESCRIPTION x The input array to be binned. Must be 1-dimensional. TYPE: koalas Series. bins The criteria to bin by. * int : Defines the number of equal-width bins in the range of x . The range of x is extended by .1% on each side to include the minimum and maximum values of x . * sequence of scalars : Defines the bin edges allowing for non-uniform width. No extension of the range of x is done. * IntervalIndex : Defines the exact bins to be used. Note that IntervalIndex for bins must be non-overlapping. TYPE: int, sequence of scalars, or IntervalIndex right Indicates whether bins includes the rightmost edge or not. If right == True (the default), then the bins [1, 2, 3, 4] indicate (1,2], (2,3], (3,4]. This argument is ignored when bins is an IntervalIndex. TYPE: bool, default True DEFAULT: True labels Specifies the labels for the returned bins. Must be the same length as the resulting bins. If False, returns only integer indicators of the bins. This affects the type of the output container (see below). This argument is ignored when bins is an IntervalIndex. If True, raises an error. When ordered=False , labels must be provided. TYPE: array or False, default None DEFAULT: None retbins Whether to return the bins or not. Useful when bins is provided as a scalar. TYPE: bool, default False DEFAULT: False precision The precision at which to store and display the bins labels. TYPE: int, default 3 DEFAULT: 3 include_lowest Whether the first interval should be left-inclusive or not. TYPE: bool, default False DEFAULT: False duplicates If bin edges are not unique, raise ValueError or drop non-uniques. TYPE: str DEFAULT: default 'raise' ordered Whether the labels are ordered or not. Applies to returned types Categorical and Series (with Categorical dtype). If True, the resulting categorical will be ordered. If False, the resulting categorical will be unordered (labels must be provided). .. versionadded:: 1.1.0 TYPE: bool, default True DEFAULT: True Returns out An array-like object representing the respective bin for each value of x . The type depends on the value of labels . * None (default) : returns a Series for Series x or a Categorical for all other inputs. The values stored within are Interval dtype. * sequence of scalars : returns a Series for Series x or a Categorical for all other inputs. The values stored within are whatever the type in the sequence is. * False : returns an ndarray of integers. TYPE: Categorical, Series, or ndarray bins The computed or specified bins. Only returned when retbins=True . For scalar or sequence bins , this is an ndarray with the computed bins. If set duplicates=drop , bins will drop non-unique bin. For an IntervalIndex bins , this is equal to bins . TYPE: numpy.ndarray or IntervalIndex. See Also qcut : Discretize variable into equal-sized buckets based on rank or based on sample quantiles. Categorical : Array type for storing data that come from a fixed set of values. Series : One-dimensional array with axis labels (including time series). IntervalIndex : Immutable Index implementing an ordered, sliceable set. Notes Any NA values will be NA in the result. Out of bounds values will be NA in the resulting Series or Categorical object. Reference :ref: the user guide <reshaping.tile.cut> for more examples. Examples: Discretize into three equal-sized bins. >>> from eds_scikit.utils.framework import bd >>> bd . cut ( ks . Series ( np . array ([ 1 , 7 , 5 , 4 , 6 , 3 ])), 3 ) ... [(0.994, 3.0], (5.0, 7.0], (3.0, 5.0], (3.0, 5.0], (5.0, 7.0], ... Categories (3, interval[float64, right]): [(0.994, 3.0] < (3.0, 5.0] ... >>> bd . cut ( ks . Series ( np . array ([ 1 , 7 , 5 , 4 , 6 , 3 ])), 3 , retbins = True ) ... ([(0.994, 3.0], (5.0, 7.0], (3.0, 5.0], (3.0, 5.0], (5.0, 7.0], ... Categories (3, interval[float64, right]): [(0.994, 3.0] < (3.0, 5.0] ... array([0.994, 3. , 5. , 7. ])) Discovers the same bins, but assign them specific labels. Notice that the returned Categorical's categories are labels and is ordered. >>> bd . cut ( ks . Series ( np . array ([ 1 , 7 , 5 , 4 , 6 , 3 ])), ... 3 , labels = [ \"bad\" , \"medium\" , \"good\" ]) ['bad', 'good', 'medium', 'medium', 'good', 'bad'] Categories (3, object): ['bad' < 'medium' < 'good'] ordered=False will result in unordered categories when labels are passed. This parameter can be used to allow non-unique labels: >>> bd . cut ( ks . Series ( np . array ([ 1 , 7 , 5 , 4 , 6 , 3 ])), 3 , ... labels = [ \"B\" , \"A\" , \"B\" ], ordered = False ) ['B', 'B', 'A', 'A', 'B', 'B'] Categories (2, object): ['A', 'B'] labels=False implies you just want the bins back. >>> bd . cut ( ks . Series ([ 0 , 1 , 1 , 2 ]), bins = 4 , labels = False ) array([0, 1, 1, 3]) Passing a Series as an input returns a Series with categorical dtype: >>> s = ks . Series ( np . array ([ 2 , 4 , 6 , 8 , 10 ]), ... index = [ 'a' , 'b' , 'c' , 'd' , 'e' ]) >>> bd . cut ( s , 3 ) ... a (1.992, 4.667] b (1.992, 4.667] c (4.667, 7.333] d (7.333, 10.0] e (7.333, 10.0] dtype: category Categories (3, interval[float64, right]): [(1.992, 4.667] < (4.667, ... Passing a Series as an input returns a Series with mapping value. It is used to map numerically to intervals based on bins. >>> s = ks . Series ( np . array ([ 2 , 4 , 6 , 8 , 10 ]), ... index = [ 'a' , 'b' , 'c' , 'd' , 'e' ]) >>> bd . cut ( s , [ 0 , 2 , 4 , 6 , 8 , 10 ], labels = False , retbins = True , right = False ) ... (a 1.0 b 2.0 c 3.0 d 4.0 e NaN dtype: float64, array([ 0, 2, 4, 6, 8, 10])) Use drop optional when bins is not unique >>> bd . cut ( s , [ 0 , 2 , 4 , 6 , 10 , 10 ], labels = False , retbins = True , ... right = False , duplicates = 'drop' ) ... (a 1.0 b 2.0 c 3.0 d 3.0 e NaN dtype: float64, array([ 0, 2, 4, 6, 10])) Passing an IntervalIndex for bins results in those categories exactly. Notice that values not covered by the IntervalIndex are set to NaN. 0 is to the left of the first bin (which is closed on the right), and 1.5 falls between two bins. >>> bins = pd . IntervalIndex . from_tuples ([( 0 , 1 ), ( 2 , 3 ), ( 4 , 5 )]) >>> bd . cut ( ks . Series ([ 0 , 0.5 , 1.5 , 2.5 , 4.5 ]), bins ) [NaN, (0.0, 1.0], NaN, (2.0, 3.0], (4.0, 5.0]] Categories (3, interval[int64, right]): [(0, 1] < (2, 3] < (4, 5]] Source code in eds_scikit/utils/custom_implem/cut.py 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 def cut ( x , bins , right : bool = True , labels = None , retbins : bool = False , precision : int = 3 , include_lowest : bool = False , duplicates : str = \"raise\" , ordered : bool = True , ): # pragma: no cover \"\"\" Bin values into discrete intervals. Use `cut` when you need to segment and sort data values into bins. This function is also useful for going from a continuous variable to a categorical variable. For example, `cut` could convert ages to groups of age ranges. Supports binning into an equal number of bins, or a pre-specified array of bins. See original function at: https://github.com/pandas-dev/pandas/blob/v1.5.2/pandas/core/reshape/tile.py#L50-L305 # noqa Parameters ---------- x : koalas Series. The input array to be binned. Must be 1-dimensional. bins : int, sequence of scalars, or IntervalIndex The criteria to bin by. * int : Defines the number of equal-width bins in the range of `x`. The range of `x` is extended by .1% on each side to include the minimum and maximum values of `x`. * sequence of scalars : Defines the bin edges allowing for non-uniform width. No extension of the range of `x` is done. * IntervalIndex : Defines the exact bins to be used. Note that IntervalIndex for `bins` must be non-overlapping. right : bool, default True Indicates whether `bins` includes the rightmost edge or not. If ``right == True`` (the default), then the `bins` ``[1, 2, 3, 4]`` indicate (1,2], (2,3], (3,4]. This argument is ignored when `bins` is an IntervalIndex. labels : array or False, default None Specifies the labels for the returned bins. Must be the same length as the resulting bins. If False, returns only integer indicators of the bins. This affects the type of the output container (see below). This argument is ignored when `bins` is an IntervalIndex. If True, raises an error. When `ordered=False`, labels must be provided. retbins : bool, default False Whether to return the bins or not. Useful when bins is provided as a scalar. precision : int, default 3 The precision at which to store and display the bins labels. include_lowest : bool, default False Whether the first interval should be left-inclusive or not. duplicates : {default 'raise', 'drop'}, optional If bin edges are not unique, raise ValueError or drop non-uniques. ordered : bool, default True Whether the labels are ordered or not. Applies to returned types Categorical and Series (with Categorical dtype). If True, the resulting categorical will be ordered. If False, the resulting categorical will be unordered (labels must be provided). .. versionadded:: 1.1.0 Returns ------- out : Categorical, Series, or ndarray An array-like object representing the respective bin for each value of `x`. The type depends on the value of `labels`. * None (default) : returns a Series for Series `x` or a Categorical for all other inputs. The values stored within are Interval dtype. * sequence of scalars : returns a Series for Series `x` or a Categorical for all other inputs. The values stored within are whatever the type in the sequence is. * False : returns an ndarray of integers. bins : numpy.ndarray or IntervalIndex. The computed or specified bins. Only returned when `retbins=True`. For scalar or sequence `bins`, this is an ndarray with the computed bins. If set `duplicates=drop`, `bins` will drop non-unique bin. For an IntervalIndex `bins`, this is equal to `bins`. See Also -------- qcut : Discretize variable into equal-sized buckets based on rank or based on sample quantiles. Categorical : Array type for storing data that come from a fixed set of values. Series : One-dimensional array with axis labels (including time series). IntervalIndex : Immutable Index implementing an ordered, sliceable set. Notes ----- Any NA values will be NA in the result. Out of bounds values will be NA in the resulting Series or Categorical object. Reference :ref:`the user guide <reshaping.tile.cut>` for more examples. Examples -------- Discretize into three equal-sized bins. >>> from eds_scikit.utils.framework import bd >>> bd.cut(ks.Series(np.array([1, 7, 5, 4, 6, 3])), 3) ... # doctest: +ELLIPSIS [(0.994, 3.0], (5.0, 7.0], (3.0, 5.0], (3.0, 5.0], (5.0, 7.0], ... Categories (3, interval[float64, right]): [(0.994, 3.0] < (3.0, 5.0] ... >>> bd.cut(ks.Series(np.array([1, 7, 5, 4, 6, 3])), 3, retbins=True) ... # doctest: +ELLIPSIS ([(0.994, 3.0], (5.0, 7.0], (3.0, 5.0], (3.0, 5.0], (5.0, 7.0], ... Categories (3, interval[float64, right]): [(0.994, 3.0] < (3.0, 5.0] ... array([0.994, 3. , 5. , 7. ])) Discovers the same bins, but assign them specific labels. Notice that the returned Categorical's categories are `labels` and is ordered. >>> bd.cut(ks.Series(np.array([1, 7, 5, 4, 6, 3])), ... 3, labels=[\"bad\", \"medium\", \"good\"]) ['bad', 'good', 'medium', 'medium', 'good', 'bad'] Categories (3, object): ['bad' < 'medium' < 'good'] ``ordered=False`` will result in unordered categories when labels are passed. This parameter can be used to allow non-unique labels: >>> bd.cut(ks.Series(np.array([1, 7, 5, 4, 6, 3])), 3, ... labels=[\"B\", \"A\", \"B\"], ordered=False) ['B', 'B', 'A', 'A', 'B', 'B'] Categories (2, object): ['A', 'B'] ``labels=False`` implies you just want the bins back. >>> bd.cut(ks.Series([0, 1, 1, 2]), bins=4, labels=False) array([0, 1, 1, 3]) Passing a Series as an input returns a Series with categorical dtype: >>> s = ks.Series(np.array([2, 4, 6, 8, 10]), ... index=['a', 'b', 'c', 'd', 'e']) >>> bd.cut(s, 3) ... # doctest: +ELLIPSIS a (1.992, 4.667] b (1.992, 4.667] c (4.667, 7.333] d (7.333, 10.0] e (7.333, 10.0] dtype: category Categories (3, interval[float64, right]): [(1.992, 4.667] < (4.667, ... Passing a Series as an input returns a Series with mapping value. It is used to map numerically to intervals based on bins. >>> s = ks.Series(np.array([2, 4, 6, 8, 10]), ... index=['a', 'b', 'c', 'd', 'e']) >>> bd.cut(s, [0, 2, 4, 6, 8, 10], labels=False, retbins=True, right=False) ... # doctest: +ELLIPSIS (a 1.0 b 2.0 c 3.0 d 4.0 e NaN dtype: float64, array([ 0, 2, 4, 6, 8, 10])) Use `drop` optional when bins is not unique >>> bd.cut(s, [0, 2, 4, 6, 10, 10], labels=False, retbins=True, ... right=False, duplicates='drop') ... # doctest: +ELLIPSIS (a 1.0 b 2.0 c 3.0 d 3.0 e NaN dtype: float64, array([ 0, 2, 4, 6, 10])) Passing an IntervalIndex for `bins` results in those categories exactly. Notice that values not covered by the IntervalIndex are set to NaN. 0 is to the left of the first bin (which is closed on the right), and 1.5 falls between two bins. >>> bins = pd.IntervalIndex.from_tuples([(0, 1), (2, 3), (4, 5)]) >>> bd.cut(ks.Series([0, 0.5, 1.5, 2.5, 4.5]), bins) [NaN, (0.0, 1.0], NaN, (2.0, 3.0], (4.0, 5.0]] Categories (3, interval[int64, right]): [(0, 1] < (2, 3] < (4, 5]] \"\"\" if x . ndim != 1 : raise ValueError ( \"x must be 1D\" ) x , dtype = x . astype ( np . int64 ), x . dtype if not np . iterable ( bins ): if is_scalar ( bins ) and bins < 1 : raise ValueError ( \"`bins` should be a positive integer.\" ) try : # for array-like sz = x . size except AttributeError : x = np . asarray ( x ) sz = x . size if sz == 0 : raise ValueError ( \"Cannot cut empty array\" ) mn , mx = x . min (), x . max () if np . isinf ( mn ) or np . isinf ( mx ): raise ValueError ( \"cannot specify integer `bins` when input data contains infinity\" ) elif mn == mx : # adjust end points before binning mn -= 0.001 * abs ( mn ) if mn != 0 else 0.001 mx += 0.001 * abs ( mx ) if mx != 0 else 0.001 bins = np . linspace ( mn , mx , bins + 1 , endpoint = True ) else : # adjust end points after binning bins = np . linspace ( mn , mx , bins + 1 , endpoint = True ) adj = ( mx - mn ) * 0.001 # 0.1% of the range if right : bins [ 0 ] -= adj else : bins [ - 1 ] += adj elif isinstance ( bins , IntervalIndex ): if bins . is_overlapping : raise ValueError ( \"Overlapping IntervalIndex is not accepted.\" ) else : if is_datetime64tz_dtype ( bins ): bins = np . asarray ( bins , dtype = DT64NS_DTYPE ) else : bins = np . asarray ( bins ) bins = _convert_bin_to_numeric_type ( bins , dtype ) # GH 26045: cast to float64 to avoid an overflow if ( np . diff ( bins . astype ( \"float64\" )) < 0 ) . any (): raise ValueError ( \"bins must increase monotonically.\" ) fac , bins = _bins_to_cuts ( x , bins , right = right , labels = labels , precision = precision , include_lowest = include_lowest , dtype = dtype , duplicates = duplicates , ordered = ordered , ) if not retbins : return fac return fac , bins","title":"cut"},{"location":"reference/utils/custom_implem/cut/#eds_scikitutilscustom_implemcut","text":"","title":"eds_scikit.utils.custom_implem.cut"},{"location":"reference/utils/custom_implem/cut/#eds_scikit.utils.custom_implem.cut.cut","text":"cut ( x , bins , right : bool = True , labels = None , retbins : bool = False , precision : int = 3 , include_lowest : bool = False , duplicates : str = 'raise' , ordered : bool = True ) Bin values into discrete intervals. Use cut when you need to segment and sort data values into bins. This function is also useful for going from a continuous variable to a categorical variable. For example, cut could convert ages to groups of age ranges. Supports binning into an equal number of bins, or a pre-specified array of bins. See original function at: https://github.com/pandas-dev/pandas/blob/v1.5.2/pandas/core/reshape/tile.py#L50-L305 # noqa PARAMETER DESCRIPTION x The input array to be binned. Must be 1-dimensional. TYPE: koalas Series. bins The criteria to bin by. * int : Defines the number of equal-width bins in the range of x . The range of x is extended by .1% on each side to include the minimum and maximum values of x . * sequence of scalars : Defines the bin edges allowing for non-uniform width. No extension of the range of x is done. * IntervalIndex : Defines the exact bins to be used. Note that IntervalIndex for bins must be non-overlapping. TYPE: int, sequence of scalars, or IntervalIndex right Indicates whether bins includes the rightmost edge or not. If right == True (the default), then the bins [1, 2, 3, 4] indicate (1,2], (2,3], (3,4]. This argument is ignored when bins is an IntervalIndex. TYPE: bool, default True DEFAULT: True labels Specifies the labels for the returned bins. Must be the same length as the resulting bins. If False, returns only integer indicators of the bins. This affects the type of the output container (see below). This argument is ignored when bins is an IntervalIndex. If True, raises an error. When ordered=False , labels must be provided. TYPE: array or False, default None DEFAULT: None retbins Whether to return the bins or not. Useful when bins is provided as a scalar. TYPE: bool, default False DEFAULT: False precision The precision at which to store and display the bins labels. TYPE: int, default 3 DEFAULT: 3 include_lowest Whether the first interval should be left-inclusive or not. TYPE: bool, default False DEFAULT: False duplicates If bin edges are not unique, raise ValueError or drop non-uniques. TYPE: str DEFAULT: default 'raise' ordered Whether the labels are ordered or not. Applies to returned types Categorical and Series (with Categorical dtype). If True, the resulting categorical will be ordered. If False, the resulting categorical will be unordered (labels must be provided). .. versionadded:: 1.1.0 TYPE: bool, default True DEFAULT: True Returns out An array-like object representing the respective bin for each value of x . The type depends on the value of labels . * None (default) : returns a Series for Series x or a Categorical for all other inputs. The values stored within are Interval dtype. * sequence of scalars : returns a Series for Series x or a Categorical for all other inputs. The values stored within are whatever the type in the sequence is. * False : returns an ndarray of integers. TYPE: Categorical, Series, or ndarray bins The computed or specified bins. Only returned when retbins=True . For scalar or sequence bins , this is an ndarray with the computed bins. If set duplicates=drop , bins will drop non-unique bin. For an IntervalIndex bins , this is equal to bins . TYPE: numpy.ndarray or IntervalIndex.","title":"cut()"},{"location":"reference/utils/custom_implem/cut/#eds_scikit.utils.custom_implem.cut.cut--see-also","text":"qcut : Discretize variable into equal-sized buckets based on rank or based on sample quantiles. Categorical : Array type for storing data that come from a fixed set of values. Series : One-dimensional array with axis labels (including time series). IntervalIndex : Immutable Index implementing an ordered, sliceable set.","title":"See Also"},{"location":"reference/utils/custom_implem/cut/#eds_scikit.utils.custom_implem.cut.cut--notes","text":"Any NA values will be NA in the result. Out of bounds values will be NA in the resulting Series or Categorical object. Reference :ref: the user guide <reshaping.tile.cut> for more examples. Examples: Discretize into three equal-sized bins. >>> from eds_scikit.utils.framework import bd >>> bd . cut ( ks . Series ( np . array ([ 1 , 7 , 5 , 4 , 6 , 3 ])), 3 ) ... [(0.994, 3.0], (5.0, 7.0], (3.0, 5.0], (3.0, 5.0], (5.0, 7.0], ... Categories (3, interval[float64, right]): [(0.994, 3.0] < (3.0, 5.0] ... >>> bd . cut ( ks . Series ( np . array ([ 1 , 7 , 5 , 4 , 6 , 3 ])), 3 , retbins = True ) ... ([(0.994, 3.0], (5.0, 7.0], (3.0, 5.0], (3.0, 5.0], (5.0, 7.0], ... Categories (3, interval[float64, right]): [(0.994, 3.0] < (3.0, 5.0] ... array([0.994, 3. , 5. , 7. ])) Discovers the same bins, but assign them specific labels. Notice that the returned Categorical's categories are labels and is ordered. >>> bd . cut ( ks . Series ( np . array ([ 1 , 7 , 5 , 4 , 6 , 3 ])), ... 3 , labels = [ \"bad\" , \"medium\" , \"good\" ]) ['bad', 'good', 'medium', 'medium', 'good', 'bad'] Categories (3, object): ['bad' < 'medium' < 'good'] ordered=False will result in unordered categories when labels are passed. This parameter can be used to allow non-unique labels: >>> bd . cut ( ks . Series ( np . array ([ 1 , 7 , 5 , 4 , 6 , 3 ])), 3 , ... labels = [ \"B\" , \"A\" , \"B\" ], ordered = False ) ['B', 'B', 'A', 'A', 'B', 'B'] Categories (2, object): ['A', 'B'] labels=False implies you just want the bins back. >>> bd . cut ( ks . Series ([ 0 , 1 , 1 , 2 ]), bins = 4 , labels = False ) array([0, 1, 1, 3]) Passing a Series as an input returns a Series with categorical dtype: >>> s = ks . Series ( np . array ([ 2 , 4 , 6 , 8 , 10 ]), ... index = [ 'a' , 'b' , 'c' , 'd' , 'e' ]) >>> bd . cut ( s , 3 ) ... a (1.992, 4.667] b (1.992, 4.667] c (4.667, 7.333] d (7.333, 10.0] e (7.333, 10.0] dtype: category Categories (3, interval[float64, right]): [(1.992, 4.667] < (4.667, ... Passing a Series as an input returns a Series with mapping value. It is used to map numerically to intervals based on bins. >>> s = ks . Series ( np . array ([ 2 , 4 , 6 , 8 , 10 ]), ... index = [ 'a' , 'b' , 'c' , 'd' , 'e' ]) >>> bd . cut ( s , [ 0 , 2 , 4 , 6 , 8 , 10 ], labels = False , retbins = True , right = False ) ... (a 1.0 b 2.0 c 3.0 d 4.0 e NaN dtype: float64, array([ 0, 2, 4, 6, 8, 10])) Use drop optional when bins is not unique >>> bd . cut ( s , [ 0 , 2 , 4 , 6 , 10 , 10 ], labels = False , retbins = True , ... right = False , duplicates = 'drop' ) ... (a 1.0 b 2.0 c 3.0 d 3.0 e NaN dtype: float64, array([ 0, 2, 4, 6, 10])) Passing an IntervalIndex for bins results in those categories exactly. Notice that values not covered by the IntervalIndex are set to NaN. 0 is to the left of the first bin (which is closed on the right), and 1.5 falls between two bins. >>> bins = pd . IntervalIndex . from_tuples ([( 0 , 1 ), ( 2 , 3 ), ( 4 , 5 )]) >>> bd . cut ( ks . Series ([ 0 , 0.5 , 1.5 , 2.5 , 4.5 ]), bins ) [NaN, (0.0, 1.0], NaN, (2.0, 3.0], (4.0, 5.0]] Categories (3, interval[int64, right]): [(0, 1] < (2, 3] < (4, 5]] Source code in eds_scikit/utils/custom_implem/cut.py 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 def cut ( x , bins , right : bool = True , labels = None , retbins : bool = False , precision : int = 3 , include_lowest : bool = False , duplicates : str = \"raise\" , ordered : bool = True , ): # pragma: no cover \"\"\" Bin values into discrete intervals. Use `cut` when you need to segment and sort data values into bins. This function is also useful for going from a continuous variable to a categorical variable. For example, `cut` could convert ages to groups of age ranges. Supports binning into an equal number of bins, or a pre-specified array of bins. See original function at: https://github.com/pandas-dev/pandas/blob/v1.5.2/pandas/core/reshape/tile.py#L50-L305 # noqa Parameters ---------- x : koalas Series. The input array to be binned. Must be 1-dimensional. bins : int, sequence of scalars, or IntervalIndex The criteria to bin by. * int : Defines the number of equal-width bins in the range of `x`. The range of `x` is extended by .1% on each side to include the minimum and maximum values of `x`. * sequence of scalars : Defines the bin edges allowing for non-uniform width. No extension of the range of `x` is done. * IntervalIndex : Defines the exact bins to be used. Note that IntervalIndex for `bins` must be non-overlapping. right : bool, default True Indicates whether `bins` includes the rightmost edge or not. If ``right == True`` (the default), then the `bins` ``[1, 2, 3, 4]`` indicate (1,2], (2,3], (3,4]. This argument is ignored when `bins` is an IntervalIndex. labels : array or False, default None Specifies the labels for the returned bins. Must be the same length as the resulting bins. If False, returns only integer indicators of the bins. This affects the type of the output container (see below). This argument is ignored when `bins` is an IntervalIndex. If True, raises an error. When `ordered=False`, labels must be provided. retbins : bool, default False Whether to return the bins or not. Useful when bins is provided as a scalar. precision : int, default 3 The precision at which to store and display the bins labels. include_lowest : bool, default False Whether the first interval should be left-inclusive or not. duplicates : {default 'raise', 'drop'}, optional If bin edges are not unique, raise ValueError or drop non-uniques. ordered : bool, default True Whether the labels are ordered or not. Applies to returned types Categorical and Series (with Categorical dtype). If True, the resulting categorical will be ordered. If False, the resulting categorical will be unordered (labels must be provided). .. versionadded:: 1.1.0 Returns ------- out : Categorical, Series, or ndarray An array-like object representing the respective bin for each value of `x`. The type depends on the value of `labels`. * None (default) : returns a Series for Series `x` or a Categorical for all other inputs. The values stored within are Interval dtype. * sequence of scalars : returns a Series for Series `x` or a Categorical for all other inputs. The values stored within are whatever the type in the sequence is. * False : returns an ndarray of integers. bins : numpy.ndarray or IntervalIndex. The computed or specified bins. Only returned when `retbins=True`. For scalar or sequence `bins`, this is an ndarray with the computed bins. If set `duplicates=drop`, `bins` will drop non-unique bin. For an IntervalIndex `bins`, this is equal to `bins`. See Also -------- qcut : Discretize variable into equal-sized buckets based on rank or based on sample quantiles. Categorical : Array type for storing data that come from a fixed set of values. Series : One-dimensional array with axis labels (including time series). IntervalIndex : Immutable Index implementing an ordered, sliceable set. Notes ----- Any NA values will be NA in the result. Out of bounds values will be NA in the resulting Series or Categorical object. Reference :ref:`the user guide <reshaping.tile.cut>` for more examples. Examples -------- Discretize into three equal-sized bins. >>> from eds_scikit.utils.framework import bd >>> bd.cut(ks.Series(np.array([1, 7, 5, 4, 6, 3])), 3) ... # doctest: +ELLIPSIS [(0.994, 3.0], (5.0, 7.0], (3.0, 5.0], (3.0, 5.0], (5.0, 7.0], ... Categories (3, interval[float64, right]): [(0.994, 3.0] < (3.0, 5.0] ... >>> bd.cut(ks.Series(np.array([1, 7, 5, 4, 6, 3])), 3, retbins=True) ... # doctest: +ELLIPSIS ([(0.994, 3.0], (5.0, 7.0], (3.0, 5.0], (3.0, 5.0], (5.0, 7.0], ... Categories (3, interval[float64, right]): [(0.994, 3.0] < (3.0, 5.0] ... array([0.994, 3. , 5. , 7. ])) Discovers the same bins, but assign them specific labels. Notice that the returned Categorical's categories are `labels` and is ordered. >>> bd.cut(ks.Series(np.array([1, 7, 5, 4, 6, 3])), ... 3, labels=[\"bad\", \"medium\", \"good\"]) ['bad', 'good', 'medium', 'medium', 'good', 'bad'] Categories (3, object): ['bad' < 'medium' < 'good'] ``ordered=False`` will result in unordered categories when labels are passed. This parameter can be used to allow non-unique labels: >>> bd.cut(ks.Series(np.array([1, 7, 5, 4, 6, 3])), 3, ... labels=[\"B\", \"A\", \"B\"], ordered=False) ['B', 'B', 'A', 'A', 'B', 'B'] Categories (2, object): ['A', 'B'] ``labels=False`` implies you just want the bins back. >>> bd.cut(ks.Series([0, 1, 1, 2]), bins=4, labels=False) array([0, 1, 1, 3]) Passing a Series as an input returns a Series with categorical dtype: >>> s = ks.Series(np.array([2, 4, 6, 8, 10]), ... index=['a', 'b', 'c', 'd', 'e']) >>> bd.cut(s, 3) ... # doctest: +ELLIPSIS a (1.992, 4.667] b (1.992, 4.667] c (4.667, 7.333] d (7.333, 10.0] e (7.333, 10.0] dtype: category Categories (3, interval[float64, right]): [(1.992, 4.667] < (4.667, ... Passing a Series as an input returns a Series with mapping value. It is used to map numerically to intervals based on bins. >>> s = ks.Series(np.array([2, 4, 6, 8, 10]), ... index=['a', 'b', 'c', 'd', 'e']) >>> bd.cut(s, [0, 2, 4, 6, 8, 10], labels=False, retbins=True, right=False) ... # doctest: +ELLIPSIS (a 1.0 b 2.0 c 3.0 d 4.0 e NaN dtype: float64, array([ 0, 2, 4, 6, 8, 10])) Use `drop` optional when bins is not unique >>> bd.cut(s, [0, 2, 4, 6, 10, 10], labels=False, retbins=True, ... right=False, duplicates='drop') ... # doctest: +ELLIPSIS (a 1.0 b 2.0 c 3.0 d 3.0 e NaN dtype: float64, array([ 0, 2, 4, 6, 10])) Passing an IntervalIndex for `bins` results in those categories exactly. Notice that values not covered by the IntervalIndex are set to NaN. 0 is to the left of the first bin (which is closed on the right), and 1.5 falls between two bins. >>> bins = pd.IntervalIndex.from_tuples([(0, 1), (2, 3), (4, 5)]) >>> bd.cut(ks.Series([0, 0.5, 1.5, 2.5, 4.5]), bins) [NaN, (0.0, 1.0], NaN, (2.0, 3.0], (4.0, 5.0]] Categories (3, interval[int64, right]): [(0, 1] < (2, 3] < (4, 5]] \"\"\" if x . ndim != 1 : raise ValueError ( \"x must be 1D\" ) x , dtype = x . astype ( np . int64 ), x . dtype if not np . iterable ( bins ): if is_scalar ( bins ) and bins < 1 : raise ValueError ( \"`bins` should be a positive integer.\" ) try : # for array-like sz = x . size except AttributeError : x = np . asarray ( x ) sz = x . size if sz == 0 : raise ValueError ( \"Cannot cut empty array\" ) mn , mx = x . min (), x . max () if np . isinf ( mn ) or np . isinf ( mx ): raise ValueError ( \"cannot specify integer `bins` when input data contains infinity\" ) elif mn == mx : # adjust end points before binning mn -= 0.001 * abs ( mn ) if mn != 0 else 0.001 mx += 0.001 * abs ( mx ) if mx != 0 else 0.001 bins = np . linspace ( mn , mx , bins + 1 , endpoint = True ) else : # adjust end points after binning bins = np . linspace ( mn , mx , bins + 1 , endpoint = True ) adj = ( mx - mn ) * 0.001 # 0.1% of the range if right : bins [ 0 ] -= adj else : bins [ - 1 ] += adj elif isinstance ( bins , IntervalIndex ): if bins . is_overlapping : raise ValueError ( \"Overlapping IntervalIndex is not accepted.\" ) else : if is_datetime64tz_dtype ( bins ): bins = np . asarray ( bins , dtype = DT64NS_DTYPE ) else : bins = np . asarray ( bins ) bins = _convert_bin_to_numeric_type ( bins , dtype ) # GH 26045: cast to float64 to avoid an overflow if ( np . diff ( bins . astype ( \"float64\" )) < 0 ) . any (): raise ValueError ( \"bins must increase monotonically.\" ) fac , bins = _bins_to_cuts ( x , bins , right = right , labels = labels , precision = precision , include_lowest = include_lowest , dtype = dtype , duplicates = duplicates , ordered = ordered , ) if not retbins : return fac return fac , bins","title":"Notes"},{"location":"reference/utils/flowchart/","text":"eds_scikit.utils.flowchart","title":"`eds_scikit.utils.flowchart`"},{"location":"reference/utils/flowchart/#eds_scikitutilsflowchart","text":"","title":"eds_scikit.utils.flowchart"},{"location":"reference/utils/flowchart/flowchart/","text":"eds_scikit.utils.flowchart.flowchart Flowchart Flowchart ( initial_description : str , data : Union [ DataFrame , Dict [ str , Iterable ]], concat_criterion_description : bool = True , to_count : str = 'person_id' ) Main class to define an flowchart (inclusion diagram) PARAMETER DESCRIPTION initial_description Description of the initial population TYPE: str data Either a Pandas/Koalas DataFrame, or a dictionary of iterables. If a dictionary, the initial cohort should be proivided under the initial key. TYPE: Union [ DataFrame , Dict [ str , Iterable ]] concat_criterion_description Whether to concatenate provided description together when adding multiple criteria TYPE: bool , optional DEFAULT: True to_count Only if data is a DataFrame: column of data from which the count is computed. Usually, this will be the column containing patient or stay IDs. TYPE: str , optional DEFAULT: 'person_id' Source code in eds_scikit/utils/flowchart/flowchart.py 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 def __init__ ( self , initial_description : str , data : Union [ DataFrame , Dict [ str , Iterable ]], concat_criterion_description : bool = True , to_count : str = \"person_id\" , ): \"\"\" Main class to define an flowchart (inclusion diagram) Parameters ---------- initial_description : str Description of the initial population data : Union[DataFrame, Dict[str, Iterable]] Either a Pandas/Koalas DataFrame, or a dictionary of iterables. If a dictionary, the initial cohort should be proivided under the **initial** key. concat_criterion_description : bool, optional Whether to concatenate provided description together when adding multiple criteria to_count : str, optional Only if `data` is a DataFrame: column of `data` from which the count is computed. Usually, this will be the column containing patient or stay IDs. \"\"\" self . initial_description = initial_description self . data = data self . to_count = to_count self . check_data () self . ids = self . get_unique () self . criteria = [] self . concat_criterion_description = concat_criterion_description self . final_split = None self . drawing = None add_criterion add_criterion ( description : str , criterion_name : str , excluded_description : str = '' ) Adds a criterion to the flowchart PARAMETER DESCRIPTION description Description of the cohort passing the criterion TYPE: str criterion_name If data is a DataFrame, criterion_name is a boolean column of data to split between passing cohort ( data[criterion_name] == True ) and excluded column ( data[criterion_name] == False ) If data is a dictionary, criterion_name is a key of data containing the passing cohort as an iterable of IDs (list, set , Series, array, etc.) TYPE: str excluded_description Description of the cohort excluded by the criterion TYPE: str DEFAULT: '' Source code in eds_scikit/utils/flowchart/flowchart.py 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 def add_criterion ( self , description : str , criterion_name : str , excluded_description : str = \"\" , ): \"\"\" Adds a criterion to the flowchart ![](../../../_static/flowchart/criterion.png) Parameters ---------- description : str Description of the cohort passing the criterion criterion_name : str - If `data` is a DataFrame, `criterion_name` is a boolean column of `data` to split between passing cohort (`data[criterion_name] == True`) and excluded column (`data[criterion_name] == False`) - If `data` is a dictionary, `criterion_name` is a key of `data` containing the passing cohort as an iterable of IDs (list, set , Series, array, etc.) excluded_description: str Description of the cohort excluded by the criterion \"\"\" input_data = ( Data ( self . ids , ) if not self . criteria else self . criteria [ - 1 ] . output_data ) passing_criterion_ids = self . get_unique ( criterion_name = criterion_name ) output_data = Data ( passing_criterion_ids & input_data . ids , ) excluded_data = Data ( input_data . ids - passing_criterion_ids , ) description = ( description if not self . concat_criterion_description else ( self . get_last_description () + description ) ) added_criterion = Criterion ( description = description , excluded_description = excluded_description , input_data = input_data , output_data = output_data , excluded_data = excluded_data , ) self . criteria . append ( added_criterion ) add_final_split add_final_split ( left_description : str , right_description : str , criterion_name : str , left_title : str = '' , right_title : str = '' ) Adds a final split in two distinct cohorts. Should be called after all other critera were added. PARAMETER DESCRIPTION left_description Description of the left cohort TYPE: str right_description Description of the right cohort TYPE: str criterion_name If data is a DataFrame, criterion_name is a boolean column of data to split between passing cohort ( data[criterion_name] == True ) and excluded column ( data[criterion_name] == False ) If data is a dictionary, criterion_name is a key of data containing the passing cohort as an iterable of IDs (list, set , Series, array, etc.) TYPE: str left_title Title of the left cohort TYPE: str , optional DEFAULT: '' right_title title of the right cohort TYPE: str , optional DEFAULT: '' Source code in eds_scikit/utils/flowchart/flowchart.py 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 def add_final_split ( self , left_description : str , right_description : str , criterion_name : str , left_title : str = \"\" , right_title : str = \"\" , ): \"\"\" Adds a final split in two distinct cohorts. Should be called after all other critera were added. ![](../../../_static/flowchart/split.png) Parameters ---------- left_description : str Description of the left cohort right_description : str Description of the right cohort criterion_name : str - If `data` is a DataFrame, `criterion_name` is a boolean column of `data` to split between passing cohort (`data[criterion_name] == True`) and excluded column (`data[criterion_name] == False`) - If `data` is a dictionary, `criterion_name` is a key of `data` containing the passing cohort as an iterable of IDs (list, set , Series, array, etc.) left_title : str, optional Title of the left cohort right_title : str, optional title of the right cohort \"\"\" input_data = ( Data ( self . ids , ) if not self . criteria else self . criteria [ - 1 ] . output_data ) left_criterion_ids = self . get_unique ( criterion_name = criterion_name ) left_data = Data ( left_criterion_ids & input_data . ids , ) right_data = Data ( input_data . ids - left_criterion_ids , ) left_description = ( left_description if not self . concat_criterion_description else ( self . get_last_description () + left_description ) ) right_description = ( right_description if not self . concat_criterion_description else ( self . get_last_description () + right_description ) ) added_criterion = Criterion ( description = left_description , excluded_description = right_description , input_data = input_data , output_data = left_data , excluded_data = right_data , ) added_criterion . left_title = left_title added_criterion . right_title = right_title self . final_split = added_criterion generate_flowchart generate_flowchart ( alternate : bool = False , fontsize : int = 10 ) Generate and display the flowchart PARAMETER DESCRIPTION alternate Wether to alternate the excluded box positions TYPE: bool , optional DEFAULT: False fontsize fontsize TYPE: int , optional DEFAULT: 10 Source code in eds_scikit/utils/flowchart/flowchart.py 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 def generate_flowchart ( self , alternate : bool = False , fontsize : int = 10 , ): \"\"\" Generate and display the flowchart Parameters ---------- alternate : bool, optional Wether to alternate the excluded box positions fontsize : int, optional fontsize \"\"\" max_criterion_width = max ( [ c . get_bbox ( fontsize = fontsize )[ \"w\" ] for c in self . criteria ] ) arrow_length = 1.2 * ( max_criterion_width / 2 ) directions = [ \"right\" , \"left\" ] if alternate else [ \"right\" , \"right\" ] d = Drawing () d . config ( font = \"dejavu sans\" , fontsize = fontsize , unit = 1 ) start_description = ( self . initial_description + \" \\n \" + f \"( { self . criteria [ 0 ] . input_data } )\" ) start_bbox = Criterion . get_bbox ( None , txt = start_description ) d += flow . Start ( ** start_bbox ) . label ( start_description ) for i , c in enumerate ( self . criteria ): d = c . draw ( d , arrow_length = arrow_length , direction = directions [ i % 2 ], fontsize = fontsize , ) if self . final_split is not None : d = self . final_split . draw ( d , final_split = True , fontsize = fontsize ) self . drawing = d return d save save ( filename : Union [ str , Path ], transparent : bool = False , dpi : int = 72 ) Save the generated flowchart PARAMETER DESCRIPTION filename path to the saved file (should end with svg or png) TYPE: Union [ str , Path ] transparent Wether to use a transparent background or not TYPE: bool , optional DEFAULT: False dpi Resolution (only when saving png) TYPE: int , optional DEFAULT: 72 Source code in eds_scikit/utils/flowchart/flowchart.py 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 def save ( self , filename : Union [ str , Path ], transparent : bool = False , dpi : int = 72 ): \"\"\" Save the generated flowchart Parameters ---------- filename : Union[str, Path] path to the saved file (should end with svg or png) transparent : bool, optional Wether to use a transparent background or not dpi : int, optional Resolution (only when saving png) \"\"\" self . drawing . save ( fname = filename , transparent = transparent , dpi = dpi )","title":"flowchart"},{"location":"reference/utils/flowchart/flowchart/#eds_scikitutilsflowchartflowchart","text":"","title":"eds_scikit.utils.flowchart.flowchart"},{"location":"reference/utils/flowchart/flowchart/#eds_scikit.utils.flowchart.flowchart.Flowchart","text":"Flowchart ( initial_description : str , data : Union [ DataFrame , Dict [ str , Iterable ]], concat_criterion_description : bool = True , to_count : str = 'person_id' ) Main class to define an flowchart (inclusion diagram) PARAMETER DESCRIPTION initial_description Description of the initial population TYPE: str data Either a Pandas/Koalas DataFrame, or a dictionary of iterables. If a dictionary, the initial cohort should be proivided under the initial key. TYPE: Union [ DataFrame , Dict [ str , Iterable ]] concat_criterion_description Whether to concatenate provided description together when adding multiple criteria TYPE: bool , optional DEFAULT: True to_count Only if data is a DataFrame: column of data from which the count is computed. Usually, this will be the column containing patient or stay IDs. TYPE: str , optional DEFAULT: 'person_id' Source code in eds_scikit/utils/flowchart/flowchart.py 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 def __init__ ( self , initial_description : str , data : Union [ DataFrame , Dict [ str , Iterable ]], concat_criterion_description : bool = True , to_count : str = \"person_id\" , ): \"\"\" Main class to define an flowchart (inclusion diagram) Parameters ---------- initial_description : str Description of the initial population data : Union[DataFrame, Dict[str, Iterable]] Either a Pandas/Koalas DataFrame, or a dictionary of iterables. If a dictionary, the initial cohort should be proivided under the **initial** key. concat_criterion_description : bool, optional Whether to concatenate provided description together when adding multiple criteria to_count : str, optional Only if `data` is a DataFrame: column of `data` from which the count is computed. Usually, this will be the column containing patient or stay IDs. \"\"\" self . initial_description = initial_description self . data = data self . to_count = to_count self . check_data () self . ids = self . get_unique () self . criteria = [] self . concat_criterion_description = concat_criterion_description self . final_split = None self . drawing = None","title":"Flowchart"},{"location":"reference/utils/flowchart/flowchart/#eds_scikit.utils.flowchart.flowchart.Flowchart.add_criterion","text":"add_criterion ( description : str , criterion_name : str , excluded_description : str = '' ) Adds a criterion to the flowchart PARAMETER DESCRIPTION description Description of the cohort passing the criterion TYPE: str criterion_name If data is a DataFrame, criterion_name is a boolean column of data to split between passing cohort ( data[criterion_name] == True ) and excluded column ( data[criterion_name] == False ) If data is a dictionary, criterion_name is a key of data containing the passing cohort as an iterable of IDs (list, set , Series, array, etc.) TYPE: str excluded_description Description of the cohort excluded by the criterion TYPE: str DEFAULT: '' Source code in eds_scikit/utils/flowchart/flowchart.py 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 def add_criterion ( self , description : str , criterion_name : str , excluded_description : str = \"\" , ): \"\"\" Adds a criterion to the flowchart ![](../../../_static/flowchart/criterion.png) Parameters ---------- description : str Description of the cohort passing the criterion criterion_name : str - If `data` is a DataFrame, `criterion_name` is a boolean column of `data` to split between passing cohort (`data[criterion_name] == True`) and excluded column (`data[criterion_name] == False`) - If `data` is a dictionary, `criterion_name` is a key of `data` containing the passing cohort as an iterable of IDs (list, set , Series, array, etc.) excluded_description: str Description of the cohort excluded by the criterion \"\"\" input_data = ( Data ( self . ids , ) if not self . criteria else self . criteria [ - 1 ] . output_data ) passing_criterion_ids = self . get_unique ( criterion_name = criterion_name ) output_data = Data ( passing_criterion_ids & input_data . ids , ) excluded_data = Data ( input_data . ids - passing_criterion_ids , ) description = ( description if not self . concat_criterion_description else ( self . get_last_description () + description ) ) added_criterion = Criterion ( description = description , excluded_description = excluded_description , input_data = input_data , output_data = output_data , excluded_data = excluded_data , ) self . criteria . append ( added_criterion )","title":"add_criterion()"},{"location":"reference/utils/flowchart/flowchart/#eds_scikit.utils.flowchart.flowchart.Flowchart.add_final_split","text":"add_final_split ( left_description : str , right_description : str , criterion_name : str , left_title : str = '' , right_title : str = '' ) Adds a final split in two distinct cohorts. Should be called after all other critera were added. PARAMETER DESCRIPTION left_description Description of the left cohort TYPE: str right_description Description of the right cohort TYPE: str criterion_name If data is a DataFrame, criterion_name is a boolean column of data to split between passing cohort ( data[criterion_name] == True ) and excluded column ( data[criterion_name] == False ) If data is a dictionary, criterion_name is a key of data containing the passing cohort as an iterable of IDs (list, set , Series, array, etc.) TYPE: str left_title Title of the left cohort TYPE: str , optional DEFAULT: '' right_title title of the right cohort TYPE: str , optional DEFAULT: '' Source code in eds_scikit/utils/flowchart/flowchart.py 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 def add_final_split ( self , left_description : str , right_description : str , criterion_name : str , left_title : str = \"\" , right_title : str = \"\" , ): \"\"\" Adds a final split in two distinct cohorts. Should be called after all other critera were added. ![](../../../_static/flowchart/split.png) Parameters ---------- left_description : str Description of the left cohort right_description : str Description of the right cohort criterion_name : str - If `data` is a DataFrame, `criterion_name` is a boolean column of `data` to split between passing cohort (`data[criterion_name] == True`) and excluded column (`data[criterion_name] == False`) - If `data` is a dictionary, `criterion_name` is a key of `data` containing the passing cohort as an iterable of IDs (list, set , Series, array, etc.) left_title : str, optional Title of the left cohort right_title : str, optional title of the right cohort \"\"\" input_data = ( Data ( self . ids , ) if not self . criteria else self . criteria [ - 1 ] . output_data ) left_criterion_ids = self . get_unique ( criterion_name = criterion_name ) left_data = Data ( left_criterion_ids & input_data . ids , ) right_data = Data ( input_data . ids - left_criterion_ids , ) left_description = ( left_description if not self . concat_criterion_description else ( self . get_last_description () + left_description ) ) right_description = ( right_description if not self . concat_criterion_description else ( self . get_last_description () + right_description ) ) added_criterion = Criterion ( description = left_description , excluded_description = right_description , input_data = input_data , output_data = left_data , excluded_data = right_data , ) added_criterion . left_title = left_title added_criterion . right_title = right_title self . final_split = added_criterion","title":"add_final_split()"},{"location":"reference/utils/flowchart/flowchart/#eds_scikit.utils.flowchart.flowchart.Flowchart.generate_flowchart","text":"generate_flowchart ( alternate : bool = False , fontsize : int = 10 ) Generate and display the flowchart PARAMETER DESCRIPTION alternate Wether to alternate the excluded box positions TYPE: bool , optional DEFAULT: False fontsize fontsize TYPE: int , optional DEFAULT: 10 Source code in eds_scikit/utils/flowchart/flowchart.py 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 def generate_flowchart ( self , alternate : bool = False , fontsize : int = 10 , ): \"\"\" Generate and display the flowchart Parameters ---------- alternate : bool, optional Wether to alternate the excluded box positions fontsize : int, optional fontsize \"\"\" max_criterion_width = max ( [ c . get_bbox ( fontsize = fontsize )[ \"w\" ] for c in self . criteria ] ) arrow_length = 1.2 * ( max_criterion_width / 2 ) directions = [ \"right\" , \"left\" ] if alternate else [ \"right\" , \"right\" ] d = Drawing () d . config ( font = \"dejavu sans\" , fontsize = fontsize , unit = 1 ) start_description = ( self . initial_description + \" \\n \" + f \"( { self . criteria [ 0 ] . input_data } )\" ) start_bbox = Criterion . get_bbox ( None , txt = start_description ) d += flow . Start ( ** start_bbox ) . label ( start_description ) for i , c in enumerate ( self . criteria ): d = c . draw ( d , arrow_length = arrow_length , direction = directions [ i % 2 ], fontsize = fontsize , ) if self . final_split is not None : d = self . final_split . draw ( d , final_split = True , fontsize = fontsize ) self . drawing = d return d","title":"generate_flowchart()"},{"location":"reference/utils/flowchart/flowchart/#eds_scikit.utils.flowchart.flowchart.Flowchart.save","text":"save ( filename : Union [ str , Path ], transparent : bool = False , dpi : int = 72 ) Save the generated flowchart PARAMETER DESCRIPTION filename path to the saved file (should end with svg or png) TYPE: Union [ str , Path ] transparent Wether to use a transparent background or not TYPE: bool , optional DEFAULT: False dpi Resolution (only when saving png) TYPE: int , optional DEFAULT: 72 Source code in eds_scikit/utils/flowchart/flowchart.py 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 def save ( self , filename : Union [ str , Path ], transparent : bool = False , dpi : int = 72 ): \"\"\" Save the generated flowchart Parameters ---------- filename : Union[str, Path] path to the saved file (should end with svg or png) transparent : bool, optional Wether to use a transparent background or not dpi : int, optional Resolution (only when saving png) \"\"\" self . drawing . save ( fname = filename , transparent = transparent , dpi = dpi )","title":"save()"}]}